{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb7a3eaa1f0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import utils\n",
    "import export\n",
    "\n",
    "# Setup\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--method', type=str, default='SLL', choices=[\n",
    "    'CER', 'REF', 'SLL', 'SLL_G'\n",
    "])\n",
    "parser.add_argument('--budget_pct', type=float, default=0.25)\n",
    "parser.add_argument('--g0_method', type=str, default='random', choices=[\n",
    "  'random', # randomly distribution of g0\n",
    "  'large_cluster', # a random node and [g0_size] of its neighbors are in g0\n",
    "  'many_clusters', # 10 random nodes and [g0_size] of their neighbors are in g0\n",
    "  ])\n",
    "parser.add_argument('--g0_size', type=float, default=0.2)\n",
    "parser.add_argument('--lr', type=float, default=10)\n",
    "parser.add_argument('--T_s', type=int, default=1263)\n",
    "parser.add_argument('--T_u', type=int, default=-1)\n",
    "parser.add_argument('--dataset', type=str, default='cora', choices=[\n",
    "    'Cora', 'Cora-ML', 'Citeseer', 'Pubmed', 'Polblogs', 'ACM', 'BlogCatalog', 'Flickr', 'UAI'\n",
    "])\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.25)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_neighbor_subgraphs(adj: torch.Tensor, size: int, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor ~ `(n * size)` of node indices for `n` supgraphs of size `size`\n",
    "    \"\"\"\n",
    "    res = torch.zeros([n, size], dtype=torch.long)\n",
    "\n",
    "    for i in range(n):\n",
    "        out = []\n",
    "        stack = [random.randint(0, adj.shape[0] - 1)]\n",
    "        while len(out) < size:\n",
    "            if len(stack) == 0:\n",
    "                stack.append(random.randint(0, adj.shape[0] - 1))\n",
    "            curNode = stack.pop()\n",
    "            if curNode not in out:\n",
    "                out.append(curNode)\n",
    "                children = adj[curNode].nonzero().t()[0].cpu().tolist()\n",
    "                stack = children + stack\n",
    "        res[i] = torch.tensor(out)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_rand_subgraphs(size: int, n: int) -> torch.Tensor:\n",
    "    return torch.randint(adj.shape[0], [n, size], dtype=torch.long)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Sequential, DenseGCNConv\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_gcn(hid: int=64):\n",
    "    return Sequential('x, adj', [\n",
    "        (DenseGCNConv(feat.shape[1], hid), 'x, adj -> x'),\n",
    "        ReLU(inplace=True),\n",
    "        (DenseGCNConv(hid, hid), 'x, adj -> x'),\n",
    "        ReLU(inplace=True),\n",
    "        Linear(hid, int(label.max()) + 1),\n",
    "    ]).to(device)\n",
    "\n",
    "def train(model, dataloader: DataLoader, epochs: int):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "    t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Model training\")\n",
    "    loss = torch.tensor(0)\n",
    "    for _ in t:\n",
    "        for feats, adjs, labels, train_masks in dataloader:\n",
    "            pred = model(feats, adjs)\n",
    "            mask = train_masks.flatten()\n",
    "            loss = F.cross_entropy(pred.flatten(end_dim=1)[mask], labels.flatten(end_dim=1)[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
    "\n",
    "def train_adj(model, feat, adj, label, train_mask, epochs: int):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "    t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Model training\")\n",
    "    loss = torch.tensor(0)\n",
    "    for _ in t:\n",
    "        pred = model(feat, adj)\n",
    "        loss = F.cross_entropy(pred.squeeze()[train_mask], label[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
    "\n",
    "def eval(model, test_mask, text=\"\"):\n",
    "    model.eval()\n",
    "    pred = model(feat, adj)\n",
    "    acc = ((pred.argmax(dim=2).squeeze() == label)[test_mask].sum() / test_mask.sum()).item()\n",
    "    print(text + f\"Accuracy: {acc: 0.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 256\n",
    "sample_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "adj, feat, label, train_mask, val_mask, test_mask = utils.load_data(args.dataset, args.seed, device=device)\n",
    "pgd_subgraphs = get_neighbor_subgraphs(adj.cpu(), sample_size, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = adj[pgd_subgraphs]\n",
    "subgraph_feats = feat[pgd_subgraphs]\n",
    "subgraph_adjs = torch.zeros(num_samples, sample_size, sample_size).to(device)\n",
    "subgraph_labels = label[pgd_subgraphs]\n",
    "subgraph_train_masks = train_mask[pgd_subgraphs]\n",
    "for i in range(num_samples):\n",
    "    subgraph_adjs[i] = temp[i][:,pgd_subgraphs[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training: 100%|██████████| 100/100 [00:03<00:00, 26.65it/s, loss=0.38]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  72.54%\n"
     ]
    }
   ],
   "source": [
    "clean_dataloader = DataLoader(\n",
    "    TensorDataset(subgraph_feats, subgraph_adjs, subgraph_labels, subgraph_train_masks), \n",
    "    batch_size=128, shuffle=True)\n",
    "\n",
    "gcn = get_gcn()\n",
    "train(gcn, clean_dataloader, 100)\n",
    "eval(gcn, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.9161760807037354\n",
      "Epoch 10, training loss: 1.0627546310424805\n",
      "Epoch 20, training loss: 0.4512659013271332\n",
      "Epoch 30, training loss: 0.2120225578546524\n",
      "Epoch 40, training loss: 0.12651602923870087\n",
      "Epoch 50, training loss: 0.09486710280179977\n",
      "Epoch 60, training loss: 0.09469141066074371\n",
      "Epoch 70, training loss: 0.0886521115899086\n",
      "Epoch 80, training loss: 0.066765695810318\n",
      "Epoch 90, training loss: 0.06861631572246552\n",
      "Epoch 100, training loss: 0.0684695616364479\n",
      "Epoch 110, training loss: 0.053196825087070465\n",
      "Epoch 120, training loss: 0.06589846312999725\n",
      "Epoch 130, training loss: 0.06179617717862129\n",
      "Epoch 140, training loss: 0.05705156922340393\n",
      "Epoch 150, training loss: 0.04810468852519989\n",
      "Epoch 160, training loss: 0.04297434911131859\n",
      "Epoch 170, training loss: 0.04362943395972252\n",
      "Epoch 180, training loss: 0.044159047305583954\n",
      "Epoch 190, training loss: 0.04222859442234039\n"
     ]
    }
   ],
   "source": [
    "import deeprobust\n",
    "from deeprobust.graph.data import Dataset\n",
    "from deeprobust.graph.defense import GCN\n",
    "from deeprobust.graph.global_attack import PGDAttack\n",
    "from deeprobust.graph.utils import preprocess\n",
    "\n",
    "# Setup Victim Model\n",
    "victim_model = GCN(nfeat=feat.shape[1], nclass=label.max().item()+1,\n",
    "                    nhid=16, dropout=0.5, weight_decay=5e-4, device=device)\n",
    "victim_model.fit(feat, adj, label, train_mask, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:36<00:00,  5.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# Setup Attack Model\n",
    "model = PGDAttack(model=victim_model, nnodes=adj.shape[0], loss_type='CE', device=device)\n",
    "model.attack(feat, adj, label, train_mask, n_perturbations=10)\n",
    "modified_adj = model.modified_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_sparse\n",
    "\n",
    "def normalize_adj_tensor(adj, sparse=False):\n",
    "    \"\"\"Normalize adjacency tensor matrix.\n",
    "    \"\"\"\n",
    "    device = adj.device\n",
    "    if sparse:\n",
    "        # warnings.warn('If you find the training process is too slow, you can uncomment line 207 in deeprobust/graph/utils.py. Note that you need to install torch_sparse')\n",
    "        # TODO if this is too slow, uncomment the following code,\n",
    "        # but you need to install torch_scatter\n",
    "        return normalize_sparse_tensor(adj)\n",
    "        adj = to_scipy(adj)\n",
    "        mx = normalize_adj(adj)\n",
    "        return sparse_mx_to_torch_sparse_tensor(mx).to(device)\n",
    "    else:\n",
    "        mx = adj + torch.eye(adj.shape[0]).to(device)\n",
    "        rowsum = mx.sum(1)\n",
    "        r_inv = rowsum.pow(-1/2).flatten()\n",
    "        r_inv[torch.isinf(r_inv)] = 0.\n",
    "        r_mat_inv = torch.diag(r_inv)\n",
    "        mx = r_mat_inv @ mx\n",
    "        mx = mx @ r_mat_inv\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_modified_adj(complement, A, M):\n",
    "    m = torch.zeros((A.shape[0], A.shape[0])).to(device)\n",
    "    tril_indices = torch.tril_indices(row=A.shape[0], col=A.shape[0], offset=-1)\n",
    "    m[tril_indices[0], tril_indices[1]] = M\n",
    "    m = m + m.t()\n",
    "    modified_adj = complement * m + A\n",
    "    return modified_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [2485, 2485] cannot be broadcast to indexing result of shape [3086370]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/u/nyw6dh/HCDM/MismatchBatch/main.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m complement \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mones_like(adj) \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39meye(adj\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39mto(device) \u001b[39m-\u001b[39m A) \u001b[39m-\u001b[39m A\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m t:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     A_p \u001b[39m=\u001b[39m get_modified_adj(complement, A, M)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m     pred \u001b[39m=\u001b[39m θ(X, A_p)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     L \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(pred, T)\n",
      "\u001b[1;32m/u/nyw6dh/HCDM/MismatchBatch/main.ipynb Cell 13\u001b[0m in \u001b[0;36mget_modified_adj\u001b[0;34m(complement, A, M)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros((A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], A\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m tril_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtril_indices(row\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], col\u001b[39m=\u001b[39mA\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], offset\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m m[tril_indices[\u001b[39m0\u001b[39m], tril_indices[\u001b[39m1\u001b[39m]] \u001b[39m=\u001b[39m M\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m m \u001b[39m=\u001b[39m m \u001b[39m+\u001b[39m m\u001b[39m.\u001b[39mt()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bgroupml02.cs.virginia.edu/u/nyw6dh/HCDM/MismatchBatch/main.ipynb#X21sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m modified_adj \u001b[39m=\u001b[39m complement \u001b[39m*\u001b[39m m \u001b[39m+\u001b[39m A\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [2485, 2485] cannot be broadcast to indexing result of shape [3086370]"
     ]
    }
   ],
   "source": [
    "import deeprutils\n",
    "# ATTACK\n",
    "A = adj\n",
    "ε = 1000\n",
    "X = feat\n",
    "T = label\n",
    "surrogate_lr = 1e-3\n",
    "epochs = 30\n",
    "\n",
    "M = torch.zeros_like(A).float().to(device)\n",
    "θ = GCN(nfeat=X.shape[1], nclass=T.max().item()+1, nhid=32, lr=surrogate_lr, device=device).to(device)\n",
    "\n",
    "t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "A_p = torch.zeros_like(A).to(device).requires_grad_(True) # Initialize modified adj\n",
    "complement = (torch.ones_like(adj) - torch.eye(adj.shape[0]).to(device) - A) - A\n",
    "\n",
    "for epoch in t:\n",
    "    A_p = get_modified_adj(complement, A, M)\n",
    "    pred = θ(X, A_p)\n",
    "    L = F.cross_entropy(pred, T)\n",
    "    A_grad = torch.autograd.grad(L, A_p)[0]\n",
    "    lr = 200 / np.sqrt(epoch + 1)\n",
    "    M.data.add_(lr * A_grad)\n",
    "    deeprutils.projection(ε, A_p)\n",
    "\n",
    "    t.set_postfix({\n",
    "        \"loss\": L.item(),\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'deeprutils' from '/u/nyw6dh/HCDM/MismatchBatch/deeprutils.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import deeprutils\n",
    "importlib.reload(deeprutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeprutils\n",
    "\n",
    "def attack(self, ori_features, adj, labels, idx_train, n_perturbations, epochs=200, **kwargs):\n",
    "    complement = (torch.ones_like(adj) - torch.eye(adj.shape[0]).to(self.device) - adj) - adj\n",
    "    def get_modified_adj(ori_adj, adj_changes):\n",
    "        m = torch.zeros((adj.shape[0], adj.shape[0])).to(device)\n",
    "        tril_indices = torch.tril_indices(row=adj.shape[0], col=adj.shape[0], offset=-1)\n",
    "        m[tril_indices[0], tril_indices[1]] = adj_changes\n",
    "        m = m + m.t()\n",
    "        modified_adj = complement * m + ori_adj\n",
    "        return modified_adj\n",
    "    \n",
    "    victim_model = get_gcn()\n",
    "    victim_model.eval()\n",
    "    adj_changes = torch.zeros_like(adj).to(device).requires_grad_(True)\n",
    "\n",
    "    for t in tqdm(range(epochs)):\n",
    "        modified_adj = get_modified_adj(adj, adj_changes)\n",
    "        adj_norm = deeprutils.normalize_adj_tensor(modified_adj)\n",
    "        output = victim_model(ori_features, adj_norm)\n",
    "        # loss = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "        loss = F.cross_entropy(output[idx_train], labels[idx_train])\n",
    "        adj_grad = torch.autograd.grad(loss, adj_changes)[0]\n",
    "\n",
    "        lr = 200 / np.sqrt(t+1)\n",
    "        adj_changes.data.add_(lr * adj_grad)\n",
    "\n",
    "        deeprutils.projection(n_perturbations, adj_changes)\n",
    "\n",
    "    self.random_sample(ori_adj, ori_features, labels, idx_train, n_perturbations)\n",
    "    self.modified_adj = self.get_modified_adj(ori_adj).detach()\n",
    "    self.check_adj_tensor(self.modified_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attack(model, dataloader: DataLoader, epochs: int):\n",
    "surrogate = get_gcn()\n",
    "surrogate.train()\n",
    "optimizer = torch.optim.Adam(surrogate.parameters(), 1e-3)\n",
    "t = tqdm(range(30), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "t.set_description(\"Attacking training\")\n",
    "loss = torch.tensor(0)\n",
    "for _ in t:\n",
    "    for feats, adjs, labels, train_masks in clean_dataloader:\n",
    "        pred = model(feats, adjs)\n",
    "        mask = train_masks.flatten()\n",
    "        loss = F.cross_entropy(pred.flatten(end_dim=1)[mask], labels.flatten(end_dim=1)[mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_postfix({\"loss\": round(loss.item(), 2)})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_c116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
