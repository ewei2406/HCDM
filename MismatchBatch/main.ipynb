{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "import utils\n",
    "import export\n",
    "\n",
    "# Setup\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--method', type=str, default='SLL', choices=[\n",
    "    'CER', 'REF', 'SLL', 'SLL_G'\n",
    "])\n",
    "parser.add_argument('--budget_pct', type=float, default=0.25)\n",
    "parser.add_argument('--g0_method', type=str, default='random', choices=[\n",
    "  'random', # randomly distribution of g0\n",
    "  'large_cluster', # a random node and [g0_size] of its neighbors are in g0\n",
    "  'many_clusters', # 10 random nodes and [g0_size] of their neighbors are in g0\n",
    "  ])\n",
    "parser.add_argument('--g0_size', type=float, default=0.2)\n",
    "parser.add_argument('--lr', type=float, default=10)\n",
    "parser.add_argument('--T_s', type=int, default=1263)\n",
    "parser.add_argument('--T_u', type=int, default=-1)\n",
    "parser.add_argument('--dataset', type=str, default='cora', choices=[\n",
    "    'Cora', 'Cora-ML', 'Citeseer', 'Pubmed', 'Polblogs', 'ACM', 'BlogCatalog', 'Flickr', 'UAI'\n",
    "])\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.25)\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_neighbor_subgraphs(adj: torch.Tensor, size: int, n: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns a tensor ~ `(n * size)` of node indices for `n` supgraphs of size `size`\n",
    "    \"\"\"\n",
    "    res = torch.zeros([n, size], dtype=torch.long)\n",
    "\n",
    "    for i in range(n):\n",
    "        out = []\n",
    "        stack = [random.randint(0, adj.shape[0] - 1)]\n",
    "        while len(out) < size:\n",
    "            if len(stack) == 0:\n",
    "                stack.append(random.randint(0, adj.shape[0] - 1))\n",
    "            curNode = stack.pop()\n",
    "            if curNode not in out:\n",
    "                out.append(curNode)\n",
    "                children = adj[curNode].nonzero().t()[0].cpu().tolist()\n",
    "                stack = children + stack\n",
    "        res[i] = torch.tensor(out)\n",
    "\n",
    "    return res\n",
    "\n",
    "def get_rand_subgraphs(adj, size: int, n: int) -> torch.Tensor:\n",
    "    return torch.randint(adj.shape[0], [n, size], dtype=torch.long)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n",
      "Selecting 1 largest connected components\n"
     ]
    }
   ],
   "source": [
    "num_samples = 1024\n",
    "sample_size = 64\n",
    "adj, feat, label, train_mask, val_mask, test_mask = utils.load_data(args.dataset, args.seed, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples(A, num_samples, sample_size):\n",
    "    s = get_neighbor_subgraphs(A.cpu(), sample_size, num_samples)\n",
    "\n",
    "    temp = A[s]\n",
    "    subgraph_feats = feat[s]\n",
    "    subgraph_adjs = torch.zeros(num_samples, sample_size, sample_size).to(device)\n",
    "    subgraph_labels = label[s]\n",
    "    subgraph_train_masks = train_mask[s]\n",
    "    for i in range(num_samples):\n",
    "        subgraph_adjs[i] = temp[i][:,s[i]]\n",
    "\n",
    "    return subgraph_feats, subgraph_adjs, subgraph_labels, subgraph_train_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "subgraph_feats, subgraph_adjs, subgraph_labels, subgraph_train_masks = get_samples(adj, num_samples, sample_size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import Sequential, DenseGCNConv\n",
    "from torch.nn import Linear, ReLU\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def get_gcn(hid: int=64):\n",
    "    return Sequential('x, adj', [\n",
    "        (DenseGCNConv(feat.shape[1], hid), 'x, adj -> x'),\n",
    "        ReLU(inplace=True),\n",
    "        (DenseGCNConv(hid, hid), 'x, adj -> x'),\n",
    "        ReLU(inplace=True),\n",
    "        Linear(hid, int(label.max()) + 1),\n",
    "    ]).to(device)\n",
    "\n",
    "def train(model, dataloader: DataLoader, epochs: int):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "    t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Model training\")\n",
    "    loss = torch.tensor(0)\n",
    "    for _ in t:\n",
    "        for feats, adjs, labels, train_masks in dataloader:\n",
    "            pred = model(feats, adjs)\n",
    "            mask = train_masks.flatten()\n",
    "            loss = F.cross_entropy(pred.flatten(end_dim=1)[mask], labels.flatten(end_dim=1)[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
    "\n",
    "def train_adj(model, feat, adj, label, train_mask, epochs: int):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n",
    "    t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Model training\")\n",
    "    loss = torch.tensor(0)\n",
    "    for _ in t:\n",
    "        pred = model(feat, adj)\n",
    "        loss = F.cross_entropy(pred.squeeze()[train_mask], label[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_postfix({\"loss\": round(loss.item(), 2)})\n",
    "\n",
    "def eval_adj(model, adj, test_mask):\n",
    "    model.eval()\n",
    "    pred = model(feat, adj)\n",
    "    acc = ((pred.argmax(dim=2).squeeze() == label)[test_mask].sum() / test_mask.sum()).item()\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training: 100%|██████████| 100/100 [00:24<00:00,  4.03it/s, loss=0]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  77.72%\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader(adj, num_samples, sample_size, batch_size):\n",
    "    subgraph_feats, subgraph_adjs, subgraph_labels, subgraph_train_masks = get_samples(adj, num_samples, sample_size)\n",
    "    return DataLoader(\n",
    "    TensorDataset(subgraph_feats, subgraph_adjs, subgraph_labels, subgraph_train_masks), \n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "gcn = get_gcn()\n",
    "train(gcn, get_dataloader(adj, num_samples, sample_size, 128), 100)\n",
    "eval_adj(gcn, adj, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeprobust.graph.defense import GCN\n",
    "import deeprutils\n",
    "# ATTACK\n",
    "\n",
    "def PGD_Attack(A, X, ε, T, train_mask, val_mask, surrogate_lr=1e-2, epochs=30):\n",
    "    # A = adj\n",
    "    # ε = int(0.5 * adj.sum().item())\n",
    "    # X = feat\n",
    "    # T = label\n",
    "    # surrogate_lr = 1e-2\n",
    "    # epochs = 30\n",
    "\n",
    "    M = torch.zeros_like(A).float().to(device)\n",
    "    θ = GCN(nfeat=X.shape[1], nclass=T.max().item()+1, nhid=32, lr=surrogate_lr, device=device).to(device)\n",
    "    # θ.fit(X, A, T, train_mask, val_mask, train_iters=100)\n",
    "\n",
    "    t = tqdm(range(epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    A_p = torch.zeros_like(A).to(device).requires_grad_(True) # Initialize modified adj\n",
    "\n",
    "    for epoch in t:\n",
    "        pred = θ(X, A_p)\n",
    "        L = F.cross_entropy(pred.squeeze(), T)\n",
    "        A_grad = torch.autograd.grad(L, A_p)[0]\n",
    "        lr = 1 / np.sqrt(epoch + 1)\n",
    "        M.data.add_(lr * A_grad)\n",
    "        M = utils.truncate(M, A)\n",
    "        M = utils.projection(M, ε)\n",
    "        A_p = utils.xor(A, utils.discretize(M)).to(device).requires_grad_(True)\n",
    "        θ.fit(X, A_p, T, train_mask, val_mask, train_iters=1)\n",
    "\n",
    "        t.set_postfix({\n",
    "            \"loss\": L.item(),\n",
    "        })\n",
    "\n",
    "    best = torch.tensor([0])\n",
    "    max_loss = -10000\n",
    "    for i in range(10):\n",
    "        A_p = utils.xor(A, utils.discretize(M)).to(device).requires_grad_(True)\n",
    "        pred = θ(X, A_p)\n",
    "        L = F.cross_entropy(pred.squeeze(), T)\n",
    "\n",
    "        if L.item() > max_loss:\n",
    "            max_loss = L.item()\n",
    "            best = A_p\n",
    "\n",
    "    best.requires_grad_(False)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Partition_PGD(k, A, X, ε, T, train_mask, val_mask, surrogate_lr=1e-2, epochs=30):\n",
    "    size = int(A.shape[0] / k)\n",
    "\n",
    "    batched_A_p = A.clone()\n",
    "\n",
    "    for i in range(k):\n",
    "        sub_A = A[size * i : size * (i + 1)][:,size * i : size * (i + 1)]\n",
    "        sub_X = X[size * i : size * (i + 1)]\n",
    "        sub_T = T[size * i : size * (i + 1)]\n",
    "        sub_tmask = train_mask[size * i : size * (i + 1)]\n",
    "        sub_vmask = val_mask[size * i : size * (i + 1)]\n",
    "\n",
    "        sub_best = PGD_Attack(sub_A, sub_X, ε / k, sub_T, sub_tmask, sub_vmask, surrogate_lr=surrogate_lr, epochs=epochs)\n",
    "\n",
    "        batched_A_p[size * i : size * (i + 1)][:,size * i : size * (i + 1)] = sub_best\n",
    "    \n",
    "    return batched_A_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Batch_PGD(size, n, A, X, ε, T, train_mask, val_mask, surrogate_lr=1e-2, epochs=30):\n",
    "    batched_A_p = A.clone()\n",
    "\n",
    "    for i in range(n):\n",
    "        indices = torch.randperm(A.shape[0])[:size]\n",
    "        sub_A = A[indices][:,indices]\n",
    "        sub_X = X[indices]\n",
    "        sub_T = T[indices]\n",
    "        sub_tmask = train_mask[indices]\n",
    "        sub_vmask = val_mask[indices]\n",
    "\n",
    "        sub_best = PGD_Attack(sub_A, sub_X, ε / n, sub_T, sub_tmask, sub_vmask, surrogate_lr=surrogate_lr, epochs=epochs)\n",
    "\n",
    "        batched_A_p[indices][:,indices] = sub_best\n",
    "    \n",
    "    return batched_A_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget = int(0.5 * adj.sum().item())\n",
    "PGD_Attack(adj, feat, budget, label, train_mask, val_mask)\n",
    "test = get_gcn()\n",
    "train_adj(test, feat, batched_A_p, label, train_mask, 100)\n",
    "eval_adj(test, batched_A_p, test_mask)\n",
    "\n",
    "\n",
    "b_best = Batch_PGD(sample_size, num_samples, adj, feat, budget, label, train_mask, val_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model training: 100%|██████████| 100/100 [00:02<00:00, 35.40it/s, loss=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  45.67%\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [04:46<?, ?it/s]\n",
      "Model training: 100%|██████████| 100/100 [00:18<00:00,  5.50it/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  67.15%\n"
     ]
    }
   ],
   "source": [
    "gcn = get_gcn()\n",
    "train(gcn, get_dataloader(best, num_samples, sample_size, 128), 100)\n",
    "eval_adj(gcn, best, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_c116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
