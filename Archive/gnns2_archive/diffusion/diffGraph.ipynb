{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion Model test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "import dgl\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import selection\n",
    "import argparse\n",
    "import schedules\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--subgraph_size', type=int, default=16)\n",
    "parser.add_argument('--num_save', type=int, default=50)\n",
    "parser.add_argument('--file', type=str, default=\"./out.csv\")\n",
    "parser.add_argument('--batch_size', type=int, default=256)\n",
    "parser.add_argument('--timesteps', type=int, default=400)\n",
    "parser.add_argument('--epochs', type=int, default=5)\n",
    "parser.add_argument('--method', type=str, default='walk', choices=[\n",
    "    'walk', 'rand', 'cluster'\n",
    "])\n",
    "parser.add_argument('--dataset', type=str, default='cora', choices=[\n",
    "    'cora', 'chameleon', 'BlogCatalog', 'flickr'\n",
    "])\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if device != 'cpu': torch.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n",
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "if args.dataset == 'cora': \n",
    "    dataset = dgl.data.CoraGraphDataset()\n",
    "    g = dataset[0]\n",
    "if args.dataset == 'chameleon':\n",
    "    import loadSNAP\n",
    "    import utils\n",
    "    graph = dgl.from_networkx(loadSNAP.load_graph(\"chameleon/musae_chameleon_edges.csv\"))\n",
    "\n",
    "    # Load labels\n",
    "    labels = torch.tensor(loadSNAP.load_labels(\"chameleon/musae_chameleon_target.csv\"))\n",
    "    labels = utils.discretize(labels, 5)\n",
    "\n",
    "    graph.ndata['label'] = labels\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Load feats\n",
    "    rawFeat = loadSNAP.load_features(\"chameleon/musae_chameleon_features.json\")\n",
    "    feat = torch.zeros((graph.num_nodes(), 4000))\n",
    "    for key in rawFeat:\n",
    "        feat[key] = utils.idx_to_bool(torch.tensor(rawFeat[key]), 4000)\n",
    "    graph.ndata['feat'] = feat\n",
    "    feat = feat.to(device)\n",
    "\n",
    "    # Train test split\n",
    "    train_mask = torch.bernoulli(torch.full([graph.num_nodes()], 0.3)).bool()\n",
    "    graph.ndata['train_mask'] = train_mask\n",
    "    graph.ndata['test_mask'] = ~train_mask\n",
    "    adj = graph.adj().to_dense()\n",
    "    g = graph\n",
    "else:\n",
    "    import dataloader\n",
    "    graph = dataloader.load_DGL(args.dataset)\n",
    "    feat = graph.ndata['feat'].to(device)\n",
    "    labels = graph.ndata['label'].to(device)\n",
    "    adj = graph.adj().to_dense()\n",
    "    g = graph\n",
    "\n",
    "subgraph_size = args.subgraph_size\n",
    "batch_size = args.batch_size\n",
    "timesteps = args.timesteps\n",
    "\n",
    "if args.method == 'walk': method = selection.subgraph_node2vec_random_walk\n",
    "if args.method == 'rand': method = selection.subgraph_random\n",
    "if args.method == 'cluster': method = selection.subgraph_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "channels = 1\n",
    "for i in range(batch_size * 50):\n",
    "    dataset.append({\"pixel_values\" : (method(subgraph_size, g).adj().to_dense().unsqueeze(0) * 2) - 1})\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "from unet import Unet\n",
    "import torch\n",
    "\n",
    "model = Unet(\n",
    "    dim=subgraph_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4, 8)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 50/50 [00:06<00:00,  8.11it/s, loss=0.256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 50/50 [00:05<00:00,  9.45it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 50/50 [00:05<00:00,  8.66it/s, loss=0.1]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 50/50 [00:06<00:00,  8.20it/s, loss=0.0737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch: 100%|██████████| 50/50 [00:06<00:00,  7.77it/s, loss=0.0704]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import unet\n",
    "importlib.reload(unet)\n",
    "\n",
    "unet.trainUnet(\n",
    "    model=model,\n",
    "    epochs=args.epochs,\n",
    "    dataloader=dataloader,\n",
    "    optimizer=Adam(model.parameters(), lr=1e-3),\n",
    "    device=device,\n",
    "    timesteps=timesteps,\n",
    "    scheduler=schedules.linear_beta_schedule\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sampling loop time step: 100%|██████████| 400/400 [00:06<00:00, 65.72it/s]\n"
     ]
    }
   ],
   "source": [
    "import sampling\n",
    "samples = sampling.sample(\n",
    "    model, \n",
    "    image_size=subgraph_size, \n",
    "    batch_size=batch_size, \n",
    "    channels=channels, \n",
    "    scheduler=schedules.linear_beta_schedule, \n",
    "    timesteps=timesteps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1MAAACeCAYAAADXJlBrAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAFNElEQVR4nO3d0Y0bNxRAUa3hKlJFmghSgatMBYGbSBUpw/JHvj2aXFFccnTOrwGJWD2N94LAvo/7/X4DAADg//ny2QcAAADYkZgCAAAIxBQAAEAgpgAAAAIxBQAAEHw9+sc/vnx7+Kf+/v73n4dv8udvv58+ENfy/cdfHzPfz8zyrNkze7udm1s4suKzFo6sOLN+P+DIr2bWzRQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACA6X9o5aXmYJGrPMnNlRzD6jeNayGzPLLGaNV3EzBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAACCw6W9FvKyGzPLVZlbdjNzibrZ5xEzwqu4mQIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAweHS3jNmLkGzuI/dmFlG8axlNzMX8q42s2fOc7v5HsEVuJkCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAMHTS3tHueriPq7LzLIj88ZKrvqMXO08wOu4mQIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAwTJLe2cu7ttxASDjjPpszSw7MifsxswCK3MzBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAACCZZb2nnHl5aa7nptjFvsCsAL/R8BruJkCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAMFWS3vP2HXh3K7n5nmrLfY9+1owy8i5tbiUd2WuOeL3g87NFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgCAAAILre0F67IMlKuauTczlxu7bs0j88DnrPic/ZK3EwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAABF8/+wD8x0ZpnjV7PswsI5yZkTOzNvK1zO1a3v3zMLM8y3P2tdxMAQAABGIKAAAgEFMAAACBmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIDgLZf2zl4mNur93m0JGp/HzDLLyBkxt+zGzLKK2c/ZK3EzBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGYAgAACMQUAABAIKYAAACCt1zaO3uZ2MyFe5apMYJZY5ZRszbytcw/s5g1drPjrL36O+RmCgAAIBBTAAAAgZgCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEb7m0d0UW97Ebs8YsI2fNYl+OrPiZzXy/swuyHzHXzLLCd9bNFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAIBBTAAAAgZgCAAAILO3dyGqL+yzlYyVmllEs9n1fV/5ZmzV2s8vMupkCAAAIxBQAAEAgpgAAAAIxBQAAEIgpAACAQEwBAAAEYgoAACAQUwAAAIGlvW9o5rJJGMHMsprVFvvCSswsj+yykPcMN1MAAACBmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIBATAEAAARiCgAAILC09w3NXDb5/cfj88AjZpYdmVtWMmoBqpllhF0W8p7hZgoAACAQUwAAAIGYAgAACMQUAABAIKYAAAACMQUAABCIKQAAgEBMAQAABB/3+/2zzwAAALAdN1MAAACBmAIAAAjEFAAAQCCmAAAAAjEFAAAQiCkAAIDgJ/mCX4EqjLAfAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1080x1080 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def showPics(num, imgs):\n",
    "    plt.figure()\n",
    "    f, axarr = plt.subplots(1, num, figsize=(15,15))\n",
    "    rand = random.sample(range(len(imgs)), num)\n",
    "    for i, imgIdx in enumerate(rand):\n",
    "        # print(i['pixel_values'][0].squeeze().shape)\n",
    "        axarr[i].imshow(imgs[imgIdx])\n",
    "        axarr[i].axis('off')\n",
    "        if i == num - 1: break\n",
    "\n",
    "\n",
    "\n",
    "def mirror(A: np.ndarray):\n",
    "    return np.tril(A) + np.triu(A.T, 1)\n",
    "\n",
    "train_data = [x[\"pixel_values\"].squeeze() for x in dataset]\n",
    "pred_data = [mirror(x) for x in (samples[-1].squeeze() > 0)]\n",
    "\n",
    "comparisons = 5\n",
    "showPics(comparisons, train_data)\n",
    "showPics(comparisons, pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "def calculateStats(adj: torch.tensor, label: str, index: int):\n",
    "    out = {}\n",
    "    adj = mirror(adj > 0)\n",
    "    nxg = nx.from_numpy_matrix(adj)\n",
    "\n",
    "    out['label'] = label\n",
    "    out['index'] = index\n",
    "    out['num_edges'] = adj.sum()\n",
    "    out['num_nodes'] = adj.shape[0]\n",
    "    out['density'] = adj.sum() / adj.shape[0]\n",
    "    out['num_triangles'] = int(sum(nx.triangles(nxg).values()) / 3)\n",
    "    out['max_diameter'] = max([max(j.values()) for (i,j) in nx.shortest_path_length(nxg)])\n",
    "\n",
    "    out.update(vars(args))\n",
    "    out.pop(\"file\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'density': 2.545869140625, 'max_diameter': 7.36265625}\n",
      "{'density': 1.264892578125, 'max_diameter': 4.5703125}\n"
     ]
    }
   ],
   "source": [
    "def avg(stats):\n",
    "    avg = {\n",
    "        'density': 0,\n",
    "        'max_diameter': 0\n",
    "    }\n",
    "    ct = 0\n",
    "    for stat in stats:\n",
    "        ct += 1\n",
    "        avg['density'] += stat.get('density') or 0\n",
    "        avg['max_diameter'] += stat.get('max_diameter') or 0\n",
    "    \n",
    "    for key in avg:\n",
    "        avg[key] /= ct\n",
    "    \n",
    "    return avg\n",
    "\n",
    "print(avg([calculateStats(train_data[x].numpy(), \"train\", i) for x in range(len(train_data))]))\n",
    "print(avg([calculateStats(pred_data[x], \"test\", i) for x in range(len(pred_data))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculateStats(pred_data[2], \"pred\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'train',\n",
       " 'index': 12799,\n",
       " 'num_edges': 44,\n",
       " 'num_nodes': 16,\n",
       " 'density': 2.75,\n",
       " 'num_triangles': 6,\n",
       " 'max_diameter': 6,\n",
       " 'seed': 123,\n",
       " 'subgraph_size': 16,\n",
       " 'num_save': 50,\n",
       " 'batch_size': 256,\n",
       " 'timesteps': 400,\n",
       " 'epochs': 5,\n",
       " 'method': 'walk',\n",
       " 'dataset': 'cora'}"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculateStats(train_data[2].numpy(), \"train\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainStats = [calculateStats(train_data[x].numpy(), \"train\", i) for x in range(len(train_data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'train',\n",
       " 'index': 12799,\n",
       " 'num_edges': 32,\n",
       " 'num_nodes': 16,\n",
       " 'density': 2.0,\n",
       " 'num_triangles': 0,\n",
       " 'max_diameter': 10,\n",
       " 'seed': 123,\n",
       " 'subgraph_size': 16,\n",
       " 'num_save': 50,\n",
       " 'batch_size': 256,\n",
       " 'timesteps': 400,\n",
       " 'epochs': 5,\n",
       " 'method': 'walk',\n",
       " 'dataset': 'cora'}"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainStats[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'density': 2.545869140625, 'max_diameter': 7.36265625}"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def avg(data):\n",
    "    stats = [calculateStats(train_data[x].numpy(), \"train\", i) for x in range(len(train_data))]\n",
    "    avg = {\n",
    "        'density': 0,\n",
    "        'max_diameter': 0\n",
    "    }\n",
    "    ct = 0\n",
    "    for stat in stats:\n",
    "        ct += 1\n",
    "        avg['density'] += stat.get('density') or 0\n",
    "        avg['max_diameter'] += stat.get('max_diameter') or 0\n",
    "    \n",
    "    for key in avg:\n",
    "        avg[key] /= ct\n",
    "    \n",
    "    return avg\n",
    "\n",
    "avg(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import export\n",
    "\n",
    "for i, idx in enumerate(random.sample(range(len(train_data)), args.num_save)):\n",
    "    export.saveData(args.file, calculateStats(train_data[idx].numpy(), \"train\", i))\n",
    "\n",
    "for i, idx in enumerate(random.sample(range(len(pred_data)), args.num_save)):\n",
    "    export.saveData(args.file, calculateStats(pred_data[idx], \"pred\", i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_c116",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 (main, Mar 31 2022, 08:41:55) [GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
