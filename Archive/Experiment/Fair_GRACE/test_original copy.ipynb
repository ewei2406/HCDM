{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputed weights for weighting-based debiasing UGE-W Loaded\n",
      "Creating DGL graph...\n",
      "Finished data loading and preprocessing.\n",
      "  NumNodes: 9992\n",
      "  NumEdges: 2010410\n",
      "  NumFeats: 18\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from dataset import load\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "from eval import label_classification, eval_unbiasedness_movielens\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataname', type=str, default='movielens')\n",
    "parser.add_argument('--gpu', type=int, default=1)\n",
    "parser.add_argument('--split', type=str, default='random')\n",
    "parser.add_argument('--debias_method', type=str, default='uge-c', choices=['uge-r', 'uge-w', 'uge-c', 'none'], help='debiasing method to apply')\n",
    "parser.add_argument('--debias_attr', type=str, default='age', help='sensitive attribute to be debiased')\n",
    "parser.add_argument('--reg_weight', type=float, default=2, help='weight for the regularization based debiasing term')  \n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=200, help='Number of training periods.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.')\n",
    "parser.add_argument('--wd', type=float, default=1e-5, help='Weight decay.')\n",
    "parser.add_argument('--temp', type=float, default=1.0, help='Temperature.')\n",
    "\n",
    "parser.add_argument('--act_fn', type=str, default='relu')\n",
    "\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=256, help='Hidden layer dim.')\n",
    "parser.add_argument(\"--out_dim\", type=int, default=256, help='Output layer dim.')\n",
    "\n",
    "parser.add_argument(\"--num_layers\", type=int, default=2, help='Number of GNN layers.')\n",
    "parser.add_argument('--der1', type=float, default=0.2, help='Drop edge ratio of the 1st augmentation.')\n",
    "parser.add_argument('--der2', type=float, default=0.2, help='Drop edge ratio of the 2nd augmentation.')\n",
    "parser.add_argument('--dfr1', type=float, default=0.2, help='Drop feature ratio of the 1st augmentation.')\n",
    "parser.add_argument('--dfr2', type=float, default=0.2, help='Drop feature ratio of the 2nd augmentation.')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "if args.gpu != -1 and th.cuda.is_available():\n",
    "    args.device = 'cuda:{}'.format(args.gpu)\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n",
    "# Step 1: Load hyperparameters =================================================================== #\n",
    "lr = args.lr\n",
    "hid_dim = args.hid_dim\n",
    "out_dim = args.out_dim\n",
    "\n",
    "num_layers = args.num_layers\n",
    "act_fn = ({'relu': nn.ReLU(), 'prelu': nn.PReLU()})[args.act_fn]\n",
    "\n",
    "drop_edge_rate_1 = args.der1\n",
    "drop_edge_rate_2 = args.der2\n",
    "drop_feature_rate_1 = args.dfr1\n",
    "drop_feature_rate_2 = args.dfr2\n",
    "\n",
    "temp = args.temp\n",
    "epochs = args.epochs\n",
    "wd = args.wd\n",
    "debias_method = args.debias_method\n",
    "\n",
    "# Step 2: Prepare data =================================================================== #\n",
    "if debias_method in ['uge-w', 'uge-c']:\n",
    "    dataset = '{}_debias_{}'.format(args.dataname, args.debias_attr)\n",
    "else:\n",
    "    dataset = args.dataname\n",
    "\n",
    "graph = load(dataset)\n",
    "in_dim = graph.ndata['feat'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group finished.\n",
      "  attr_comb_group_num: 242\n",
      "  nobias_attr_comb_group_num: 43\n"
     ]
    }
   ],
   "source": [
    "from dataset import SENSITIVE_ATTR_DICT  # predefined sensitive attributes for different datasets\n",
    "from dataset import DATA_FOLDER\n",
    "import pandas as pd\n",
    "\n",
    "SENSITIVE_ATTR_DICT = {\n",
    "    'movielens': ['gender', 'occupation', 'age'],\n",
    "    'pokec': ['gender', 'region', 'AGE'],\n",
    "    'pokec-z': ['gender', 'region', 'AGE'],\n",
    "    'pokec-n': ['gender', 'region', 'AGE'],\n",
    "}\n",
    "# Group nodes\n",
    "debias_attr = args.debias_attr\n",
    "attribute_list = SENSITIVE_ATTR_DICT[args.dataname]\n",
    "\n",
    "non_sens_attr_ls = [attr for attr in attribute_list if attr!=debias_attr]\n",
    "non_sens_attr_idx = [i for i in range(len(attribute_list)) if attribute_list[i]!=debias_attr]\n",
    "\n",
    "attribute_file = '{}/{}_node_attribute.csv'.format(DATA_FOLDER, args.dataname)\n",
    "node_attributes = pd.read_csv(attribute_file)\n",
    "\n",
    "attr_comb_groups = node_attributes.groupby(attribute_list)\n",
    "nobias_comb_groups = node_attributes.groupby(non_sens_attr_ls)\n",
    "\n",
    "attr_comb_groups_map = {tuple(group[1].iloc[0]):list(group[1].index) \n",
    "                        for group in attr_comb_groups}\n",
    "nobias_attr_comb_groups_map = {tuple(group[1].iloc[0][non_sens_attr_ls]):list(group[1].index) \n",
    "                            for group in nobias_comb_groups}\n",
    "\n",
    "print ('Group finished.')\n",
    "print ('  attr_comb_group_num:', len(attr_comb_groups_map.keys()))\n",
    "print ('  nobias_attr_comb_group_num:', len(nobias_attr_comb_groups_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tuple(x, index_ls):\n",
    "  return tuple([x[idx] for idx in index_ls])\n",
    "\n",
    "def mem_eff_matmul_mean(mtx1, mtx2):\n",
    "  mtx1_rows = list(mtx1.shape)[0]\n",
    "  if mtx1_rows <= 1000:\n",
    "    return th.mean(th.matmul(mtx1, mtx2))\n",
    "  else:\n",
    "    value_sum = 0\n",
    "    for i in range(mtx1_rows // 1000):\n",
    "      value_sum += th.sum(th.matmul(mtx1[i*1000:(i+1)*1000, :], mtx2))\n",
    "    if mtx1_rows % 1000 != 0:\n",
    "      value_sum += th.sum(th.matmul(mtx1[(i+1)*1000:, :], mtx2))\n",
    "    return value_sum / (list(mtx1.shape)[0] * list(mtx2.shape)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'weight' in graph.edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Graph(num_nodes=9992, num_edges=1617948,\n",
       "       ndata_schemes={}\n",
       "       edata_schemes={'weight': Scheme(shape=(), dtype=torch.float32)}),\n",
       " tensor([[1., 0., 0.,  ..., 1., 1., 0.],\n",
       "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         [1., 0., 0.,  ..., 1., 1., 1.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 1., 0., 0.]]))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "\n",
    "def aug(graph, feat_drop_rate, edge_mask_rate):\n",
    "    n_node = graph.num_nodes()\n",
    "    x = graph.ndata['feat']\n",
    "\n",
    "    edge_mask = mask_edge(graph, edge_mask_rate)\n",
    "    feat = drop_feature(x, feat_drop_rate)\n",
    "\n",
    "    src = graph.edges()[0]\n",
    "    dst = graph.edges()[1]\n",
    "\n",
    "    nsrc = src[edge_mask]\n",
    "    ndst = dst[edge_mask]\n",
    "    weights = graph.edata['weight'][edge_mask]\n",
    "\n",
    "    ng = dgl.graph((nsrc, ndst), num_nodes=n_node)\n",
    "    ng.edata['weight'] = weights\n",
    "    ng = ng.add_self_loop()\n",
    "\n",
    "    return ng, feat\n",
    "\n",
    "def drop_feature(x, drop_prob):\n",
    "    drop_mask = th.empty((x.size(1),),\n",
    "                        dtype=th.float32,\n",
    "                        device=x.device).uniform_(0, 1) < drop_prob\n",
    "    x = x.clone()\n",
    "    x[:, drop_mask] = 0\n",
    "\n",
    "    return x\n",
    "\n",
    "def mask_edge(graph, mask_prob):\n",
    "    E = graph.num_edges()\n",
    "\n",
    "    mask_rates = th.FloatTensor(np.ones(E) * mask_prob)\n",
    "    masks = th.bernoulli(1 - mask_rates)\n",
    "    mask_idx = masks.nonzero().squeeze(1)\n",
    "    return mask_idx\n",
    "\n",
    "dr = 0.2\n",
    "aug(graph, feat_drop_rate=dr, edge_mask_rate=dr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 272640\n",
      "Epoch=000, loss: 9.85, regu_loss: 61.40\n",
      "Epoch=000, loss=11.0743\n",
      "Epoch=001, loss: 9.80, regu_loss: 20.57\n",
      "Epoch=001, loss=10.2113\n",
      "Epoch=002, loss: 9.79, regu_loss: 6.85\n",
      "Epoch=002, loss=9.9246\n",
      "Epoch=003, loss: 9.75, regu_loss: 2.01\n",
      "Epoch=003, loss=9.7904\n",
      "Epoch=004, loss: 9.77, regu_loss: 0.71\n",
      "Epoch=004, loss=9.7833\n",
      "Epoch=005, loss: 9.67, regu_loss: 0.98\n",
      "Epoch=005, loss=9.6945\n",
      "Epoch=006, loss: 9.57, regu_loss: 0.30\n",
      "Epoch=006, loss=9.5752\n",
      "Epoch=007, loss: 9.47, regu_loss: 0.45\n",
      "Epoch=007, loss=9.4816\n",
      "Epoch=008, loss: 9.39, regu_loss: 0.62\n",
      "Epoch=008, loss=9.3977\n",
      "Epoch=009, loss: 9.39, regu_loss: 1.04\n",
      "Epoch=009, loss=9.4154\n",
      "Epoch=010, loss: 9.39, regu_loss: 1.28\n",
      "Epoch=010, loss=9.4110\n",
      "Epoch=011, loss: 9.41, regu_loss: 1.12\n",
      "Epoch=011, loss=9.4339\n",
      "Epoch=012, loss: 9.39, regu_loss: 1.41\n",
      "Epoch=012, loss=9.4142\n",
      "Epoch=013, loss: 9.38, regu_loss: 0.80\n",
      "Epoch=013, loss=9.3991\n",
      "Epoch=014, loss: 9.37, regu_loss: 1.78\n",
      "Epoch=014, loss=9.4095\n",
      "Epoch=015, loss: 9.38, regu_loss: 1.69\n",
      "Epoch=015, loss=9.4108\n",
      "Epoch=016, loss: 9.38, regu_loss: 2.94\n",
      "Epoch=016, loss=9.4347\n",
      "Epoch=017, loss: 9.37, regu_loss: 3.11\n",
      "Epoch=017, loss=9.4287\n",
      "Epoch=018, loss: 9.36, regu_loss: 1.34\n",
      "Epoch=018, loss=9.3889\n",
      "Epoch=019, loss: 9.38, regu_loss: 1.06\n",
      "Epoch=019, loss=9.3991\n",
      "Epoch=020, loss: 9.37, regu_loss: 1.48\n",
      "Epoch=020, loss=9.3959\n",
      "Epoch=021, loss: 9.36, regu_loss: 0.45\n",
      "Epoch=021, loss=9.3647\n",
      "Epoch=022, loss: 9.35, regu_loss: 0.50\n",
      "Epoch=022, loss=9.3631\n",
      "Epoch=023, loss: 9.36, regu_loss: 1.01\n",
      "Epoch=023, loss=9.3776\n",
      "Epoch=024, loss: 9.35, regu_loss: 0.72\n",
      "Epoch=024, loss=9.3692\n",
      "Epoch=025, loss: 9.36, regu_loss: 0.31\n",
      "Epoch=025, loss=9.3666\n",
      "Epoch=026, loss: 9.62, regu_loss: 0.26\n",
      "Epoch=026, loss=9.6234\n",
      "Epoch=027, loss: 9.36, regu_loss: 0.33\n",
      "Epoch=027, loss=9.3634\n",
      "Epoch=028, loss: 9.36, regu_loss: 0.31\n",
      "Epoch=028, loss=9.3614\n",
      "Epoch=029, loss: 9.36, regu_loss: 1.03\n",
      "Epoch=029, loss=9.3762\n",
      "Epoch=030, loss: 9.35, regu_loss: 0.21\n",
      "Epoch=030, loss=9.3557\n",
      "Epoch=031, loss: 9.35, regu_loss: 1.30\n",
      "Epoch=031, loss=9.3807\n",
      "Epoch=032, loss: 9.35, regu_loss: 1.39\n",
      "Epoch=032, loss=9.3774\n",
      "Epoch=033, loss: 9.35, regu_loss: 1.98\n",
      "Epoch=033, loss=9.3860\n",
      "Epoch=034, loss: 9.34, regu_loss: 0.28\n",
      "Epoch=034, loss=9.3481\n",
      "Epoch=035, loss: 9.34, regu_loss: 0.75\n",
      "Epoch=035, loss=9.3535\n",
      "Epoch=036, loss: 9.33, regu_loss: 0.78\n",
      "Epoch=036, loss=9.3478\n",
      "Epoch=037, loss: 9.33, regu_loss: 0.56\n",
      "Epoch=037, loss=9.3442\n",
      "Epoch=038, loss: 9.33, regu_loss: 0.54\n",
      "Epoch=038, loss=9.3446\n",
      "Epoch=039, loss: 9.33, regu_loss: 0.69\n",
      "Epoch=039, loss=9.3418\n",
      "Epoch=040, loss: 9.33, regu_loss: 0.53\n",
      "Epoch=040, loss=9.3400\n",
      "Epoch=041, loss: 9.33, regu_loss: 0.06\n",
      "Epoch=041, loss=9.3321\n",
      "Epoch=042, loss: 9.32, regu_loss: 0.31\n",
      "Epoch=042, loss=9.3304\n",
      "Epoch=043, loss: 9.32, regu_loss: 0.84\n",
      "Epoch=043, loss=9.3357\n",
      "Epoch=044, loss: 9.32, regu_loss: 0.22\n",
      "Epoch=044, loss=9.3280\n",
      "Epoch=045, loss: 9.32, regu_loss: 0.08\n",
      "Epoch=045, loss=9.3186\n",
      "Epoch=046, loss: 9.31, regu_loss: 0.25\n",
      "Epoch=046, loss=9.3180\n",
      "Epoch=047, loss: 9.31, regu_loss: 0.47\n",
      "Epoch=047, loss=9.3228\n",
      "Epoch=048, loss: 9.31, regu_loss: 0.15\n",
      "Epoch=048, loss=9.3116\n",
      "Epoch=049, loss: 9.32, regu_loss: 0.23\n",
      "Epoch=049, loss=9.3295\n",
      "Epoch=050, loss: 9.30, regu_loss: 0.12\n",
      "Epoch=050, loss=9.3033\n",
      "Epoch=051, loss: 9.31, regu_loss: 0.22\n",
      "Epoch=051, loss=9.3168\n",
      "Epoch=052, loss: 9.30, regu_loss: 0.27\n",
      "Epoch=052, loss=9.3024\n",
      "Epoch=053, loss: 9.29, regu_loss: 0.13\n",
      "Epoch=053, loss=9.2899\n",
      "Epoch=054, loss: 9.28, regu_loss: 0.26\n",
      "Epoch=054, loss=9.2856\n",
      "Epoch=055, loss: 9.28, regu_loss: 0.19\n",
      "Epoch=055, loss=9.2802\n",
      "Epoch=056, loss: 9.28, regu_loss: 0.91\n",
      "Epoch=056, loss=9.2942\n",
      "Epoch=057, loss: 9.27, regu_loss: 0.26\n",
      "Epoch=057, loss=9.2719\n",
      "Epoch=058, loss: 9.24, regu_loss: 0.29\n",
      "Epoch=058, loss=9.2430\n",
      "Epoch=059, loss: 9.26, regu_loss: 0.34\n",
      "Epoch=059, loss=9.2690\n",
      "Epoch=060, loss: 9.27, regu_loss: 0.83\n",
      "Epoch=060, loss=9.2896\n",
      "Epoch=061, loss: 9.44, regu_loss: 0.85\n",
      "Epoch=061, loss=9.4585\n",
      "Epoch=062, loss: 9.26, regu_loss: 0.90\n",
      "Epoch=062, loss=9.2757\n",
      "Epoch=063, loss: 9.29, regu_loss: 0.70\n",
      "Epoch=063, loss=9.3038\n",
      "Epoch=064, loss: 9.29, regu_loss: 0.42\n",
      "Epoch=064, loss=9.2950\n",
      "Epoch=065, loss: 9.28, regu_loss: 0.71\n",
      "Epoch=065, loss=9.2897\n",
      "Epoch=066, loss: 9.27, regu_loss: 0.56\n",
      "Epoch=066, loss=9.2821\n",
      "Epoch=067, loss: 9.27, regu_loss: 0.58\n",
      "Epoch=067, loss=9.2791\n",
      "Epoch=068, loss: 9.26, regu_loss: 0.98\n",
      "Epoch=068, loss=9.2783\n",
      "Epoch=069, loss: 9.27, regu_loss: 0.57\n",
      "Epoch=069, loss=9.2796\n",
      "Epoch=070, loss: 9.25, regu_loss: 0.76\n",
      "Epoch=070, loss=9.2637\n",
      "Epoch=071, loss: 9.24, regu_loss: 0.47\n",
      "Epoch=071, loss=9.2473\n",
      "Epoch=072, loss: 9.23, regu_loss: 2.24\n",
      "Epoch=072, loss=9.2781\n",
      "Epoch=073, loss: 9.24, regu_loss: 1.00\n",
      "Epoch=073, loss=9.2580\n",
      "Epoch=074, loss: 9.22, regu_loss: 0.55\n",
      "Epoch=074, loss=9.2268\n",
      "Epoch=075, loss: 9.27, regu_loss: 0.99\n",
      "Epoch=075, loss=9.2942\n",
      "Epoch=076, loss: 9.23, regu_loss: 0.34\n",
      "Epoch=076, loss=9.2321\n",
      "Epoch=077, loss: 9.20, regu_loss: 0.40\n",
      "Epoch=077, loss=9.2086\n",
      "Epoch=078, loss: 9.19, regu_loss: 0.71\n",
      "Epoch=078, loss=9.2065\n",
      "Epoch=079, loss: 9.27, regu_loss: 0.67\n",
      "Epoch=079, loss=9.2819\n",
      "Epoch=080, loss: 9.23, regu_loss: 0.40\n",
      "Epoch=080, loss=9.2421\n",
      "Epoch=081, loss: 9.20, regu_loss: 0.30\n",
      "Epoch=081, loss=9.2017\n",
      "Epoch=082, loss: 9.25, regu_loss: 0.66\n",
      "Epoch=082, loss=9.2602\n",
      "Epoch=083, loss: 9.21, regu_loss: 0.79\n",
      "Epoch=083, loss=9.2283\n",
      "Epoch=084, loss: 9.19, regu_loss: 0.66\n",
      "Epoch=084, loss=9.2044\n",
      "Epoch=085, loss: 9.28, regu_loss: 1.04\n",
      "Epoch=085, loss=9.3044\n",
      "Epoch=086, loss: 9.22, regu_loss: 0.74\n",
      "Epoch=086, loss=9.2330\n",
      "Epoch=087, loss: 9.27, regu_loss: 0.40\n",
      "Epoch=087, loss=9.2740\n",
      "Epoch=088, loss: 9.27, regu_loss: 0.37\n",
      "Epoch=088, loss=9.2804\n",
      "Epoch=089, loss: 9.20, regu_loss: 0.72\n",
      "Epoch=089, loss=9.2190\n",
      "Epoch=090, loss: 9.22, regu_loss: 1.00\n",
      "Epoch=090, loss=9.2377\n",
      "Epoch=091, loss: 9.23, regu_loss: 1.37\n",
      "Epoch=091, loss=9.2616\n",
      "Epoch=092, loss: 9.22, regu_loss: 1.34\n",
      "Epoch=092, loss=9.2456\n",
      "Epoch=093, loss: 9.23, regu_loss: 0.70\n",
      "Epoch=093, loss=9.2432\n",
      "Epoch=094, loss: 9.25, regu_loss: 1.15\n",
      "Epoch=094, loss=9.2736\n",
      "Epoch=095, loss: 9.23, regu_loss: 0.19\n",
      "Epoch=095, loss=9.2306\n",
      "Epoch=096, loss: 9.19, regu_loss: 0.29\n",
      "Epoch=096, loss=9.1957\n",
      "Epoch=097, loss: 9.21, regu_loss: 0.52\n",
      "Epoch=097, loss=9.2187\n",
      "Epoch=098, loss: 9.19, regu_loss: 0.67\n",
      "Epoch=098, loss=9.2011\n",
      "Epoch=099, loss: 9.19, regu_loss: 0.58\n",
      "Epoch=099, loss=9.1986\n",
      "Epoch=100, loss: 9.21, regu_loss: 0.55\n",
      "Epoch=100, loss=9.2175\n",
      "Epoch=101, loss: 9.17, regu_loss: 0.70\n",
      "Epoch=101, loss=9.1834\n",
      "Epoch=102, loss: 9.18, regu_loss: 0.43\n",
      "Epoch=102, loss=9.1871\n",
      "Epoch=103, loss: 9.18, regu_loss: 0.37\n",
      "Epoch=103, loss=9.1833\n",
      "Epoch=104, loss: 9.17, regu_loss: 0.63\n",
      "Epoch=104, loss=9.1792\n",
      "Epoch=105, loss: 9.16, regu_loss: 0.70\n",
      "Epoch=105, loss=9.1739\n",
      "Epoch=106, loss: 9.21, regu_loss: 1.38\n",
      "Epoch=106, loss=9.2342\n",
      "Epoch=107, loss: 9.18, regu_loss: 0.16\n",
      "Epoch=107, loss=9.1796\n",
      "Epoch=108, loss: 9.17, regu_loss: 0.37\n",
      "Epoch=108, loss=9.1731\n",
      "Epoch=109, loss: 9.27, regu_loss: 2.41\n",
      "Epoch=109, loss=9.3140\n",
      "Epoch=110, loss: 9.17, regu_loss: 0.32\n",
      "Epoch=110, loss=9.1761\n",
      "Epoch=111, loss: 9.23, regu_loss: 1.80\n",
      "Epoch=111, loss=9.2629\n",
      "Epoch=112, loss: 9.16, regu_loss: 0.51\n",
      "Epoch=112, loss=9.1717\n",
      "Epoch=113, loss: 9.18, regu_loss: 0.70\n",
      "Epoch=113, loss=9.1928\n",
      "Epoch=114, loss: 9.17, regu_loss: 0.73\n",
      "Epoch=114, loss=9.1856\n",
      "Epoch=115, loss: 9.23, regu_loss: 0.44\n",
      "Epoch=115, loss=9.2354\n",
      "Epoch=116, loss: 9.18, regu_loss: 0.68\n",
      "Epoch=116, loss=9.1960\n",
      "Epoch=117, loss: 9.18, regu_loss: 0.48\n",
      "Epoch=117, loss=9.1861\n",
      "Epoch=118, loss: 9.17, regu_loss: 0.60\n",
      "Epoch=118, loss=9.1804\n",
      "Epoch=119, loss: 9.16, regu_loss: 0.49\n",
      "Epoch=119, loss=9.1721\n",
      "Epoch=120, loss: 9.16, regu_loss: 0.60\n",
      "Epoch=120, loss=9.1737\n",
      "Epoch=121, loss: 9.18, regu_loss: 0.34\n",
      "Epoch=121, loss=9.1913\n",
      "Epoch=122, loss: 9.46, regu_loss: 0.42\n",
      "Epoch=122, loss=9.4648\n",
      "Epoch=123, loss: 9.18, regu_loss: 0.18\n",
      "Epoch=123, loss=9.1822\n",
      "Epoch=124, loss: 9.26, regu_loss: 1.03\n",
      "Epoch=124, loss=9.2761\n",
      "Epoch=125, loss: 9.19, regu_loss: 1.01\n",
      "Epoch=125, loss=9.2079\n",
      "Epoch=126, loss: 9.19, regu_loss: 0.88\n",
      "Epoch=126, loss=9.2099\n",
      "Epoch=127, loss: 9.19, regu_loss: 0.76\n",
      "Epoch=127, loss=9.2018\n",
      "Epoch=128, loss: 9.20, regu_loss: 1.75\n",
      "Epoch=128, loss=9.2325\n",
      "Epoch=129, loss: 9.18, regu_loss: 0.98\n",
      "Epoch=129, loss=9.1976\n",
      "Epoch=130, loss: 9.17, regu_loss: 0.24\n",
      "Epoch=130, loss=9.1729\n",
      "Epoch=131, loss: 9.21, regu_loss: 0.23\n",
      "Epoch=131, loss=9.2173\n",
      "Epoch=132, loss: 9.24, regu_loss: 0.08\n",
      "Epoch=132, loss=9.2394\n",
      "Epoch=133, loss: 9.20, regu_loss: 0.22\n",
      "Epoch=133, loss=9.2036\n",
      "Epoch=134, loss: 9.17, regu_loss: 0.39\n",
      "Epoch=134, loss=9.1732\n",
      "Epoch=135, loss: 9.25, regu_loss: 0.97\n",
      "Epoch=135, loss=9.2699\n",
      "Epoch=136, loss: 9.22, regu_loss: 0.37\n",
      "Epoch=136, loss=9.2245\n",
      "Epoch=137, loss: 9.18, regu_loss: 0.77\n",
      "Epoch=137, loss=9.1959\n",
      "Epoch=138, loss: 9.18, regu_loss: 1.16\n",
      "Epoch=138, loss=9.2028\n",
      "Epoch=139, loss: 9.18, regu_loss: 1.03\n",
      "Epoch=139, loss=9.2054\n",
      "Epoch=140, loss: 9.16, regu_loss: 0.90\n",
      "Epoch=140, loss=9.1803\n",
      "Epoch=141, loss: 9.16, regu_loss: 1.25\n",
      "Epoch=141, loss=9.1882\n",
      "Epoch=142, loss: 9.26, regu_loss: 0.29\n",
      "Epoch=142, loss=9.2673\n",
      "Epoch=143, loss: 9.16, regu_loss: 1.54\n",
      "Epoch=143, loss=9.1898\n",
      "Epoch=144, loss: 9.18, regu_loss: 0.65\n",
      "Epoch=144, loss=9.1968\n",
      "Epoch=145, loss: 9.17, regu_loss: 1.38\n",
      "Epoch=145, loss=9.1962\n",
      "Epoch=146, loss: 9.16, regu_loss: 0.81\n",
      "Epoch=146, loss=9.1747\n",
      "Epoch=147, loss: 9.16, regu_loss: 0.64\n",
      "Epoch=147, loss=9.1749\n",
      "Epoch=148, loss: 9.24, regu_loss: 0.86\n",
      "Epoch=148, loss=9.2564\n",
      "Epoch=149, loss: 9.15, regu_loss: 0.66\n",
      "Epoch=149, loss=9.1674\n",
      "Epoch=150, loss: 9.17, regu_loss: 0.32\n",
      "Epoch=150, loss=9.1728\n",
      "Epoch=151, loss: 9.16, regu_loss: 0.83\n",
      "Epoch=151, loss=9.1764\n",
      "Epoch=152, loss: 9.15, regu_loss: 0.70\n",
      "Epoch=152, loss=9.1680\n",
      "Epoch=153, loss: 9.17, regu_loss: 0.41\n",
      "Epoch=153, loss=9.1831\n",
      "Epoch=154, loss: 9.16, regu_loss: 0.42\n",
      "Epoch=154, loss=9.1696\n",
      "Epoch=155, loss: 9.16, regu_loss: 0.46\n",
      "Epoch=155, loss=9.1734\n",
      "Epoch=156, loss: 9.15, regu_loss: 1.21\n",
      "Epoch=156, loss=9.1787\n",
      "Epoch=157, loss: 9.15, regu_loss: 0.72\n",
      "Epoch=157, loss=9.1694\n",
      "Epoch=158, loss: 9.17, regu_loss: 0.53\n",
      "Epoch=158, loss=9.1760\n",
      "Epoch=159, loss: 9.19, regu_loss: 0.74\n",
      "Epoch=159, loss=9.2035\n",
      "Epoch=160, loss: 9.16, regu_loss: 0.49\n",
      "Epoch=160, loss=9.1721\n",
      "Epoch=161, loss: 9.16, regu_loss: 0.18\n",
      "Epoch=161, loss=9.1651\n",
      "Epoch=162, loss: 9.17, regu_loss: 0.96\n",
      "Epoch=162, loss=9.1902\n",
      "Epoch=163, loss: 9.25, regu_loss: 1.67\n",
      "Epoch=163, loss=9.2814\n",
      "Epoch=164, loss: 9.15, regu_loss: 0.36\n",
      "Epoch=164, loss=9.1577\n",
      "Epoch=165, loss: 9.16, regu_loss: 0.37\n",
      "Epoch=165, loss=9.1650\n",
      "Epoch=166, loss: 9.16, regu_loss: 0.32\n",
      "Epoch=166, loss=9.1625\n",
      "Epoch=167, loss: 9.16, regu_loss: 0.35\n",
      "Epoch=167, loss=9.1671\n",
      "Epoch=168, loss: 9.18, regu_loss: 0.54\n",
      "Epoch=168, loss=9.1899\n",
      "Epoch=169, loss: 9.15, regu_loss: 0.68\n",
      "Epoch=169, loss=9.1672\n",
      "Epoch=170, loss: 9.15, regu_loss: 0.54\n",
      "Epoch=170, loss=9.1648\n",
      "Epoch=171, loss: 9.16, regu_loss: 0.45\n",
      "Epoch=171, loss=9.1739\n",
      "Epoch=172, loss: 9.19, regu_loss: 0.42\n",
      "Epoch=172, loss=9.1948\n",
      "Epoch=173, loss: 9.15, regu_loss: 0.39\n",
      "Epoch=173, loss=9.1587\n",
      "Epoch=174, loss: 9.18, regu_loss: 0.25\n",
      "Epoch=174, loss=9.1831\n",
      "Epoch=175, loss: 9.22, regu_loss: 0.31\n",
      "Epoch=175, loss=9.2236\n",
      "Epoch=176, loss: 9.22, regu_loss: 0.09\n",
      "Epoch=176, loss=9.2257\n",
      "Epoch=177, loss: 9.15, regu_loss: 0.25\n",
      "Epoch=177, loss=9.1594\n",
      "Epoch=178, loss: 9.15, regu_loss: 0.39\n",
      "Epoch=178, loss=9.1563\n",
      "Epoch=179, loss: 9.17, regu_loss: 0.37\n",
      "Epoch=179, loss=9.1809\n",
      "Epoch=180, loss: 9.15, regu_loss: 0.41\n",
      "Epoch=180, loss=9.1585\n",
      "Epoch=181, loss: 9.16, regu_loss: 0.64\n",
      "Epoch=181, loss=9.1770\n",
      "Epoch=182, loss: 9.16, regu_loss: 0.56\n",
      "Epoch=182, loss=9.1678\n",
      "Epoch=183, loss: 9.15, regu_loss: 0.59\n",
      "Epoch=183, loss=9.1610\n",
      "Epoch=184, loss: 9.16, regu_loss: 0.51\n",
      "Epoch=184, loss=9.1692\n",
      "Epoch=185, loss: 9.16, regu_loss: 0.27\n",
      "Epoch=185, loss=9.1613\n",
      "Epoch=186, loss: 9.22, regu_loss: 0.41\n",
      "Epoch=186, loss=9.2285\n",
      "Epoch=187, loss: 9.17, regu_loss: 0.41\n",
      "Epoch=187, loss=9.1745\n",
      "Epoch=188, loss: 9.16, regu_loss: 0.16\n",
      "Epoch=188, loss=9.1584\n",
      "Epoch=189, loss: 9.21, regu_loss: 0.55\n",
      "Epoch=189, loss=9.2228\n",
      "Epoch=190, loss: 9.15, regu_loss: 0.45\n",
      "Epoch=190, loss=9.1628\n",
      "Epoch=191, loss: 9.17, regu_loss: 1.12\n",
      "Epoch=191, loss=9.1966\n",
      "Epoch=192, loss: 9.16, regu_loss: 0.41\n",
      "Epoch=192, loss=9.1644\n",
      "Epoch=193, loss: 9.17, regu_loss: 0.40\n",
      "Epoch=193, loss=9.1811\n",
      "Epoch=194, loss: 9.16, regu_loss: 0.24\n",
      "Epoch=194, loss=9.1609\n",
      "Epoch=195, loss: 9.18, regu_loss: 0.55\n",
      "Epoch=195, loss=9.1882\n",
      "Epoch=196, loss: 9.19, regu_loss: 0.71\n",
      "Epoch=196, loss=9.2069\n",
      "Epoch=197, loss: 9.17, regu_loss: 0.43\n",
      "Epoch=197, loss=9.1755\n",
      "Epoch=198, loss: 9.22, regu_loss: 0.13\n",
      "Epoch=198, loss=9.2270\n",
      "Epoch=199, loss: 9.17, regu_loss: 0.29\n",
      "Epoch=199, loss=9.1749\n",
      "=== Final ===\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "import random\n",
    "\n",
    "dr = 0.2\n",
    "# Step 3: Create emb_model =================================================================== #\n",
    "emb_model = model.Grace(in_dim, hid_dim, out_dim, num_layers, act_fn, temp)\n",
    "emb_model = emb_model.to(args.device)\n",
    "print(f'# params: {count_parameters(emb_model)}')\n",
    "\n",
    "optimizer = th.optim.Adam(emb_model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "# Step 4: Training =======================================================================\n",
    "for epoch in range(args.epochs):\n",
    "    emb_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    graph1, feat1 = aug(graph, feat_drop_rate=dr, edge_mask_rate=dr)\n",
    "    graph2, feat2 = aug(graph, feat_drop_rate=dr, edge_mask_rate=dr)\n",
    "\n",
    "    graph1 = graph1.to(args.device)\n",
    "    graph2 = graph2.to(args.device)\n",
    "\n",
    "    feat1 = feat1.to(args.device)\n",
    "    feat2 = feat2.to(args.device)\n",
    "\n",
    "    loss = emb_model(graph1, graph2, feat1, feat2, useWeight=True)\n",
    "    \n",
    "    # UGE-R\n",
    "    if debias_method in ['uge-r', 'uge-c']:\n",
    "        h1 = emb_model.encoder(graph1, feat1)\n",
    "        h2 = emb_model.encoder(graph2, feat2)\n",
    "        regu_loss = 0\n",
    "        scr_groups = random.sample(list(attr_comb_groups_map.keys()), 100)  \n",
    "        dst_groups = random.sample(list(attr_comb_groups_map.keys()), 100)\n",
    "        nobias_scr_groups = [map_tuple(group, non_sens_attr_idx) for group in scr_groups]\n",
    "        nobias_dst_groups = [map_tuple(group, non_sens_attr_idx) for group in dst_groups]\n",
    "\n",
    "        for group_idx in range(len(scr_groups)):\n",
    "            for view in [h1, h2]:\n",
    "                scr_group_nodes = attr_comb_groups_map[scr_groups[group_idx]]\n",
    "                dsc_group_nodes = attr_comb_groups_map[dst_groups[group_idx]]\n",
    "                \n",
    "                scr_node_embs = view[scr_group_nodes]\n",
    "                dsc_node_embs = view[dsc_group_nodes]\n",
    "                aver_score = mem_eff_matmul_mean(scr_node_embs, dsc_node_embs.T)\n",
    "\n",
    "                nobias_scr_group_nodes = nobias_attr_comb_groups_map[nobias_scr_groups[group_idx]]\n",
    "                nobias_dsc_group_nodes = nobias_attr_comb_groups_map[nobias_dst_groups[group_idx]]\n",
    "                nobias_scr_node_embs = view[nobias_scr_group_nodes]\n",
    "                nobias_dsc_node_embs = view[nobias_dsc_group_nodes]\n",
    "                nobias_aver_score = mem_eff_matmul_mean(nobias_scr_node_embs, nobias_dsc_node_embs.T)\n",
    "\n",
    "                regu_loss += th.square(aver_score - nobias_aver_score)\n",
    "            \n",
    "        print(f\"Epoch={epoch:03d}, loss: {loss.item():.2f}, regu_loss: {regu_loss.item():.2f}\")\n",
    "\n",
    "        loss += args.reg_weight * regu_loss / 1000\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch={epoch:03d}, loss={loss.item():.4f}')\n",
    "\n",
    "# Step 5: Linear evaluation ============================================================== #\n",
    "print(\"=== Final ===\")\n",
    "\n",
    "graph = graph.add_self_loop()\n",
    "graph = graph.to(args.device)\n",
    "embeds = emb_model.get_embedding(graph, graph.ndata['feat'].to(args.device))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Unbiasedness evaluation (predicting attribute)\n",
      "-- micro-f1 when predicting gender: 0.5490384615384616\n",
      "-- micro-f1 when predicting age: 0.23557692307692307\n",
      "-- micro-f1 when predicting occupation: 0.032692307692307694\n",
      "Utility evaluation (link prediction)\n",
      "-- ndcg of link prediction: 0.32871755543670783\n"
     ]
    }
   ],
   "source": [
    "from eval import label_classification, eval_unbiasedness_movielens\n",
    "'''Evaluation Embeddings  '''\n",
    "# label_classification(embeds, graph.ndata['label'], graph.ndata['train_mask'], graph.ndata['test_mask'], split=args.split)\n",
    "res = eval_unbiasedness_movielens('movie', embeds.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'unbiasedness': {'gender': 0.33942307692307694,\n",
       "  'age': 0.038461538461538464,\n",
       "  'region': 0.0,\n",
       "  'occupation': 0.11634615384615385},\n",
       " 'utility': 0.01643908590532295}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01643908590532295"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['utility']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "Unbiasedness evaluation (predicting attribute)\n",
      "-- micro-f1 when predicting gender: 0.5134615384615384\n",
      "-- micro-f1 when predicting age: 0.14038461538461539\n",
      "-- micro-f1 when predicting occupation: 0.045192307692307684\n",
      "Utility evaluation (link prediction)\n",
      "-- ndcg of link prediction: 0.020967456252946947\n"
     ]
    }
   ],
   "source": [
    "res = eval_unbiasedness_movielens('movie', th.randn_like(embeds).cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join('../..'))\n",
    "import Utils.Export as Export\n",
    "\n",
    "results = {\n",
    "  \"dataname\": args.dataname,\n",
    "  \"epochs\": args.epochs,\n",
    "  \"debias_method\": \"random\",\n",
    "  \"debias_attr\": args.debias_attr,\n",
    "  \"reg_weight\": args.reg_weight,\n",
    "  \"temp\": args.temp,\n",
    "  \"der1\": args.der1,\n",
    "  \"der2\": args.der2,\n",
    "  \"dfr1\": args.dfr1,\n",
    "  \"dfr2\": args.dfr2,\n",
    "  \"gender_f1m\": res['unbiasedness']['gender'],\n",
    "  \"age_f1m\": res['unbiasedness']['age'],\n",
    "  \"occupation_f1m\": res['unbiasedness']['occupation'],\n",
    "  \"link_ndcg\": res['utility'],\n",
    "}\n",
    "\n",
    "Export.saveData('./results.csv', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch_c116')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
