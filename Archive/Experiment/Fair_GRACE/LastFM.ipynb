{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dgl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# raw_f = json.load(open('./processed_data/LastFM_node_feature.json'))\n",
    "# # 7624 nodes, 7842 features\n",
    "# feat = th.zeros([7624, 7842])\n",
    "\n",
    "# for i in raw_f.keys():\n",
    "#   feat[int(i), th.tensor(raw_f[i], dtype=th.long)] = 1\n",
    "\n",
    "# np.savetxt(\"LastFM_node_feature.csv\", feat.numpy(), delimiter=\",\", fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataname', type=str, default='LastFM')\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--debias_method', type=str, default='uge-r', choices=['uge-r', 'none', 'random'], help='debiasing method to apply')\n",
    "parser.add_argument('--debias_attr', type=int, default=1, help='idx of sensitive attribute to be debiased')\n",
    "parser.add_argument('--num_sens', type=int, default=3, help='# of sensitive attr to make')\n",
    "parser.add_argument('--reg_weight', type=float, default=0.2, help='weight for the regularization based debiasing term')  \n",
    "\n",
    "parser.add_argument('--epochs', type=int, default=200, help='Number of training periods.')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='Learning rate.')\n",
    "parser.add_argument('--wd', type=float, default=1e-5, help='Weight decay.')\n",
    "parser.add_argument('--temp', type=float, default=1.0, help='Temperature.')\n",
    "\n",
    "parser.add_argument(\"--hid_dim\", type=int, default=256, help='Hidden layer dim.')\n",
    "parser.add_argument(\"--out_dim\", type=int, default=256, help='Output layer dim.')\n",
    "\n",
    "parser.add_argument(\"--num_layers\", type=int, default=2, help='Number of GNN layers.')\n",
    "parser.add_argument(\"--seed\", type=int, default=100, help='seed')\n",
    "parser.add_argument('--der1', type=float, default=0.2, help='Drop edge ratio of the 1st augmentation.')\n",
    "\n",
    "parser.add_argument('--sim_diff_ratio', type=float, default=5, help='Drop feature ratio of the 2nd augmentation.')\n",
    "parser.add_argument('--enable_heuristic', type=str, default='Y', help='Drop feature ratio of the 2nd augmentation.')\n",
    "\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "if args.gpu != -1 and th.cuda.is_available():\n",
    "    args.device = 'cuda:{}'.format(args.gpu)\n",
    "else:\n",
    "    args.device = 'cpu'\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "th.manual_seed(args.seed)\n",
    "\n",
    "if args.device != 'cpu':\n",
    "    th.cuda.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_file(edge_file: str, feat_file: str, label_file:str=None, disable_header=False) -> dgl.DGLGraph:\n",
    "  edges = pd.read_csv(edge_file, engine='c')\n",
    "  edges = th.tensor(edges.to_numpy()).t()\n",
    "  graph = dgl.graph((edges[0], edges[1]))\n",
    "  \n",
    "  if disable_header:\n",
    "    feat = pd.read_csv(feat_file, header=None, engine='c')\n",
    "  else:\n",
    "    feat = pd.read_csv(feat_file, engine='c')\n",
    "\n",
    "  feat = th.tensor(feat.to_numpy()).int()\n",
    "  graph.ndata['feat'] = feat\n",
    "\n",
    "  if label_file:\n",
    "    labels = pd.read_csv(label_file, engine='c')\n",
    "    labels = th.tensor(labels.to_numpy())\n",
    "    graph.ndata['labels'] = labels.t()[1]\n",
    "\n",
    "  return graph\n",
    "\n",
    "\n",
    "def add_sens_(graph: dgl.DGLGraph, indices: th.tensor):\n",
    "  sens = graph.ndata['feat'][:, indices].clone()\n",
    "  inverse = th.full((graph.num_nodes(),), 1)\n",
    "  inverse[indices] = 0\n",
    "  graph.ndata['feat'][indices] = 0\n",
    "  graph.ndata['sens_attr'] = sens\n",
    "\n",
    "\n",
    "def calc_weights_(graph: dgl.DGLGraph, debias_attr: int, ratio: int, target: int):\n",
    "  sens = graph.ndata['sens_attr'].t()[debias_attr]\n",
    "  sim = th.tensor([sens[edge[0]] == sens[edge[1]] for edge in graph.adj().coalesce().indices().t()]).int()\n",
    "  sim = sim * (ratio - 1)\n",
    "  sim = sim + 1\n",
    "  sim = sim * target * sim.shape[0] / sim.sum()\n",
    "  graph.edata['weight'] = sim.clamp(0,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = load_graph_from_file(\n",
    "  edge_file=f'./processed_data/{args.dataname}_edge.csv', \n",
    "  feat_file=f'./processed_data/{args.dataname}_node_feature.csv', \n",
    "  disable_header=True)\n",
    "\n",
    "add_sens_(graph, graph.ndata['feat'].sum(dim=0).topk(args.num_sens).indices)\n",
    "\n",
    "if args.enable_heuristic:\n",
    "  calc_weights_(graph, args.debias_attr, ratio=args.sim_diff_ratio, target=args.der1)\n",
    "else:\n",
    "  calc_weights_(graph, args.debias_attr, ratio=1, target=args.der1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group finished.\n",
      "  attr_comb_group_num: 8\n",
      "  nobias_attr_comb_group_num: 4\n"
     ]
    }
   ],
   "source": [
    "node_attributes = pd.DataFrame(graph.ndata['sens_attr'].numpy())\n",
    "attribute_list = list(node_attributes.columns)\n",
    "non_sens_attr_ls = [i for i in attribute_list if i!=args.debias_attr]\n",
    "non_sens_attr_idx = [i for i in range(len(attribute_list)) if attribute_list[i]!=debias_attr]\n",
    "\n",
    "attr_comb_groups = node_attributes.groupby(attribute_list)\n",
    "nobias_comb_groups = node_attributes.groupby(non_sens_attr_ls)\n",
    "\n",
    "\n",
    "attr_comb_groups_map = {tuple(group[1].iloc[0]):list(group[1].index) \n",
    "                        for group in attr_comb_groups}\n",
    "nobias_attr_comb_groups_map = {tuple(group[1].iloc[0][non_sens_attr_ls]):list(group[1].index) \n",
    "                            for group in nobias_comb_groups}\n",
    "\n",
    "print ('Group finished.')\n",
    "print ('  attr_comb_group_num:', len(attr_comb_groups_map.keys()))\n",
    "print ('  nobias_attr_comb_group_num:', len(nobias_attr_comb_groups_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=7624, num_edges=29772,\n",
       "      ndata_schemes={'feat': Scheme(shape=(7842,), dtype=torch.int32)}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def aug_weight(graph: dgl.DGLGraph, drop_feat: float, drop_edge: float=0.2):\n",
    "  edge_mask = th.bernoulli(graph.edata['weight']) == 0\n",
    "  masked_edges = graph.adj().coalesce().indices()[:, edge_mask]\n",
    "\n",
    "  new_graph = dgl.graph((masked_edges[0], masked_edges[1])).to(graph.device)\n",
    "\n",
    "  feat_mask = th.rand((graph.ndata['feat'].shape[1])) < (drop_feat)\n",
    "  new_graph.ndata['feat'] = graph.ndata['feat'].clone()\n",
    "\n",
    "  new_graph.ndata['feat'][:, feat_mask] = 0\n",
    "  new_graph = new_graph.add_self_loop()\n",
    "  return new_graph\n",
    "\n",
    "def aug(graph: dgl.DGLGraph, drop_feat: float, drop_edge: float=0.2):\n",
    "  edge_mask = th.bernoulli(th.full((graph.num_edges(),), drop_edge)) == 0\n",
    "  masked_edges = graph.adj().coalesce().indices()[:, edge_mask]\n",
    "\n",
    "  new_graph = dgl.graph((masked_edges[0], masked_edges[1])).to(graph.device)\n",
    "\n",
    "  feat_mask = th.rand((graph.ndata['feat'].shape[1])) < (drop_feat)\n",
    "  new_graph.ndata['feat'] = graph.ndata['feat'].clone()\n",
    "\n",
    "  new_graph.ndata['feat'][:, feat_mask] = 0\n",
    "  new_graph = new_graph.add_self_loop()\n",
    "  return new_graph\n",
    "\n",
    "\n",
    "aug_weight(graph, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_tuple(x, index_ls):\n",
    "  return tuple([x[idx] for idx in index_ls])\n",
    "\n",
    "def mem_eff_matmul_mean(mtx1, mtx2):\n",
    "  mtx1_rows = list(mtx1.shape)[0]\n",
    "  if mtx1_rows <= 1000:\n",
    "    return th.mean(th.matmul(mtx1, mtx2))\n",
    "  else:\n",
    "    value_sum = 0\n",
    "    for i in range(mtx1_rows // 1000):\n",
    "      value_sum += th.sum(th.matmul(mtx1[i*1000:(i+1)*1000, :], mtx2))\n",
    "    if mtx1_rows % 1000 != 0:\n",
    "      value_sum += th.sum(th.matmul(mtx1[(i+1)*1000:, :], mtx2))\n",
    "    return value_sum / (list(mtx1.shape)[0] * list(mtx2.shape)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# params: 4278528\n",
      "Epoch=000, loss: 9.52, regu_loss: 0.79\n",
      "Epoch=000, loss=9.6782\n",
      "Epoch=001, loss: 9.43, regu_loss: 0.15\n",
      "Epoch=001, loss=9.4636\n",
      "Epoch=002, loss: 9.22, regu_loss: 0.21\n",
      "Epoch=002, loss=9.2626\n",
      "Epoch=003, loss: 9.12, regu_loss: 4.00\n",
      "Epoch=003, loss=9.9235\n",
      "Epoch=004, loss: 9.00, regu_loss: 0.73\n",
      "Epoch=004, loss=9.1439\n",
      "Epoch=005, loss: 9.01, regu_loss: 1.23\n",
      "Epoch=005, loss=9.2531\n",
      "Epoch=006, loss: 8.99, regu_loss: 0.60\n",
      "Epoch=006, loss=9.1087\n",
      "Epoch=007, loss: 8.94, regu_loss: 0.30\n",
      "Epoch=007, loss=9.0008\n",
      "Epoch=008, loss: 8.93, regu_loss: 0.17\n",
      "Epoch=008, loss=8.9614\n",
      "Epoch=009, loss: 8.94, regu_loss: 0.07\n",
      "Epoch=009, loss=8.9553\n",
      "Epoch=010, loss: 8.93, regu_loss: 0.11\n",
      "Epoch=010, loss=8.9476\n",
      "Epoch=011, loss: 8.89, regu_loss: 0.08\n",
      "Epoch=011, loss=8.9098\n",
      "Epoch=012, loss: 8.88, regu_loss: 0.03\n",
      "Epoch=012, loss=8.8899\n",
      "Epoch=013, loss: 8.88, regu_loss: 0.08\n",
      "Epoch=013, loss=8.8916\n",
      "Epoch=014, loss: 8.87, regu_loss: 0.16\n",
      "Epoch=014, loss=8.9023\n",
      "Epoch=015, loss: 8.85, regu_loss: 0.02\n",
      "Epoch=015, loss=8.8541\n",
      "Epoch=016, loss: 8.84, regu_loss: 0.03\n",
      "Epoch=016, loss=8.8482\n",
      "Epoch=017, loss: 8.84, regu_loss: 0.14\n",
      "Epoch=017, loss=8.8718\n",
      "Epoch=018, loss: 8.83, regu_loss: 0.04\n",
      "Epoch=018, loss=8.8429\n",
      "Epoch=019, loss: 8.83, regu_loss: 0.01\n",
      "Epoch=019, loss=8.8318\n",
      "Epoch=020, loss: 8.83, regu_loss: 0.02\n",
      "Epoch=020, loss=8.8327\n",
      "Epoch=021, loss: 8.82, regu_loss: 0.05\n",
      "Epoch=021, loss=8.8275\n",
      "Epoch=022, loss: 8.82, regu_loss: 0.02\n",
      "Epoch=022, loss=8.8187\n",
      "Epoch=023, loss: 8.81, regu_loss: 0.06\n",
      "Epoch=023, loss=8.8215\n",
      "Epoch=024, loss: 8.81, regu_loss: 0.02\n",
      "Epoch=024, loss=8.8116\n",
      "Epoch=025, loss: 8.80, regu_loss: 0.01\n",
      "Epoch=025, loss=8.8040\n",
      "Epoch=026, loss: 8.80, regu_loss: 0.07\n",
      "Epoch=026, loss=8.8127\n",
      "Epoch=027, loss: 8.79, regu_loss: 0.01\n",
      "Epoch=027, loss=8.7900\n",
      "Epoch=028, loss: 8.79, regu_loss: 0.01\n",
      "Epoch=028, loss=8.7883\n",
      "Epoch=029, loss: 8.78, regu_loss: 0.03\n",
      "Epoch=029, loss=8.7825\n",
      "Epoch=030, loss: 8.78, regu_loss: 0.02\n",
      "Epoch=030, loss=8.7790\n",
      "Epoch=031, loss: 8.77, regu_loss: 0.03\n",
      "Epoch=031, loss=8.7744\n",
      "Epoch=032, loss: 8.77, regu_loss: 0.01\n",
      "Epoch=032, loss=8.7691\n",
      "Epoch=033, loss: 8.77, regu_loss: 0.02\n",
      "Epoch=033, loss=8.7711\n",
      "Epoch=034, loss: 8.76, regu_loss: 0.02\n",
      "Epoch=034, loss=8.7668\n",
      "Epoch=035, loss: 8.76, regu_loss: 0.05\n",
      "Epoch=035, loss=8.7700\n",
      "Epoch=036, loss: 8.76, regu_loss: 0.01\n",
      "Epoch=036, loss=8.7590\n",
      "Epoch=037, loss: 8.76, regu_loss: 0.01\n",
      "Epoch=037, loss=8.7585\n",
      "Epoch=038, loss: 8.75, regu_loss: 0.02\n",
      "Epoch=038, loss=8.7554\n",
      "Epoch=039, loss: 8.75, regu_loss: 0.04\n",
      "Epoch=039, loss=8.7589\n",
      "Epoch=040, loss: 8.75, regu_loss: 0.00\n",
      "Epoch=040, loss=8.7485\n",
      "Epoch=041, loss: 8.75, regu_loss: 0.00\n",
      "Epoch=041, loss=8.7503\n",
      "Epoch=042, loss: 8.74, regu_loss: 0.01\n",
      "Epoch=042, loss=8.7476\n",
      "Epoch=043, loss: 8.74, regu_loss: 0.01\n",
      "Epoch=043, loss=8.7464\n",
      "Epoch=044, loss: 8.75, regu_loss: 0.00\n",
      "Epoch=044, loss=8.7468\n",
      "Epoch=045, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=045, loss=8.7427\n",
      "Epoch=046, loss: 8.74, regu_loss: 0.03\n",
      "Epoch=046, loss=8.7469\n",
      "Epoch=047, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=047, loss=8.7433\n",
      "Epoch=048, loss: 8.74, regu_loss: 0.01\n",
      "Epoch=048, loss=8.7453\n",
      "Epoch=049, loss: 8.74, regu_loss: 0.01\n",
      "Epoch=049, loss=8.7405\n",
      "Epoch=050, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=050, loss=8.7391\n",
      "Epoch=051, loss: 8.74, regu_loss: 0.01\n",
      "Epoch=051, loss=8.7385\n",
      "Epoch=052, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=052, loss=8.7387\n",
      "Epoch=053, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=053, loss=8.7371\n",
      "Epoch=054, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=054, loss=8.7326\n",
      "Epoch=055, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=055, loss=8.7354\n",
      "Epoch=056, loss: 8.74, regu_loss: 0.00\n",
      "Epoch=056, loss=8.7371\n",
      "Epoch=057, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=057, loss=8.7301\n",
      "Epoch=058, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=058, loss=8.7319\n",
      "Epoch=059, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=059, loss=8.7315\n",
      "Epoch=060, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=060, loss=8.7302\n",
      "Epoch=061, loss: 8.73, regu_loss: 0.02\n",
      "Epoch=061, loss=8.7344\n",
      "Epoch=062, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=062, loss=8.7320\n",
      "Epoch=063, loss: 8.73, regu_loss: 0.00\n",
      "Epoch=063, loss=8.7295\n",
      "Epoch=064, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=064, loss=8.7293\n",
      "Epoch=065, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=065, loss=8.7267\n",
      "Epoch=066, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=066, loss=8.7273\n",
      "Epoch=067, loss: 8.73, regu_loss: 0.01\n",
      "Epoch=067, loss=8.7272\n",
      "Epoch=068, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=068, loss=8.7245\n",
      "Epoch=069, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=069, loss=8.7223\n",
      "Epoch=070, loss: 8.72, regu_loss: 0.01\n",
      "Epoch=070, loss=8.7215\n",
      "Epoch=071, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=071, loss=8.7242\n",
      "Epoch=072, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=072, loss=8.7230\n",
      "Epoch=073, loss: 8.72, regu_loss: 0.01\n",
      "Epoch=073, loss=8.7218\n",
      "Epoch=074, loss: 8.72, regu_loss: 0.01\n",
      "Epoch=074, loss=8.7214\n",
      "Epoch=075, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=075, loss=8.7203\n",
      "Epoch=076, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=076, loss=8.7179\n",
      "Epoch=077, loss: 8.72, regu_loss: 0.01\n",
      "Epoch=077, loss=8.7203\n",
      "Epoch=078, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=078, loss=8.7183\n",
      "Epoch=079, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=079, loss=8.7184\n",
      "Epoch=080, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=080, loss=8.7162\n",
      "Epoch=081, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=081, loss=8.7189\n",
      "Epoch=082, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=082, loss=8.7151\n",
      "Epoch=083, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=083, loss=8.7155\n",
      "Epoch=084, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=084, loss=8.7139\n",
      "Epoch=085, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=085, loss=8.7139\n",
      "Epoch=086, loss: 8.72, regu_loss: 0.00\n",
      "Epoch=086, loss=8.7169\n",
      "Epoch=087, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=087, loss=8.7152\n",
      "Epoch=088, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=088, loss=8.7130\n",
      "Epoch=089, loss: 8.71, regu_loss: 0.01\n",
      "Epoch=089, loss=8.7137\n",
      "Epoch=090, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=090, loss=8.7123\n",
      "Epoch=091, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=091, loss=8.7119\n",
      "Epoch=092, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=092, loss=8.7109\n",
      "Epoch=093, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=093, loss=8.7112\n",
      "Epoch=094, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=094, loss=8.7132\n",
      "Epoch=095, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=095, loss=8.7096\n",
      "Epoch=096, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=096, loss=8.7123\n",
      "Epoch=097, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=097, loss=8.7126\n",
      "Epoch=098, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=098, loss=8.7108\n",
      "Epoch=099, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=099, loss=8.7074\n",
      "Epoch=100, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=100, loss=8.7085\n",
      "Epoch=101, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=101, loss=8.7107\n",
      "Epoch=102, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=102, loss=8.7079\n",
      "Epoch=103, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=103, loss=8.7083\n",
      "Epoch=104, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=104, loss=8.7144\n",
      "Epoch=105, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=105, loss=8.7102\n",
      "Epoch=106, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=106, loss=8.7077\n",
      "Epoch=107, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=107, loss=8.7059\n",
      "Epoch=108, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=108, loss=8.7096\n",
      "Epoch=109, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=109, loss=8.7066\n",
      "Epoch=110, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=110, loss=8.7037\n",
      "Epoch=111, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=111, loss=8.7088\n",
      "Epoch=112, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=112, loss=8.7070\n",
      "Epoch=113, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=113, loss=8.7043\n",
      "Epoch=114, loss: 8.71, regu_loss: 0.00\n",
      "Epoch=114, loss=8.7077\n",
      "Epoch=115, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=115, loss=8.7054\n",
      "Epoch=116, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=116, loss=8.7045\n",
      "Epoch=117, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=117, loss=8.7035\n",
      "Epoch=118, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=118, loss=8.7046\n",
      "Epoch=119, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=119, loss=8.7022\n",
      "Epoch=120, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=120, loss=8.7050\n",
      "Epoch=121, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=121, loss=8.7042\n",
      "Epoch=122, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=122, loss=8.7041\n",
      "Epoch=123, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=123, loss=8.7022\n",
      "Epoch=124, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=124, loss=8.7007\n",
      "Epoch=125, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=125, loss=8.7036\n",
      "Epoch=126, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=126, loss=8.7019\n",
      "Epoch=127, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=127, loss=8.7011\n",
      "Epoch=128, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=128, loss=8.7050\n",
      "Epoch=129, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=129, loss=8.7025\n",
      "Epoch=130, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=130, loss=8.6982\n",
      "Epoch=131, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=131, loss=8.7031\n",
      "Epoch=132, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=132, loss=8.7005\n",
      "Epoch=133, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=133, loss=8.6972\n",
      "Epoch=134, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=134, loss=8.6997\n",
      "Epoch=135, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=135, loss=8.6990\n",
      "Epoch=136, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=136, loss=8.6991\n",
      "Epoch=137, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=137, loss=8.6963\n",
      "Epoch=138, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=138, loss=8.6978\n",
      "Epoch=139, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=139, loss=8.6973\n",
      "Epoch=140, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=140, loss=8.6984\n",
      "Epoch=141, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=141, loss=8.6938\n",
      "Epoch=142, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=142, loss=8.6956\n",
      "Epoch=143, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=143, loss=8.6951\n",
      "Epoch=144, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=144, loss=8.6942\n",
      "Epoch=145, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=145, loss=8.6974\n",
      "Epoch=146, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=146, loss=8.6949\n",
      "Epoch=147, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=147, loss=8.6954\n",
      "Epoch=148, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=148, loss=8.6942\n",
      "Epoch=149, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=149, loss=8.6970\n",
      "Epoch=150, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=150, loss=8.6953\n",
      "Epoch=151, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=151, loss=8.6972\n",
      "Epoch=152, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=152, loss=8.6926\n",
      "Epoch=153, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=153, loss=8.6938\n",
      "Epoch=154, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=154, loss=8.6928\n",
      "Epoch=155, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=155, loss=8.6956\n",
      "Epoch=156, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=156, loss=8.6928\n",
      "Epoch=157, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=157, loss=8.6940\n",
      "Epoch=158, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=158, loss=8.6921\n",
      "Epoch=159, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=159, loss=8.6916\n",
      "Epoch=160, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=160, loss=8.6937\n",
      "Epoch=161, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=161, loss=8.6912\n",
      "Epoch=162, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=162, loss=8.6931\n",
      "Epoch=163, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=163, loss=8.6938\n",
      "Epoch=164, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=164, loss=8.6929\n",
      "Epoch=165, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=165, loss=8.6952\n",
      "Epoch=166, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=166, loss=8.6946\n",
      "Epoch=167, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=167, loss=8.6931\n",
      "Epoch=168, loss: 8.70, regu_loss: 0.00\n",
      "Epoch=168, loss=8.6982\n",
      "Epoch=169, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=169, loss=8.6913\n",
      "Epoch=170, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=170, loss=8.6922\n",
      "Epoch=171, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=171, loss=8.6921\n",
      "Epoch=172, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=172, loss=8.6900\n",
      "Epoch=173, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=173, loss=8.6914\n",
      "Epoch=174, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=174, loss=8.6937\n",
      "Epoch=175, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=175, loss=8.6916\n",
      "Epoch=176, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=176, loss=8.6913\n",
      "Epoch=177, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=177, loss=8.6928\n",
      "Epoch=178, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=178, loss=8.6906\n",
      "Epoch=179, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=179, loss=8.6916\n",
      "Epoch=180, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=180, loss=8.6910\n",
      "Epoch=181, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=181, loss=8.6884\n",
      "Epoch=182, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=182, loss=8.6895\n",
      "Epoch=183, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=183, loss=8.6892\n",
      "Epoch=184, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=184, loss=8.6885\n",
      "Epoch=185, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=185, loss=8.6878\n",
      "Epoch=186, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=186, loss=8.6882\n",
      "Epoch=187, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=187, loss=8.6884\n",
      "Epoch=188, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=188, loss=8.6856\n",
      "Epoch=189, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=189, loss=8.6866\n",
      "Epoch=190, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=190, loss=8.6889\n",
      "Epoch=191, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=191, loss=8.6879\n",
      "Epoch=192, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=192, loss=8.6872\n",
      "Epoch=193, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=193, loss=8.6880\n",
      "Epoch=194, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=194, loss=8.6870\n",
      "Epoch=195, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=195, loss=8.6882\n",
      "Epoch=196, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=196, loss=8.6867\n",
      "Epoch=197, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=197, loss=8.6888\n",
      "Epoch=198, loss: 8.69, regu_loss: 0.00\n",
      "Epoch=198, loss=8.6878\n",
      "Epoch=199, loss: 8.68, regu_loss: 0.00\n",
      "Epoch=199, loss=8.6847\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import model\n",
    "importlib.reload(model)\n",
    "import random\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "dr = 0.2\n",
    "# Step 3: Create emb_model =================================================================== #\n",
    "emb_model = model.Grace(\n",
    "  in_dim=graph.ndata['feat'].shape[1], \n",
    "  hid_dim=args.hid_dim, \n",
    "  out_dim=args.out_dim, \n",
    "  num_layers=args.num_layers, \n",
    "  act_fn=nn.ReLU(), \n",
    "  temp=args.temp\n",
    ")\n",
    "emb_model = emb_model.to(args.device)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n",
    "print(f'# params: {count_parameters(emb_model)}')\n",
    "\n",
    "optimizer = th.optim.Adam(emb_model.parameters(), lr=args.lr, weight_decay=args.wd)\n",
    "\n",
    "# Step 4: Training =======================================================================\n",
    "for epoch in range(args.epochs):\n",
    "    emb_model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    view_1 = aug_weight(graph, drop_feat=dr).to(args.device)\n",
    "    view_2 = aug_weight(graph, drop_feat=dr).to(args.device)\n",
    "\n",
    "    loss = emb_model(view_1, view_2, view_1.ndata['feat'], view_1.ndata['feat'], batch_size=0)\n",
    "    \n",
    "    # UGE-R\n",
    "    if args.debias_method in ['uge-r', 'uge-c']:\n",
    "        h1 = emb_model.encoder(view_1, view_1.ndata['feat'])\n",
    "        h2 = emb_model.encoder(view_2, view_2.ndata['feat'])\n",
    "        regu_loss = 0\n",
    "        scr_groups = random.sample(list(attr_comb_groups_map.keys()), 8)  \n",
    "        dst_groups = random.sample(list(attr_comb_groups_map.keys()), 8)\n",
    "        nobias_scr_groups = [map_tuple(group, non_sens_attr_idx) for group in scr_groups]\n",
    "        nobias_dst_groups = [map_tuple(group, non_sens_attr_idx) for group in dst_groups]\n",
    "\n",
    "        for group_idx in range(len(scr_groups)):\n",
    "            for view in [h1, h2]:\n",
    "                scr_group_nodes = attr_comb_groups_map[scr_groups[group_idx]]\n",
    "                dsc_group_nodes = attr_comb_groups_map[dst_groups[group_idx]]\n",
    "                \n",
    "                scr_node_embs = view[scr_group_nodes]\n",
    "                dsc_node_embs = view[dsc_group_nodes]\n",
    "                aver_score = mem_eff_matmul_mean(scr_node_embs, dsc_node_embs.T)\n",
    "\n",
    "                nobias_scr_group_nodes = nobias_attr_comb_groups_map[nobias_scr_groups[group_idx]]\n",
    "                nobias_dsc_group_nodes = nobias_attr_comb_groups_map[nobias_dst_groups[group_idx]]\n",
    "                nobias_scr_node_embs = view[nobias_scr_group_nodes]\n",
    "                nobias_dsc_node_embs = view[nobias_dsc_group_nodes]\n",
    "                nobias_aver_score = mem_eff_matmul_mean(nobias_scr_node_embs, nobias_dsc_node_embs.T)\n",
    "\n",
    "                regu_loss += th.square(aver_score - nobias_aver_score)\n",
    "            \n",
    "        print(f\"Epoch={epoch:03d}, loss: {loss.item():.2f}, regu_loss: {regu_loss.item():.2f}\")\n",
    "\n",
    "        loss += args.reg_weight * regu_loss / 1\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f'Epoch={epoch:03d}, loss={loss.item():.4f}')\n",
    "\n",
    "# Step 5: Linear evaluation ============================================================== #\n",
    "graph = graph.add_self_loop()\n",
    "graph = graph.to(args.device)\n",
    "embeds = emb_model.get_embedding(graph, graph.ndata['feat'].to(args.device)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dgl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpusrv02.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb#ch0000009vscode-remote?line=31'>32</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m0.\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpusrv02.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb#ch0000009vscode-remote?line=32'>33</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dcg_at_k(r, k) \u001b[39m/\u001b[39m idcg\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bgpusrv02.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb#ch0000009vscode-remote?line=35'>36</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39meval_link_ndcg\u001b[39m(embeds, graph: dgl\u001b[39m.\u001b[39mDGLGraph):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpusrv02.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb#ch0000009vscode-remote?line=36'>37</a>\u001b[0m   accum_ndcg \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bgpusrv02.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/Fair_GRACE/LastFM.ipynb#ch0000009vscode-remote?line=37'>38</a>\u001b[0m   node_cnt \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dgl' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_f1(embeds, graph, debias_attr):\n",
    "  evaluate_attr = graph.ndata['sens_attr'][:,debias_attr]\n",
    "  split_idx = int(graph.num_nodes() * 0.75)\n",
    "  lgreg = LogisticRegression(\n",
    "    random_state=0, \n",
    "    class_weight='balanced', \n",
    "    max_iter=500).fit(\n",
    "    embeds[:split_idx].cpu(), evaluate_attr[:split_idx].cpu())\n",
    "  pred = lgreg.predict(embeds[split_idx:].cpu())\n",
    "\n",
    "  score = f1_score(evaluate_attr[split_idx:split_idx + pred.shape[0]].cpu(), pred, average='micro')\n",
    "\n",
    "  print(f'-- micro-f1 when predicting sensitive attr #{debias_attr}: {score}')\n",
    "  return score\n",
    "  \n",
    "\n",
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size != k:\n",
    "        raise ValueError('Ranking List length < k')    \n",
    "    return np.sum((2**r - 1) / np.log2(np.arange(2, r.size + 2)))\n",
    "\n",
    "\n",
    "def ndcg_at_k(r, k):\n",
    "    sort_r = sorted(r,reverse = True)\n",
    "    idcg = dcg_at_k(sort_r, k)\n",
    "    if not idcg:\n",
    "        print('.', end=' ')\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / idcg\n",
    "\n",
    "\n",
    "def eval_link_ndcg(embeds, graph: dgl.DGLGraph):\n",
    "  accum_ndcg = 0\n",
    "  node_cnt = 0\n",
    "  sample_size = 75\n",
    "  k = 10\n",
    "  adj = graph.adj().to_dense()\n",
    "\n",
    "  for node in graph.nodes():\n",
    "    node_edges = adj[node]\n",
    "    positive_nodes = node_edges.nonzero(as_tuple=True)[0]\n",
    "    split_idx = int(positive_nodes.shape[0] / 10) + 1\n",
    "\n",
    "    if split_idx == 0 or split_idx > sample_size:\n",
    "      continue\n",
    "      \n",
    "    negative_nodes = np.random.choice(\n",
    "      (1 - node_edges).clamp(0,1).nonzero(as_tuple=True)[0], \n",
    "      sample_size - split_idx,\n",
    "      replace=False\n",
    "    )\n",
    "    positive_nodes = positive_nodes[:split_idx] # Subset of positive pair\n",
    "    eval_nodes = np.concatenate((positive_nodes, negative_nodes))\n",
    "    eval_edges = np.zeros(sample_size)\n",
    "    eval_edges[:split_idx] = 1\n",
    "\n",
    "    predicted_edges = np.dot(embeds[node], embeds[eval_nodes].T)\n",
    "    rank_pred_keys = np.argsort(predicted_edges)[::-1]\n",
    "    ranked_node_edges = eval_edges[rank_pred_keys]\n",
    "    ndcg = ndcg_at_k(ranked_node_edges, k)\n",
    "    accum_ndcg += ndcg\n",
    "\n",
    "    node_cnt += 1\n",
    "\n",
    "  score = accum_ndcg/node_cnt\n",
    "  print(f'-- ndcg of link prediction: {score:.6f}')\n",
    "  return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.debias_method == 'random':\n",
    "  embeds = th.rand(embeds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- micro-f1 when predicting sensitive attr #0: 0.7476390346274921\n",
      "-- micro-f1 when predicting sensitive attr #1: 0.7282266526757608\n",
      "-- micro-f1 when predicting sensitive attr #2: 0.6841552990556139\n",
      "-- ndcg of link prediction: 0.430182\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "  \"dataname\": args.dataname,\n",
    "  \"epochs\": args.epochs,\n",
    "  \"seed\": args.seed,\n",
    "  \"debias_method\": args.debias_method,\n",
    "  \"debias_attr\": args.debias_attr,\n",
    "  \"reg_weight\": args.reg_weight,\n",
    "  \"temp\": args.temp,\n",
    "  \"der1\": args.der1,\n",
    "  \"heuristic_drop\": args.enable_heuristic,\n",
    "  \"ratio\": args.sim_diff_ratio\n",
    "}\n",
    "\n",
    "for attr_idx in range(0, graph.ndata['sens_attr'].shape[1]):\n",
    "  results[f'f1_{attr_idx}'] = get_f1(embeds, graph, attr_idx)\n",
    "results['link'] = eval_link_ndcg(embeds, graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.join('../..'))\n",
    "import Utils.Export as Export\n",
    "Export.saveData('./results_H.csv', results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_c116",
   "language": "python",
   "name": "pytorch_c116"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ebcc87a138737cae0cbacc26b11f8cf47912c2bbdaa9a1b8b9d9018ec118b72c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
