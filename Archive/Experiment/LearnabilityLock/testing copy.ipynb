{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataloader' from '/u/nyw6dh/HCDM/Experiment/LearnabilityLock/dataloader.py'>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataloader\n",
    "import argparse\n",
    "import importlib\n",
    "importlib.reload(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default='config.yaml')\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "config = yaml.load(open('config.yml'), Loader=SafeLoader)[args.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset = dataloader.load_DGL('cora')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Attack ===================\n",
    "\n",
    "# Set up surrogate model\n",
    "# surrogate = models.GCN(\n",
    "#   in_size=dataset.ndata['feat'].shape[1],\n",
    "#   out_size=int(dataset.ndata['label'].max()) + 1,\n",
    "#   hid_size=config['hid_size'],\n",
    "#   lr=config['lr'],\n",
    "#   dropout=config['dropout'],\n",
    "#   weight_decay=config['weight_decay']\n",
    "# )\n",
    "\n",
    "# surrogate(dataset, dataset.ndata['feat'])\n",
    "# surrogate.fit(dataset, dataset.ndata['feat'], dataset.ndata['label'], 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   4,    5,    7,  ..., 2702, 2707, 2707],\n",
       "        [2448, 2025, 1065,  ..., 2418,  405, 2022]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import models\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LearnabilityLock():\n",
    "  def __init__(self, graph: dgl.DGLGraph):\n",
    "    self.adj = graph.adj().to_dense()\n",
    "    self.num_nodes = graph.num_nodes()\n",
    "    self.feat = graph.ndata['feat']\n",
    "    self.labels = graph.ndata['label']\n",
    "    \n",
    "  def set_protected(self, protected: torch.Tensor) -> None:\n",
    "    assert self.num_nodes == protected.shape[0]\n",
    "    assert protected.dtype == torch.bool\n",
    "    self.g0 = protected\n",
    "    self.gX = ~protected\n",
    "\n",
    "  def designate_protected_random(self, node_ct: int) -> None:\n",
    "    \"\"\"\n",
    "    Randomly select ct nodes to be the protected set\n",
    "    \"\"\"\n",
    "    sample = torch.rand(self.num_nodes).topk(node_ct)\n",
    "    protected = torch.zeros(self.num_nodes, dtype=torch.bool)\n",
    "    protected[sample.indices] = True\n",
    "    self.set_protected(protected)\n",
    "\n",
    "  def get_sample_idx(self, edge_ct: int) -> torch.tensor:\n",
    "    cutoff = 1 - (edge_ct / self.num_nodes ** 2) \n",
    "    sample = (torch.rand(self.adj.shape) > cutoff).int().to_sparse().indices()\n",
    "\n",
    "    return sample\n",
    "\n",
    "  def create_locked_graph(self, attack_epochs: int, surrogate: torch.nn.Module) -> dgl.data.DGLDataset:\n",
    "    assert 'protected' in self.dataset.ndata\n",
    "\n",
    "    t = tqdm(range(attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Perturbing\")\n",
    "\n",
    "    for epoch in t:\n",
    "      None\n",
    "\n",
    "\n",
    "attack = LearnabilityLock(dataset)\n",
    "attack.designate_protected_random(node_ct=100)\n",
    "attack.get_sample_idx(1000)\n",
    "\n",
    "\n",
    "# attack.create_locked_graph(attack_epochs=10, surrogate=surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 79.39it/s, loss=1.95]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 84.78it/s, loss=1.92]58, adj_g=0, model_loss=1.95]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 86.04it/s, loss=1.9]34, adj_g=0, model_loss=1.92] \n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 89.68it/s, loss=1.89], adj_g=0, model_loss=1.9]  \n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 75.37it/s, loss=1.87], adj_g=-1, model_loss=1.89]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 76.04it/s, loss=1.85], adj_g=-2, model_loss=1.87]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 78.46it/s, loss=1.83], adj_g=-2, model_loss=1.85]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 84.42it/s, loss=1.8]8, adj_g=-3, model_loss=1.83]\n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 86.12it/s, loss=1.78], adj_g=-4, model_loss=1.8] \n",
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 92.16it/s, loss=1.76], adj_g=-4, model_loss=1.78]\n",
      "Perturbing: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s, adj_l=0.892, adj_g=-4, model_loss=1.76]\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "\n",
    "# Set up surrogate model\n",
    "surrogate = models.DenseGCN(\n",
    "  in_size=dataset.ndata['feat'].shape[1],\n",
    "  out_size=int(dataset.ndata['label'].max()) + 1,\n",
    "  hid_size=config['hid_size'],\n",
    "  lr=config['lr'],\n",
    "  dropout=config['dropout'],\n",
    "  weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "perturbations = torch.zeros_like(attack.adj).float()\n",
    "count = torch.zeros_like(attack.adj).float()\n",
    "num_perturbations = 2000\n",
    "\n",
    "t = tqdm(range(10), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "t.set_description(\"Perturbing\")\n",
    "\n",
    "for epoch in t:\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(attack.adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = attack.adj + perturbations - ((attack.adj * perturbations) * 2)\n",
    "    # Utils.get_modified_adj(graph.adj, perturbations).float().to(device)\n",
    "\n",
    "    for sample_epoch in range(5):\n",
    "        # Get sample indices\n",
    "        # sampled = torch.bernoulli(sampling_matrix)\n",
    "        idx = attack.get_sample_idx(edge_ct=10000)\n",
    "\n",
    "        # Map sample to adj\n",
    "        sample = modified_adj[idx[0], idx[1]].clone().detach().requires_grad_(True)\n",
    "        modified_adj[idx[0], idx[1]] = sample\n",
    "\n",
    "        # Get grad\n",
    "        predictions = surrogate(modified_adj, attack.feat)\n",
    "        loss = F.cross_entropy(predictions[attack.g0], attack.labels[attack.g0]) \\\n",
    "            - F.cross_entropy(predictions[attack.gX], attack.labels[attack.gX])\n",
    "\n",
    "        grad = torch.autograd.grad(loss, sample)[0]\n",
    "\n",
    "        # Implement averaging\n",
    "        adj_grad[idx[0], idx[1]] += grad\n",
    "        count[idx[0], idx[1]] += 1\n",
    "\n",
    "    # Update the sampling matrix\n",
    "    # samplingMatrix.updateByGrad(adj_grad, count)\n",
    "    # samplingMatrix.getRatio()\n",
    "\n",
    "    # Average the gradient\n",
    "    adj_grad = torch.div(adj_grad, count)\n",
    "    adj_grad[adj_grad != adj_grad] = 0\n",
    "\n",
    "    # Update perturbations\n",
    "    lr = (num_perturbations) / (epoch + 1)\n",
    "    # pre_projection = int(perturbations.sum() / 2)\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "    # perturbations = Utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    for i in range(10):\n",
    "        perturbations = perturbations * (num_perturbations / perturbations.abs().sum())\n",
    "        perturbations.clamp_(-1, 1)\n",
    "        if np.abs(1 - (perturbations.abs().sum() / num_perturbations)) < 0.1:\n",
    "            break\n",
    "\n",
    "    # Train the model\n",
    "    modified_adj = attack.adj + perturbations - ((attack.adj * perturbations) * 2)\n",
    "    model_loss = surrogate.fit(modified_adj, attack.feat, attack.labels, epochs=1)\n",
    "\n",
    "    t.set_postfix({\"adj_l\": loss.item(),\n",
    "                    \"adj_g\": int(adj_grad.sum()),\n",
    "                    \"model_loss\": model_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(t: torch.Tensor):\n",
    "  print(f'shape:{t.shape}\\tsum:{t.sum()}\\tmin:{t.min()}\\tmax: {t.max()}\\tmean:{t.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:torch.Size([2708, 2708])\tsum:2000.0\tmin:0.0\tmax: 0.7944938540458679\tmean:0.00027272984152659774\n"
     ]
    }
   ],
   "source": [
    "s(perturbations.abs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sample loss: -0.06\t Edges: 2033\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    max_loss = -1000\n",
    "\n",
    "    for k in range(0,3):\n",
    "        sample = torch.bernoulli(perturbations.abs())\n",
    "        modified_adj = attack.adj + sample - ((attack.adj * sample) * 2)\n",
    "        # modified_adj = Utils.make_symmetric(modified_adj) # Removing this creates \"impossible\" adj, but works well\n",
    "\n",
    "        predictions = surrogate(modified_adj, attack.feat) \n",
    "\n",
    "        loss = F.cross_entropy(predictions[attack.g0], attack.labels[attack.g0]) \\\n",
    "            - F.cross_entropy(predictions[attack.gX], attack.labels[attack.gX])\n",
    "\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            best = sample\n",
    "            best_mod = modified_adj\n",
    "    \n",
    "    print(f\"Best sample loss: {loss:.2f}\\t Edges: {best.abs().sum():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 200/200 [00:01<00:00, 124.73it/s, loss=0.65]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.85, 0.8320552147239264)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "lock_model = models.DenseGCN(\n",
    "  in_size=dataset.ndata['feat'].shape[1],\n",
    "  out_size=int(dataset.ndata['label'].max()) + 1,\n",
    "  hid_size=config['hid_size'],\n",
    "  lr=config['lr'],\n",
    "  dropout=config['dropout'],\n",
    "  weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "lock_model.fit(\n",
    "    g=best_mod, \n",
    "    feat=attack.feat, \n",
    "    labels=attack.labels, \n",
    "    epochs=200, \n",
    "    verbose=True)\n",
    "\n",
    "pred = lock_model(best_mod, attack.feat)\n",
    "f1_g0 = metrics.f1_score(attack.labels[attack.g0], pred.argmax(dim=1)[attack.g0], average='micro')\n",
    "f1_gX = metrics.f1_score(attack.labels[attack.gX], pred.argmax(dim=1)[attack.gX], average='micro')\n",
    "f1_g0, f1_gX\n",
    "\n",
    "# match = pred.argmax(dim=1) == dataset.ndata['label']\n",
    "# match.sum() / match.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch_c116')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
