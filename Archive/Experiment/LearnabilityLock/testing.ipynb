{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataloader' from '/u/nyw6dh/HCDM/Experiment/LearnabilityLock/dataloader.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataloader\n",
    "import argparse\n",
    "import importlib\n",
    "importlib.reload(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--dataset', type=str, default='cora')\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--config', type=str, default='config.yaml')\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "config = yaml.load(open('config.yml'), Loader=SafeLoader)[args.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "dataset = dataloader.load_DGL('cora')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 100/100 [00:00<00:00, 112.10it/s, loss=0.76]\n"
     ]
    }
   ],
   "source": [
    "# 3. Attack ===================\n",
    "\n",
    "# Set up surrogate model\n",
    "# surrogate = models.GCN(\n",
    "#   in_size=dataset.ndata['feat'].shape[1],\n",
    "#   out_size=int(dataset.ndata['label'].max()) + 1,\n",
    "#   hid_size=config['hid_size'],\n",
    "#   lr=config['lr'],\n",
    "#   dropout=config['dropout'],\n",
    "#   weight_decay=config['weight_decay']\n",
    "# )\n",
    "\n",
    "# surrogate(dataset, dataset.ndata['feat'])\n",
    "surrogate.fit(dataset, dataset.ndata['feat'], dataset.ndata['label'], 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=2708, num_edges=1033,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={'sample_type': Scheme(shape=(), dtype=torch.float32)})"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import models\n",
    "import importlib\n",
    "importlib.reload(models)\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LearnabilityLock():\n",
    "  def __init__(self, graph: dgl.DGLGraph):\n",
    "    self.graph = graph\n",
    "    \n",
    "  def set_protected(self, protected: torch.Tensor) -> None:\n",
    "    assert self.graph.num_nodes() == protected.shape[0]\n",
    "    assert protected.dtype == torch.bool\n",
    "    self.graph.ndata['protected'] = protected\n",
    "\n",
    "  def designate_protected_random(self, node_ct: int) -> None:\n",
    "    \"\"\"\n",
    "    Randomly select ct nodes to be the protected set\n",
    "    \"\"\"\n",
    "    sample = torch.rand(self.graph.num_nodes()).topk(ct)\n",
    "    protected = torch.zeros(self.graph.num_nodes(), dtype=torch.bool)\n",
    "    protected[sample.indices] = True\n",
    "    self.set_protected(protected)\n",
    "\n",
    "  def get_sample_graph(self, edge_ct: int, root_graph: dgl.DGLGraph) -> dgl.DGLGraph:\n",
    "    adj = root_graph.adj().int().to_dense()\n",
    "    cutoff = 1 - (edge_ct / adj.shape[0] ** 2) \n",
    "    sample = (torch.rand(adj.shape) > cutoff).int()\n",
    "\n",
    "    positive_samples = (sample * adj).to_sparse().float()\n",
    "    negative_samples = (sample * (adj - 1)).to_sparse().float()\n",
    "    negative_samples.values()[:] = 0\n",
    "    all_samples = positive_samples + negative_samples\n",
    "    idx = all_samples.indices()\n",
    "\n",
    "    sample_graph = dgl.graph((idx[0], idx[1]), num_nodes=root_graph.num_nodes())\n",
    "\n",
    "    # 1 is positive sample, 0 is negative sample\n",
    "    sample_graph.edata['sample_type'] = all_samples.values()\n",
    "    sample_graph.edata['sample_type'].requires_grad_(True)\n",
    "\n",
    "    return sample_graph\n",
    "\n",
    "  def create_locked_graph(self, attack_epochs: int, surrogate: torch.nn.Module) -> dgl.data.DGLDataset:\n",
    "    assert 'protected' in self.dataset.ndata\n",
    "\n",
    "    t = tqdm(range(attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "    t.set_description(\"Perturbing\")\n",
    "\n",
    "    for epoch in t:\n",
    "      None\n",
    "\n",
    "\n",
    "attack = LearnabilityLock(dataset)\n",
    "attack.designate_protected_random(node_ct=100)\n",
    "attack.get_sample_graph(1000, root_graph=attack.graph)\n",
    "\n",
    "\n",
    "# attack.create_locked_graph(attack_epochs=10, surrogate=surrogate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   3,    5,    6,  ..., 2704, 2707, 2706]),\n",
       " tensor([   0,    0,    0,  ..., 2705, 2706, 2707]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_adj.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph = dataset\n",
    "ct = 100\n",
    "\n",
    "adj = graph.adj().int().to_dense()\n",
    "sample = (torch.rand(adj.shape) > 0.9).int()\n",
    "\n",
    "positive_samples = (sample * adj).to_sparse().float()\n",
    "negative_samples = (sample * (adj - 1)).to_sparse().float()\n",
    "negative_samples.values()[:] = 0\n",
    "all_samples = positive_samples + negative_samples\n",
    "idx = all_samples.indices()\n",
    "\n",
    "sample_graph = dgl.graph((idx[0], idx[1]), num_nodes=graph.num_nodes())\n",
    "\n",
    "# 1 is positive sample, 0 is negative sample\n",
    "sample_graph.edata['sample_type'] = all_samples.values()\n",
    "sample_graph.edata['sample_type'].requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "g0 = (torch.rand(dataset.num_nodes()) > 0.8) == 1\n",
    "gX = ~g0\n",
    "\n",
    "for epoch in range(2):\n",
    "    accum_grad = torch.zeros_like(sample_graph.adj().to_dense())\n",
    "    # modified_adj = modified_adj.add_self_loop()\n",
    "    pred = surrogate.forward_positive(sample_graph, dataset.ndata['feat'], sample_graph.edata['sampleType'])\n",
    "\n",
    "    loss = F.cross_entropy(pred[g0], dataset.ndata['label'][g0]) \\\n",
    "        - F.cross_entropy(pred[gX], dataset.ndata['label'][gX])\n",
    "\n",
    "    loss.backward()\n",
    "    sample_graph.edata['sample_type'].grad\n",
    "    sampled_edges = sample_graph.edges()\n",
    "    accum_grad[sampled_edges] += sample_graph.edata['sample_type'].grad\n",
    "# modified_adj.adj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 1],\n",
       "                       [0, 1, 1, 2]]),\n",
       "       values=tensor([1, 2, 1, 2]),\n",
       "       size=(3, 3), nnz=4, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1, 1, 0], [0, 1, 1], [0, 0, 0]]).to_sparse()\n",
    "b = torch.tensor([[0, 1, 0], [0, 0, 1], [0, 0, 0]]).to_sparse()\n",
    "\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s(t: torch.Tensor):\n",
    "  print(f'shape:{t.shape}\\tsum:{t.sum()}\\tmin:{t.min()}\\tmax: {t.max()}\\tmean:{t.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape:torch.Size([486708])\tsum:-2.199384653067682e-06\tmin:-5.335767809810932e-07\tmax: 4.758028069318243e-07\tmean:-4.51889957744922e-12\n"
     ]
    }
   ],
   "source": [
    "coalesced = accum_grad.coalesce()\n",
    "averaged = coalesced.values() / counts.coalesce().values()\n",
    "averaged[averaged != averaged] = 0\n",
    "adj_grad = torch.sparse_coo_tensor(coalesced.indices(), averaged)\n",
    "s(adj_grad.coalesce().values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 85.97it/s, loss=0.79]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(999999.6250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 111.71it/s, loss=0.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 109.66it/s, loss=0.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(999999.8750)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 97.15it/s, loss=0.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 94.41it/s, loss=0.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(999999.6250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 112.20it/s, loss=0.74]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 114.54it/s, loss=0.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(999999.9375)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 101.54it/s, loss=0.77]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.1250)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 99.71it/s, loss=0.76]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1000000.1875)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 1/1 [00:00<00:00, 105.88it/s, loss=0.76]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "g0 = attack.graph.ndata['protected']\n",
    "gX = ~g0\n",
    "num_perturbations = 1000\n",
    "modified_graph = attack.graph.clone()\n",
    "\n",
    "for epoch in range(10):\n",
    "  accum_grad = torch.zeros_like(attack.graph.adj())\n",
    "  counts = torch.zeros_like(accum_grad)\n",
    "  perturbations = torch.zeros_like(accum_grad)\n",
    "\n",
    "  # Sampling to create adj_grad\n",
    "  for k in range(5):\n",
    "    sample_graph = attack.get_sample_graph(edge_ct=100000, root_graph=modified_graph)\n",
    "\n",
    "    pred = surrogate.forward_positive(\n",
    "      g=sample_graph, \n",
    "      feat=attack.graph.ndata['feat'], \n",
    "      edge_weight=sample_graph.edata['sample_type'])\n",
    "    \n",
    "    # Composite loss\n",
    "    loss = F.cross_entropy(pred[g0], attack.graph.ndata['label'][g0]) \\\n",
    "      - F.cross_entropy(pred[gX], attack.graph.ndata['label'][gX])\n",
    "    loss.backward()\n",
    "\n",
    "    # Gradient clipping\n",
    "    clip_grad = sample_graph.edata['sample_type'].grad\n",
    "    # positive_edges = sample_graph.edata['sample_type'] == 1\n",
    "    # # Clip negative node gradients to only positive values\n",
    "    # F.relu_(clip_grad[~positive_edges]) \n",
    "    # # Clip positive node gradients to only negative values\n",
    "    # clip_grad[positive_edges] = -F.relu(-clip_grad[positive_edges])\n",
    "    grad_graph = torch.sparse_coo_tensor(torch.stack(sample_graph.edges()), clip_grad)\n",
    "\n",
    "    # accum_grad[sample_graph.edges()] += clip_grad # Accumulate gradient\n",
    "    accum_grad = accum_grad + grad_graph\n",
    "    counts = counts + grad_graph.bool()\n",
    "    # counts[sample_graph.edges()] += 1 # Count for average\n",
    "\n",
    "  accum_grad = accum_grad.coalesce()\n",
    "  averaged = accum_grad.values() / counts.coalesce().values() # Average\n",
    "  averaged[averaged != averaged] = 0 # Remove NaN\n",
    "  accum_grad.values()[:] = averaged\n",
    "  # adj_grad = torch.sparse_coo_tensor(accum_grad.indices(), averaged)\n",
    "  # adj_grad = adj_grad.triu(diagonal=1) # Keep only valid adj, without diagonal\n",
    "\n",
    "  lr = 1000000 / accum_grad.values().sum()\n",
    "  perturbations = perturbations + (lr * accum_grad).coalesce() # Accumulate perturbations\n",
    "\n",
    "  print((lr * accum_grad).coalesce().values().sum())\n",
    "  # A simple projection to scale up the gradient\n",
    "  for i in range(10):\n",
    "    perturbations = perturbations * (num_perturbations / perturbations.values().abs().sum())\n",
    "    perturbations.values().clamp_(-1, 1)\n",
    "    if np.abs(1 - (perturbations.values().abs().sum() / num_perturbations)) < 0.1:\n",
    "      break\n",
    "\n",
    "  # Get the modified adj from perturbations\n",
    "  modified_adj = attack.graph.adj().to_dense()\n",
    "  idx = torch.bernoulli(perturbations.values().abs()) == 1\n",
    "  adj_target_idx = perturbations.indices()[:, idx]\n",
    "  modified_adj[adj_target_idx[0], adj_target_idx[1]] = \\\n",
    "    1 - modified_adj[adj_target_idx[0], adj_target_idx[1]]\n",
    "\n",
    "  # Update the locked graph\n",
    "  edges = modified_adj.to_sparse().indices()\n",
    "  modified_graph = dgl.graph((edges[0], edges[1]))\n",
    "\n",
    "  # Train the model\n",
    "  surrogate.fit(\n",
    "    g=modified_graph, \n",
    "    feat=attack.graph.ndata['feat'], \n",
    "    labels=attack.graph.ndata['label'], \n",
    "    epochs=1, \n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 200/200 [00:01<00:00, 134.23it/s, loss=0.66]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.81, 0.8546779141104295)"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "locked_graph = modified_graph\n",
    "\n",
    "lock_model = models.GCN(\n",
    "  in_size=dataset.ndata['feat'].shape[1],\n",
    "  out_size=int(dataset.ndata['label'].max()) + 1,\n",
    "  hid_size=config['hid_size'],\n",
    "  lr=config['lr'],\n",
    "  dropout=config['dropout'],\n",
    "  weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "lock_model.fit(\n",
    "    g=locked_graph, \n",
    "    feat=attack.graph.ndata['feat'], \n",
    "    labels=attack.graph.ndata['label'], \n",
    "    epochs=200, \n",
    "    verbose=True)\n",
    "\n",
    "pred = lock_model(dataset, dataset.ndata['feat'])\n",
    "f1_g0 = metrics.f1_score(dataset.ndata['label'][g0], pred.argmax(dim=1)[g0], average='micro')\n",
    "f1_gX = metrics.f1_score(dataset.ndata['label'][gX], pred.argmax(dim=1)[gX], average='micro')\n",
    "f1_g0, f1_gX\n",
    "\n",
    "# match = pred.argmax(dim=1) == dataset.ndata['label']\n",
    "# match.sum() / match.shape[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch_c116')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
