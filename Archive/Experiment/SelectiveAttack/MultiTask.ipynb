{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Environment ====\n",
      "  torch version: 1.9.0\n",
      "  device: cuda:0\n",
      "  torch seed: 123\n",
      "==== Dataset: cora ====\n",
      "Loading cora dataset...\n",
      "\n",
      "[i] Dataset Summary: \n",
      "\tadj shape: [2708, 2708]\n",
      "\tfeature shape: [2708, 1433]\n",
      "\tnum labels: 7\n",
      "\tsplit seed: 123\n",
      "\ttrain|val|test: 140|500|1000\n",
      "tensor([1177, 1263,  507, 1209], device='cuda:0')\n",
      "Number of protected nodes: 285\n",
      "Protected Size: 10.52%\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join('../..'))\n",
    "\n",
    "################################################\n",
    "# Arguments\n",
    "################################################\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed for model')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu')\n",
    "\n",
    "parser.add_argument('--method', type=str, default='SLL', help='method')\n",
    "parser.add_argument('--model_lr', type=float, default=0.01, help='Initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters)')\n",
    "parser.add_argument('--hidden_layers', type=int, default=32, help='Number of hidden layers')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate for GCN')\n",
    "\n",
    "parser.add_argument('--protect_size', type=float, default=0.1, help='Number of randomly chosen protected nodes')\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.25, help='Perturbation rate (percentage of available edges)')\n",
    "\n",
    "# simplistic1, simplistic2, noise, SLLnoSample, SLL\n",
    "# parser.add_argument('--numtasks', type=int, default=3, help='num additional tasks')\n",
    "parser.add_argument('--sample_size', type=int, default=500, help='')\n",
    "parser.add_argument('--num_samples', type=int, default=20, help='')\n",
    "\n",
    "\n",
    "parser.add_argument('--reg_epochs', type=int, default=100, help='Epochs to train models')\n",
    "parser.add_argument('--ptb_epochs', type=int, default=30, help='Epochs to perturb adj matrix')\n",
    "parser.add_argument('--surrogate_epochs', type=int, default=0, help='Epochs to train surrogate before perturb')\n",
    "\n",
    "parser.add_argument('--save', type=str, default='N', help='save the outputs to csv')\n",
    "parser.add_argument('--save_location', type=str, default=\"./SelectiveAttack.csv\", help='where to save the outputs to csv')\n",
    "parser.add_argument('--dataset', type=str, default='cora', help='dataset')\n",
    "\n",
    "\n",
    "parser.add_argument('--check_universal', type=str, default='N', help='check universal protection')\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "################################################\n",
    "# Environment\n",
    "################################################\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(f'cuda:{args.gpu}' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if device != 'cpu':\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print('==== Environment ====')\n",
    "print(f'  torch version: {torch.__version__}')\n",
    "print(f'  device: {device}')\n",
    "print(f'  torch seed: {args.seed}')\n",
    "\n",
    "################################################\n",
    "# Dataset\n",
    "################################################\n",
    "\n",
    "from Utils import GraphData\n",
    "\n",
    "print(f'==== Dataset: {args.dataset} ====')\n",
    "\n",
    "graph = GraphData.getGraph(\"../../Datasets\", args.dataset, \"gcn\", args.seed, device)\n",
    "graph.summarize()\n",
    "\n",
    "tasks = graph.features.sum(dim=0).topk(k=args.numtasks).indices\n",
    "print(tasks)\n",
    "\n",
    "################################################\n",
    "# Designate protected\n",
    "################################################\n",
    "\n",
    "g0 = torch.rand(graph.features.shape[0]) <= args.protect_size\n",
    "# g0 = graph.labels == 5 \n",
    "g0 = g0.to(device)\n",
    "gX = ~g0\n",
    "\n",
    "print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
    "print(f\"Protected Size: {g0.sum() / graph.features.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Sampling Matrix\n",
    "################################################\n",
    "\n",
    "from SamplingMatrix import SamplingMatrix\n",
    "\n",
    "samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
    "\n",
    "samplingMatrix.get_sample()\n",
    "samplingMatrix.getRatio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "a.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1177 1263 507 1209'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing: 100%|██████████| 30/30 [00:27<00:00,  1.09it/s, adj_l=3.46, adj_g=-3, pre-p=1319, target=1319, loss=tensor(3.4554, device='cuda:0', grad_fn=<AddBackward0>)]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sample loss: 1.73\t Edges: 1304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "################################################\n",
    "# Generate Perturbations (RANDOM NOISE)\n",
    "################################################\n",
    "import Utils.Utils as Utils\n",
    "import torch.nn.functional as F\n",
    "from Models.GCN import GCN\n",
    "\n",
    "surrogate = GCN(\n",
    "      input_features=graph.features.shape[1],\n",
    "      output_classes=graph.labels.max().item()+1,\n",
    "      hidden_layers=args.hidden_layers,\n",
    "      device=device,\n",
    "      lr=args.model_lr,\n",
    "      dropout=args.dropout,\n",
    "      weight_decay=args.weight_decay,\n",
    "      name=f\"surrogate\"\n",
    "  ).to(device)\n",
    "\n",
    "surrogates = {}\n",
    "for task in tasks:\n",
    "  m = graph.features[:,task].max().item() + 1\n",
    "  m = int(m)\n",
    "  print(m)\n",
    "  surrogates[task.item()] = GCN(\n",
    "      input_features=graph.features.shape[1],\n",
    "      output_classes=m,\n",
    "      hidden_layers=args.hidden_layers,\n",
    "      device=device,\n",
    "      lr=args.model_lr,\n",
    "      dropout=args.dropout,\n",
    "      weight_decay=args.weight_decay,\n",
    "      name=f\"surrogate{task}\"\n",
    "  ).to(device)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "perturbations = torch.zeros_like(graph.adj).float()\n",
    "count = torch.zeros_like(graph.adj).float()\n",
    "num_perturbations = args.ptb_rate * graph.adj.sum()\n",
    "\n",
    "t = tqdm(range(args.ptb_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "t.set_description(\"Perturbing\")\n",
    "\n",
    "for epoch in t:\n",
    "  # Re-initialize adj_grad\n",
    "  adj_grad = torch.zeros_like(graph.adj).float()\n",
    "\n",
    "  # Get modified adj\n",
    "  modified_adj = Utils.get_modified_adj(graph.adj, perturbations).float().to(device)\n",
    "\n",
    "  if args.method == 'SLL':\n",
    "\n",
    "    for sample_epoch in range(args.num_samples):\n",
    "      # Get sample indices\n",
    "      # sampled = torch.bernoulli(sampling_matrix)\n",
    "      idx = samplingMatrix.get_sample()\n",
    "      # print(idx)\n",
    "\n",
    "      # Map sample to adj\n",
    "      sample = modified_adj[idx[0], idx[1]].clone().detach().requires_grad_(True).to(device)\n",
    "      modified_adj[idx[0], idx[1]] = sample\n",
    "\n",
    "      # Get grad\n",
    "      # predictions = surrogate(graph.features, modified_adj)\n",
    "\n",
    "      loss = 0\n",
    "      for s in surrogates:\n",
    "        surrogates[s].eval()\n",
    "        modified_feat = graph.features.clone()\n",
    "        modified_feat[:, s] = 0\n",
    "        pred = surrogates[s](modified_feat, modified_adj)\n",
    "\n",
    "        loss += F.cross_entropy(pred[g0], graph.features[g0, s].long()) \\\n",
    "          - F.cross_entropy(pred[gX], graph.features[gX, s].long())\n",
    "\n",
    "      predictions = surrogate(graph.features, modified_adj)\n",
    "      loss += F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "          - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "      grad = torch.autograd.grad(loss, sample)[0]\n",
    "\n",
    "      # Implement averaging\n",
    "      adj_grad[idx[0], idx[1]] += grad\n",
    "      count[idx[0], idx[1]] += 1\n",
    "\n",
    "      # Update the sampling matrix\n",
    "      samplingMatrix.updateByGrad(adj_grad, count)\n",
    "      samplingMatrix.getRatio()\n",
    "\n",
    "      # Average the gradient\n",
    "      adj_grad = torch.div(adj_grad, count)\n",
    "      adj_grad[adj_grad != adj_grad] = 0\n",
    "  \n",
    "  else:\n",
    "    # Get grad\n",
    "    modified_adj = modified_adj.clone().detach().requires_grad_(True).to(device)\n",
    "\n",
    "    loss = 0\n",
    "    for s in surrogates:\n",
    "      surrogates[s].eval()\n",
    "      modified_feat = graph.features.clone()\n",
    "      modified_feat[:, s] = 0\n",
    "      pred = surrogates[s](modified_feat, modified_adj)\n",
    "\n",
    "      loss += F.cross_entropy(pred[g0].long(), graph.features[g0, s]) \\\n",
    "        - F.cross_entropy(pred[gX], graph.features[gX, s])\n",
    "\n",
    "    predictions = surrogate(graph.features, modified_adj)\n",
    "    loss += F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "        - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "    grad = torch.autograd.grad(loss, sample)[0]\n",
    "    adj_grad = torch.autograd.grad(loss, modified_adj)[0]\n",
    "\n",
    "  # Update perturbations\n",
    "  lr = (num_perturbations) / (epoch + 1)\n",
    "  pre_projection = int(perturbations.sum() / 2)\n",
    "  perturbations = perturbations + (lr * adj_grad)\n",
    "  perturbations = Utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "  # Train the model\n",
    "  modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "  surrogate.train1epoch(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_test)\n",
    "\n",
    "  t.set_postfix({\"adj_l\": loss.item(),\n",
    "                  \"adj_g\": int(adj_grad.sum()),\n",
    "                  \"pre-p\": pre_projection,\n",
    "                  \"target\": int(num_perturbations / 2),\n",
    "                  \"loss\": loss})\n",
    "\n",
    "with torch.no_grad(): \n",
    "  max_loss = -1000\n",
    "  for k in range(0,3):\n",
    "    sample = torch.bernoulli(perturbations)\n",
    "    modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "    modified_adj = Utils.make_symmetric(modified_adj) # Removing this creates \"impossible\" adj, but works well\n",
    "\n",
    "    predictions = surrogate(graph.features, modified_adj) \n",
    "\n",
    "    loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "        - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "    if loss > max_loss:\n",
    "      max_loss = loss\n",
    "      best = sample\n",
    "      best_mod = modified_adj\n",
    "  \n",
    "  print(f\"Best sample loss: {loss:.2f}\\t Edges: {best.abs().sum() / 2:.0f}\")\n",
    "  \n",
    "  locked_adj = Utils.get_modified_adj(graph.adj, best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(10556, device='cuda:0'), tensor(10386, device='cuda:0'))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.adj.sum(), locked_adj.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:1177\t dg0:-6.10% \t dgX:0.98%\n",
      "task:1177\t dg0:-3.95% \t dgX:0.02%\n",
      "task:1177\t dg0:-2.13% \t dgX:-0.20%\n",
      "task:1263\t dg0:-3.32% \t dgX:-1.06%\n",
      "task:1263\t dg0:-3.87% \t dgX:-0.17%\n",
      "task:1263\t dg0:-3.63% \t dgX:0.02%\n",
      "task:507\t dg0:4.32% \t dgX:0.50%\n",
      "task:507\t dg0:3.37% \t dgX:-0.43%\n",
      "task:507\t dg0:2.51% \t dgX:0.20%\n",
      "task:1209\t dg0:1.60% \t dgX:-0.77%\n",
      "task:1209\t dg0:3.92% \t dgX:2.34%\n",
      "task:1209\t dg0:2.38% \t dgX:0.40%\n",
      "task:label\t dg0:-22.17% \t dgX:-0.99%\n",
      "task:label\t dg0:-20.42% \t dgX:-0.33%\n",
      "task:label\t dg0:-20.68% \t dgX:-2.60%\n",
      "==== Accuracies ====\n",
      "         ΔG0\tΔGX\n",
      "task1 | -63.3%\t-3.9%\n",
      "{'seed': 123, 'method': 'SLL', 'dataset': 'cora', 'protect_size': 0.1, 'reg_epochs': 100, 'ptb_epochs': 30, 'ptb_rate': 0.25, 'ptb_sample_num': 20, 'ptb_sample_size': 500, 'edges': 2609.0, 'base_g0': 0.75789475440979, 'base_gX': 0.7829137444496155, 'label_d_g0': -0.6327510347544286, 'label_d_gX': -0.03922729075774416, 'label_dH_g0': -381, 'label_dH_gX': -203, 'label_dH_g0gX': -1463, 'task0_d_g0': -0.12181822349551104, 'task0_d_gX': 0.008041993475348582, 'task0_dH_g0': 31, 'task0_dH_gX': 31, 'task0_dH_g0gX': 31, 'task1_d_g0': -0.10814876094344432, 'task1_d_gX': -0.012170252964063823, 'task1_dH_g0': 29, 'task1_dH_gX': 29, 'task1_dH_g0gX': 29, 'task2_d_g0': 0.10205679289044901, 'task2_d_gX': 0.002708961406547676, 'task2_dH_g0': 159, 'task2_dH_gX': 159, 'task2_dH_g0gX': 159, 'task3_d_g0': 0.07905085432149961, 'task3_d_gX': 0.019696236338382023, 'task3_dH_g0': 165, 'task3_dH_gX': 165, 'task3_dH_g0gX': 165}\n"
     ]
    }
   ],
   "source": [
    "# locked_adj\n",
    "\n",
    "################################################\n",
    "# Evaluation\n",
    "################################################\n",
    "from Models.GCN import GCN\n",
    "import Utils.Metrics as Metrics\n",
    "\n",
    "n = 3\n",
    "\n",
    "task_acc = {}\n",
    "for task in tasks:\n",
    "    m = graph.features[:,task].max().item() + 1\n",
    "    m = int(m)\n",
    "    task_acc[task] = {\n",
    "        \"dg0\": 0,\n",
    "        \"dgX\": 0\n",
    "    }\n",
    "    for k in range(n):\n",
    "        baseline_model = GCN(\n",
    "            input_features=graph.features.shape[1],\n",
    "            output_classes=m,\n",
    "            hidden_layers=args.hidden_layers,\n",
    "            device=device,\n",
    "            lr=args.model_lr,\n",
    "            dropout=args.dropout,\n",
    "            weight_decay=args.weight_decay,\n",
    "            name=f\"surrogate{task}\"\n",
    "        ).to(device)\n",
    "\n",
    "        modified_feat = graph.features.clone()\n",
    "        modified_feat[:, task] = 0\n",
    "        baseline_model.fitManual(modified_feat, graph.adj, graph.features[:, task].long(), graph.idx_train, graph.idx_test, args.reg_epochs, verbose=False)\n",
    "        pred = baseline_model(modified_feat, graph.adj)\n",
    "        baseline_acc = Metrics.partial_acc(pred, graph.features[:, task], g0, gX, verbose=False)\n",
    "\n",
    "        locked_model = GCN(\n",
    "            input_features=graph.features.shape[1],\n",
    "            output_classes=m,\n",
    "            hidden_layers=args.hidden_layers,\n",
    "            device=device,\n",
    "            lr=args.model_lr,\n",
    "            dropout=args.dropout,\n",
    "            weight_decay=args.weight_decay,\n",
    "            name=f\"locked{task}\"\n",
    "        ).to(device)\n",
    "\n",
    "        locked_model.fitManual(modified_feat, locked_adj, graph.features[:, task].long(), graph.idx_train, graph.idx_test, args.reg_epochs, verbose=False)\n",
    "        pred = locked_model(modified_feat, locked_adj)\n",
    "        locked_acc = Metrics.partial_acc(pred, graph.features[:, task], g0, gX, verbose=False)\n",
    "        # # lock_gX += locked_acc[\"gX\"]\n",
    "\n",
    "        step_dg0 =  ((locked_acc[\"g0\"] / baseline_acc[\"g0\"]) - 1) / n\n",
    "        step_dgX = ((locked_acc[\"gX\"] / baseline_acc[\"gX\"]) - 1) / n\n",
    "        print(f\"task:{task}\\t dg0:{step_dg0:.2%} \\t dgX:{step_dgX:.2%}\")\n",
    "\n",
    "        task_acc[task][\"dg0\"] += step_dg0\n",
    "        task_acc[task][\"dgX\"] += step_dgX\n",
    "\n",
    "dg0 = 0\n",
    "dgX = 0\n",
    "n = 3\n",
    "\n",
    "for k in range(n):\n",
    "    baseline_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=graph.labels.max().item()+1,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"baseline\"\n",
    "    ).to(device)\n",
    "\n",
    "    baseline_model.fit(graph, args.reg_epochs, verbose=False)\n",
    "\n",
    "    pred = baseline_model(graph.features, graph.adj)\n",
    "    baseline_acc = Metrics.partial_acc(pred, graph.labels, g0, gX, verbose=False)\n",
    "\n",
    "    locked_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=graph.labels.max().item()+1,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"locked\"\n",
    "    )\n",
    "\n",
    "    locked_model.fitManual(graph.features, locked_adj, graph.labels, graph.idx_train, graph.idx_test, args.reg_epochs, verbose=False)\n",
    "\n",
    "    pred = locked_model(graph.features, locked_adj)\n",
    "    locked_acc = Metrics.partial_acc(pred, graph.labels, g0, gX, verbose=False)\n",
    "\n",
    "    # base_g0 += baseline_acc[\"g0\"]\n",
    "    # base_gX += baseline_acc[\"gX\"]\n",
    "    # lock_g0 += locked_acc[\"g0\"]\n",
    "    # lock_gX += locked_acc[\"gX\"]\n",
    "\n",
    "    step_dg0 =  ((locked_acc[\"g0\"] / baseline_acc[\"g0\"]) - 1) / n\n",
    "    step_dgX = ((locked_acc[\"gX\"] / baseline_acc[\"gX\"]) - 1) / n\n",
    "    print(f\"task:label\\t dg0:{step_dg0:.2%} \\t dgX:{step_dgX:.2%}\")\n",
    "    dg0 += step_dg0\n",
    "    dgX += step_dgX\n",
    "\n",
    "################################################\n",
    "# Summarize\n",
    "################################################\n",
    "\n",
    "# dg0 = ((lock_g0 / base_g0) - 1) / n\n",
    "# dgX = ((lock_gX / base_gX) - 1) / n\n",
    "\n",
    "print(\"==== Accuracies ====\")\n",
    "print(f\"         ΔG0\\tΔGX\")\n",
    "print(f\"task1 | {dg0:.1%}\\t{dgX:.1%}\")\n",
    "\n",
    "diff = locked_adj - graph.adj\n",
    "diffSummary = Metrics.show_metrics(diff, graph.labels, g0, device, verbose=False)\n",
    "\n",
    "# print(diffSummary)\n",
    "\n",
    "################################################\n",
    "# Save\n",
    "################################################\n",
    "\n",
    "import Utils.Export as Export\n",
    "\n",
    "def getDiff(labels, location, changeType):\n",
    "    diffSummary = Metrics.show_metrics(diff, labels, g0, device, verbose=False)\n",
    "    loc = diffSummary[location][changeType]\n",
    "    dH = loc[\"same\"] - loc[\"diff\"]\n",
    "    return dH\n",
    "\n",
    "def lab_dH(location):\n",
    "    return getDiff(graph.labels, location, \"add\") - getDiff(graph.labels, location, \"remove\")\n",
    "\n",
    "def dH(task, location):\n",
    "    lab = graph.features[:,task].long()\n",
    "    return getDiff(lab, \"g0\", \"add\") - getDiff(lab, \"g0\", \"remove\")\n",
    "\n",
    "results = {\n",
    "    \"seed\": args.seed,\n",
    "    \"method\": args.method,\n",
    "    \"dataset\": args.dataset,\n",
    "    \"protect_size\": args.protect_size,\n",
    "    \"reg_epochs\": args.reg_epochs,\n",
    "    \"ptb_epochs\": args.ptb_epochs,\n",
    "    \"ptb_rate\": args.ptb_rate,\n",
    "    \"ptb_sample_num\": args.num_samples,\n",
    "    \"ptb_sample_size\": args.sample_size,\n",
    "    # \"ratio_g0\": samplingMatrix.g0_ratio.item(),\n",
    "    # \"ratio_gX\": samplingMatrix.gX_ratio.item(),\n",
    "    # \"ratio_g0gX\": samplingMatrix.g0gX_ratio.item(),\n",
    "    \"edges\": diff.abs().sum().item(),\n",
    "    \"base_g0\": baseline_acc[\"g0\"],\n",
    "    \"base_gX\": baseline_acc[\"gX\"],\n",
    "    \"label_d_g0\": dg0,\n",
    "    \"label_d_gX\": dgX,\n",
    "    \"label_dH_g0\": lab_dH('g0'),\n",
    "    \"label_dH_gX\": lab_dH('gX'),\n",
    "    \"label_dH_g0gX\": lab_dH('g0gX'),\n",
    "}\n",
    "\n",
    "for i, task in enumerate(task_acc):\n",
    "    results[f'task{i}_d_g0'] = task_acc[task]['dg0']\n",
    "    results[f'task{i}_d_gX'] = task_acc[task]['dgX']\n",
    "    results[f'task{i}_dH_g0'] = dH(task, 'g0')\n",
    "    results[f'task{i}_dH_gX'] = dH(task, 'gX')\n",
    "    results[f'task{i}_dH_g0gX'] = dH(task, 'g0gX')\n",
    "\n",
    "\n",
    "print(results)\n",
    "Export.saveData('./multiTask.csv', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pygraph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7a38a3b595fc6f4bb38b67d61493a288b49ae2ed3e9a2b5c1361925b2354e393"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
