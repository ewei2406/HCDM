{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments ===================================\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--gpu_id', type=int, default=1)\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--config', type=str, default='config.yaml')\n",
    "parser.add_argument('--dataset', type=str, default='cora', choices=['cora', 'citeseer', 'BlogCatalog', 'flickr', 'Polblogs'])\n",
    "parser.add_argument('--g0_method', type=str, default='many_clusters', choices=[\n",
    "  'random', # randomly distribution of g0\n",
    "  'bias', # a random class has a 3x higher likelihood of being in g0\n",
    "  'large_cluster', # a random node and [g0_size] of its neighbors are in g0\n",
    "  'many_clusters', # 10 random nodes and [g0_size] of their neighbors are in g0\n",
    "  ])\n",
    "parser.add_argument('--g0_size', type=float, default=0.1)\n",
    "parser.add_argument('--attack_method', type=str, default='sll', choices=[\n",
    "  'sll', # Selective Learnability Lock\n",
    "  'sll_no_g', # Disable gradient guidance\n",
    "  'noise', # Noise protection\n",
    "  'heuristic' # Heuristic protection\n",
    "  ])\n",
    "parser.add_argument('--budget_pct', type=float, default=0.25)\n",
    "parser.add_argument('--attack_epochs', type=int, default=30)\n",
    "parser.add_argument('--save_results', type=str, default='Y', choices=['N', 'Y'])\n",
    "parser.add_argument('--save_graph', type=str, default='N', choices=['N', 'Y'])\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "if args.gpu_id >= 0:\n",
    "  device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if device != 'cpu': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "config = yaml.load(open('config.yml'), Loader=SafeLoader)[args.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  NumNodes: 2708\n",
      "  NumEdges: 10556\n",
      "  NumFeats: 1433\n",
      "  NumClasses: 7\n",
      "  NumTrainingSamples: 140\n",
      "  NumValidationSamples: 500\n",
      "  NumTestSamples: 1000\n",
      "Done loading data from cached files.\n"
     ]
    }
   ],
   "source": [
    "# Load graph ===================================\n",
    "graph = dataloader.load_DGL(args.dataset)\n",
    "feat = graph.ndata['feat'].to(device)\n",
    "labels = graph.ndata['label'].to(device)\n",
    "adj = graph.adj().to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 100/100 [00:00<00:00, 332.56it/s, loss=0.86]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import models\n",
    "\n",
    "comparisonMLP = models.SimpleMLP(\n",
    "  in_size=feat.shape[1],\n",
    "  out_size=labels.max().item()+1,\n",
    "  hid_size=config['hid_size'],\n",
    "  lr=config['lr'],\n",
    "  dropout=config['dropout'],\n",
    "  weight_decay=config['weight_decay']\n",
    ").to(device)\n",
    "\n",
    "comparisonMLP.fit(feat, labels, 100)\n",
    "acc = (comparisonMLP(feat).argmax(dim=1) == labels).sum() / feat.shape[0]\n",
    "print(f\"Accuracy: {acc:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2492,  786, 1520, 2125, 2598,  291, 2018], device='cuda:1')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compareModels(feat: torch.Tensor, adj: torch.Tensor, task_idx):\n",
    "    featClone = feat.clone()\n",
    "    featClone[task_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0 size: 270\n",
      "G0 pct: 9.97%\n"
     ]
    }
   ],
   "source": [
    "# Designate g0 ===================================\n",
    "g0_size = int(args.g0_size * graph.num_nodes())\n",
    "\n",
    "def get_clusters(num_roots: int, max_hops: int, target_size: int) -> torch.tensor:\n",
    "  root_nodes = torch.rand(graph.num_nodes()).topk(num_roots).indices\n",
    "\n",
    "  for hop in range(max_hops):\n",
    "    newNodes = adj[root_nodes].nonzero().t()[1]\n",
    "    root_nodes = torch.cat((root_nodes, newNodes))\n",
    "    root_nodes = torch.unique(root_nodes)\n",
    "    if root_nodes.shape[0] >= target_size:\n",
    "      break\n",
    "\n",
    "  g0 = torch.zeros(graph.num_nodes())\n",
    "  g0[root_nodes[:target_size]] = 1\n",
    "  g0 = g0.bool()\n",
    "  return g0\n",
    "\n",
    "if args.g0_method == 'many_clusters': # 10 nodes and their neighbors\n",
    "  g0 = get_clusters(10, 10, g0_size)\n",
    "elif args.g0_method == 'large_cluster': # 1 node and its neighbors\n",
    "  g0 = get_clusters(1, 10, g0_size)\n",
    "elif args.g0_method == 'random': # g0 is random/bias\n",
    "  g0_probs = torch.ones(graph.num_nodes())\n",
    "  g0_probs = g0_probs * (g0_size / g0_probs.sum())\n",
    "  g0_probs.clamp_(0, 1)\n",
    "  g0 = torch.bernoulli(g0_probs).bool()\n",
    "elif args.g0_method == 'bias': # g0 is skewed toward a class by factor of 3\n",
    "  bias = torch.randint(0, labels.max() + 1, [1]).item()\n",
    "  print(f'G0 class bias: {bias}')\n",
    "  g0_probs = torch.ones(graph.num_nodes())\n",
    "  g0_probs[labels == bias] = 3\n",
    "  g0_probs = g0_probs * (g0_size / g0_probs.sum())\n",
    "  g0_probs.clamp_(0, 1)\n",
    "  g0 = torch.bernoulli(g0_probs).bool()\n",
    "\n",
    "print(f'G0 size: {g0.sum().item()}')\n",
    "print(f'G0 pct: {g0.sum().item() / graph.num_nodes():.2%}')\n",
    "\n",
    "g0 = g0.cpu()\n",
    "gX = ~g0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'sampling_matrix' from '/u/nyw6dh/HCDM/Experiment/LLFeat/sampling_matrix.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import sampling_matrix\n",
    "importlib.reload(sampling_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1998, 2708])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SamplingMatrix:\n",
    "  def __init__(self, g0, gX, sample_size):\n",
    "    self.sample_size = sample_size\n",
    "    self.g0_idx = g0.nonzero().squeeze()\n",
    "    self.gX_idx = gX.nonzero().squeeze()\n",
    "    self.balance = torch.tensor([1, 1, 1]) # g0, gX, g0gX\n",
    "\n",
    "    self.normalize_balance()\n",
    "  \n",
    "  def normalize_balance(self):\n",
    "    self.balance = self.balance / self.balance.sum()\n",
    "\n",
    "  def update_balance(self, new_ratio):\n",
    "    self.balance = (self.balance + new_ratio) / 2\n",
    "    self.normalize_balance()\n",
    "\n",
    "  def _sample(self, sample_size, edge_idx):\n",
    "    idx_sample = torch.tensor(random.choices(range(edge_idx.shape[0]), k=sample_size))\n",
    "    return edge_idx[idx_sample]\n",
    "  \n",
    "  def _sample_edges(self, sample_size, start_idx, end_idx):\n",
    "    sample_size = int(sample_size)\n",
    "    return torch.stack((self._sample(sample_size, start_idx), self._sample(sample_size, end_idx)))\n",
    "\n",
    "  def sample(self):\n",
    "    g0_samples = self._sample_edges(self.sample_size * self.balance[0], self.g0_idx, self.g0_idx)\n",
    "    gX_samples = self._sample_edges(self.sample_size * self.balance[1], self.gX_idx, self.gX_idx)\n",
    "    g0gX_samples = self._sample_edges(self.sample_size * self.balance[2], self.g0_idx, self.gX_idx)\n",
    "\n",
    "    return torch.cat((g0_samples, gX_samples, g0gX_samples), dim=1)\n",
    "\n",
    "a = SamplingMatrix(g0, gX, 2000)\n",
    "sample = a.sample()\n",
    "\n",
    "import scipy\n",
    "reverse = scipy.stats.rankdata(sample, method='dense', axis=1) - 1\n",
    "\n",
    "adj[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1546, 2377, 1527,  ..., 1212, 1963, 2600],\n",
       "         [2616, 2244, 1816,  ...,  935,  715,  439]]),\n",
       " 1287)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample, reverse.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1665, 1665])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatten = torch.zeros(adj.shape[0], dtype=torch.bool)\n",
    "flatten[torch.flatten(sample)] = True\n",
    "\n",
    "sampled_adj = adj[flatten, :][:, flatten].to(device)\n",
    "sampled_adj.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['sll_sample_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attacking with method: sll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLL:   0%|          | 0/30 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SamplingMatrix' object has no attribute 'get_sample_pairs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=96'>97</a>\u001b[0m modified_adj \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mget_modified_adj(adj, perturbations)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=98'>99</a>\u001b[0m \u001b[39mfor\u001b[39;00m sample_epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39msll_num_samples\u001b[39m\u001b[39m'\u001b[39m]): \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m   \u001b[39m# Get sample indices\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m   idx \u001b[39m=\u001b[39m samplingMatrix\u001b[39m.\u001b[39;49mget_sample_pairs()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=101'>102</a>\u001b[0m   rev \u001b[39m=\u001b[39m idx\u001b[39m.\u001b[39margsort()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bhcdmg2.cs.virginia.edu/u/nyw6dh/HCDM/Experiment/LLFeat/LL_8_30.ipynb#W5sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m   flatten \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(modified_adj\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mbool)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SamplingMatrix' object has no attribute 'get_sample_pairs'"
     ]
    }
   ],
   "source": [
    "# Perform attack ==========================\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_perturbations = (graph.num_edges() / 2) * args.budget_pct\n",
    "\n",
    "print(f'Attacking with method: {args.attack_method}')\n",
    "\n",
    "if args.attack_method == 'heuristic':\n",
    "  locked_adj = adj.clone()\n",
    "  locked_adj[:, g0] = 0\n",
    "  locked_adj[g0, :] = 0\n",
    "elif args.attack_method == 'noise':\n",
    "  noise = torch.zeros_like(adj)\n",
    "  noise[g0, :] = 1\n",
    "  noise[:, gX] = 0\n",
    "  noise *= 2 * num_perturbations / noise.sum()\n",
    "  noise = torch.bernoulli(noise.clamp(0, 1))\n",
    "  noise = utils.make_symmetric(noise)\n",
    "  locked_adj = utils.get_modified_adj(adj, noise)\n",
    "elif args.attack_method == 'sll_no_g':\n",
    "\n",
    "  # Initialize perturbations\n",
    "  perturbations = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Create surrogate model to mimic downstream\n",
    "  surrogate = models.DenseGCN(\n",
    "      in_size=feat.shape[1],\n",
    "      out_size=labels.max().item()+1,\n",
    "      hid_size=config['hid_size'],\n",
    "      lr=config['lr'],\n",
    "      dropout=config['dropout'],\n",
    "      weight_decay=config['weight_decay']\n",
    "  ).to(device)\n",
    "\n",
    "  t = tqdm(range(args.attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "  t.set_description(\"SLL (no gradient guidance)\")\n",
    "\n",
    "  for epoch in t:\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).requires_grad_(True).float().to(device)\n",
    "\n",
    "    # Get grad of modified adj w.r.t attack loss\n",
    "    pred = surrogate(feat, modified_adj)\n",
    "    loss = F.cross_entropy(pred[g0], labels[g0]) \\\n",
    "        - F.cross_entropy(pred[gX], labels[gX])\n",
    "    adj_grad = torch.autograd.grad(loss, modified_adj)[0].cpu()\n",
    "\n",
    "    # Update perturbations\n",
    "    lr = (config['sll_no_g_lr']) / ((epoch + 1))\n",
    "    pre_projection = int(perturbations.sum() / 2)\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "    perturbations = utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the surrogate\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).to(device)\n",
    "    model_loss = surrogate.fit(feat, modified_adj, labels, epochs=1, verbose=False)\n",
    "\n",
    "    t.set_postfix({\"adj_l\": loss.item(),\n",
    "                    \"adj_g\": (adj_grad.sum().item()),\n",
    "                    \"pre-p\": pre_projection,\n",
    "                    \"target\": int(num_perturbations / 2),\n",
    "                    \"model_loss\": model_loss})\n",
    "elif args.attack_method == 'sll':\n",
    "  # Initialize perturbations\n",
    "  perturbations = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Initialize sampling matrix\n",
    "  samplingMatrix = sampling_matrix.SamplingMatrix(\n",
    "    g0=g0, gX=gX, sample_size=config['sll_sample_size'])\n",
    "  count = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Create surrogate model to mimic downstream\n",
    "  surrogate = models.DenseGCN(\n",
    "      in_size=feat.shape[1],\n",
    "      out_size=labels.max().item()+1,\n",
    "      hid_size=config['hid_size'],\n",
    "      lr=config['lr'],\n",
    "      dropout=config['dropout'],\n",
    "      weight_decay=config['weight_decay']\n",
    "  ).to(device)\n",
    "\n",
    "  t = tqdm(range(args.attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "  t.set_description(\"SLL\")\n",
    "\n",
    "  for epoch in t:\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).float()\n",
    "\n",
    "    for sample_epoch in range(config['sll_num_samples']): \n",
    "      # Get sample indices\n",
    "      idx = samplingMatrix.get_sample_pairs()\n",
    "      rev = idx.argsort()\n",
    "\n",
    "      flatten = torch.zeros(modified_adj.shape[0], dtype=torch.bool)\n",
    "      flatten[torch.flatten(idx)] = True\n",
    "\n",
    "      sampled_adj = modified_adj[flatten, :][:, flatten].to(device)\n",
    "      sampled_feat = feat[flatten].to(device)\n",
    "      \n",
    "      rev_cuda = rev.to(device)\n",
    "      idx_cuda = idx.to(device)\n",
    "\n",
    "      sample = sampled_adj[rev[0], rev[1]].clone().detach().requires_grad_(True).to(device)\n",
    "      sampled_adj[rev[0], rev[1]] = sample\n",
    "\n",
    "      # Get grad\n",
    "      pred = surrogate(sampled_feat, sampled_adj)\n",
    "      loss = F.cross_entropy(pred[g0[flatten]], labels[flatten][g0[flatten]]) \\\n",
    "          - F.cross_entropy(pred[gX[flatten]], labels[flatten][gX[flatten]])\n",
    "\n",
    "      grad = torch.autograd.grad(loss, sample)[0].cpu()\n",
    "\n",
    "      # Implement averaging of duplicate samples\n",
    "      adj_grad[idx[0], idx[1]] += grad\n",
    "      count[idx[0], idx[1]] += 1\n",
    "\n",
    "\n",
    "    # Update the sampling matrix\n",
    "    samplingMatrix.updateByGrad(adj_grad, count)\n",
    "\n",
    "    # Average the gradient\n",
    "    adj_grad = torch.div(adj_grad, count)\n",
    "    adj_grad[adj_grad != adj_grad] = 0\n",
    "    \n",
    "    # Update perturbations\n",
    "    lr = (config['sll_lr']) / ((epoch + 1))\n",
    "    pre_projection = int(perturbations.sum())\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "\n",
    "    perturbations[graph.edges()[0], graph.edges()[1]].mul_(-1)\n",
    "    perturbations.clamp_(0, 1)\n",
    "\n",
    "    for k in range(5):\n",
    "      perturbations = (perturbations * (num_perturbations / perturbations.sum())).clamp(-1, 1)\n",
    "      if abs((perturbations.sum() / num_perturbations) - 1) > 0.9: break\n",
    "\n",
    "    # perturbations = utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the model\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).to(device)\n",
    "    model_loss = surrogate.fit(feat, modified_adj, labels, epochs=1, verbose=False)\n",
    "\n",
    "    t.set_postfix({\"attack_loss\": loss.item(),\n",
    "                    # \"adj_g\": (adj_grad.sum().item()),\n",
    "                    \"pre-projection\": pre_projection,\n",
    "                    \"target\": int(num_perturbations),\n",
    "                    \"surrogate_loss\": model_loss})\n",
    "\n",
    "# Discretize the best locked_adj ============\n",
    "if args.attack_method in ['sll', 'sll_no_g']:\n",
    "  locked_adj = None\n",
    "  with torch.no_grad():\n",
    "    max_loss = -1000\n",
    "    for k in range(0,3):\n",
    "        sample = torch.bernoulli(perturbations)\n",
    "        modified_adj = utils.get_modified_adj(adj, sample)\n",
    "        modified_adj = utils.make_symmetric(modified_adj) \n",
    "        predictions = surrogate(feat, modified_adj.to(device)) \n",
    "\n",
    "        loss = F.cross_entropy(predictions[g0], labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], labels[gX])\n",
    "\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            locked_adj = modified_adj\n",
    "    \n",
    "    print(f\"Best sample loss: {max_loss:.2f}\")\n",
    "\n",
    "diff = adj - locked_adj\n",
    "print(f'Edges modified: {diff.abs().sum() / 2:.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 100/100 [00:01<00:00, 55.35it/s, loss=0.38]\n",
      "GCN Training: 100%|██████████| 100/100 [00:01<00:00, 56.40it/s, loss=0.46]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation ==============================\n",
    "import sklearn.metrics as metrics\n",
    "gX_train = torch.logical_and(gX, graph.ndata['train_mask'])\n",
    "gX_test = torch.logical_and(gX, graph.ndata['test_mask'])\n",
    "\n",
    "def eval_adj(test_adj: torch.tensor):\n",
    "    model = models.DenseGCN(\n",
    "        in_size=feat.shape[1],\n",
    "        out_size=labels.max().item()+1,\n",
    "        hid_size=config['hid_size'],\n",
    "        lr=config['lr'],\n",
    "        dropout=config['dropout'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    ).to(device)\n",
    "    model.fit(feat, test_adj, labels, epochs=100, mask=gX_train)\n",
    "    pred = model(feat, test_adj).cpu()\n",
    "\n",
    "    f1_g0 = metrics.f1_score(labels[g0].cpu(), pred.argmax(dim=1)[g0], average='micro')\n",
    "    f1_gX = metrics.f1_score(labels[gX_test].cpu(), pred.argmax(dim=1)[gX_test], average='micro')\n",
    "    \n",
    "    return f1_g0, f1_gX\n",
    "\n",
    "f1_g0_base, f1_gX_base = eval_adj(adj)\n",
    "f1_g0_lock, f1_gX_lock = eval_adj(locked_adj)\n",
    "\n",
    "d_g0 = (f1_g0_lock - f1_g0_base) / f1_g0_base\n",
    "d_gX = (f1_gX_lock - f1_gX_base) / f1_gX_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H Overall: 81.0%\n",
      "ACC      f1_g0\tf1_gX\tH_g0\tH_gX\tH_g0gX\n",
      "base   | 74.0%\t74.1%\t84.6%\t81.7%\t81.0%\n",
      "lock   | 52.6%\t71.2%\t19.6%\t79.9%\t68.3%\n",
      "delta  | -28.9%\t-4.0%\t-76.8%\t-2.2%\t-15.7%\n",
      "Changes\n",
      "g0: 640.0\n",
      "gX: 394.0\n",
      "g0gX: 1528.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "H_overall = utils.calc_homophily(adj, labels)\n",
    "\n",
    "H_g0_base = utils.calc_homophily(adj, labels, g0)\n",
    "H_gX_base = utils.calc_homophily(adj, labels, gX)\n",
    "H_g0gX_base = utils.inner_homophily(adj, labels, g0, gX)\n",
    "\n",
    "H_g0_lock = utils.calc_homophily(locked_adj, labels, g0)\n",
    "H_gX_lock = utils.calc_homophily(locked_adj, labels, gX)\n",
    "H_g0gX_lock = utils.inner_homophily(locked_adj, labels, g0, gX)\n",
    "\n",
    "d_H_g0 = (H_g0_lock - H_g0_base) / H_g0_base\n",
    "d_H_gX = (H_gX_lock - H_gX_base) / H_gX_base\n",
    "d_H_g0gX = (H_g0gX_lock - H_g0gX_base) / H_g0gX_base\n",
    "\n",
    "changes_g0 = diff[g0, :][:, g0].abs().sum().item()\n",
    "changes_gX = diff[gX, :][:, gX].abs().sum().item()\n",
    "changes_g0gX = diff.abs().sum().item() - (changes_g0 + changes_gX)\n",
    "\n",
    "print(f'H Overall: {H_overall:.1%}')\n",
    "print(f'ACC      f1_g0\\tf1_gX\\tH_g0\\tH_gX\\tH_g0gX')\n",
    "print(f'base   | {f1_g0_base:.1%}\\t{f1_gX_base:.1%}\\t{H_g0_base:.1%}\\t{H_gX_base:.1%}\\t{H_g0gX_base:.1%}')\n",
    "print(f'lock   | {f1_g0_lock:.1%}\\t{f1_gX_lock:.1%}\\t{H_g0_lock:.1%}\\t{H_gX_lock:.1%}\\t{H_g0gX_lock:.1%}')\n",
    "print(f'delta  | {d_g0:.1%}\\t{d_gX:.1%}\\t{d_H_g0:.1%}\\t{d_H_gX:.1%}\\t{d_H_g0gX:.1%}')\n",
    "\n",
    "print(f'Changes')\n",
    "print(f'g0: {changes_g0}')\n",
    "print(f'gX: {changes_gX}')\n",
    "print(f'g0gX: {changes_g0gX}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch_c116')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
