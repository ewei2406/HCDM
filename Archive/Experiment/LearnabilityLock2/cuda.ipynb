{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataloader' from '/u/nyw6dh/HCDM/Experiment/LearnabilityLock2/dataloader.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dataloader\n",
    "import torch\n",
    "import numpy as np\n",
    "import argparse\n",
    "import importlib\n",
    "importlib.reload(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments ===================================\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--gpu_id', type=int, default=0)\n",
    "parser.add_argument('--seed', type=int, default=123)\n",
    "parser.add_argument('--config', type=str, default='config2.yml')\n",
    "parser.add_argument('--dataset', type=str, default='flickr', choices=['cora', 'citeseer', 'BlogCatalog', 'flickr', 'Polblogs'])\n",
    "parser.add_argument('--g0_method', type=str, default='many_clusters', choices=[\n",
    "  'random', # randomly distribution of g0\n",
    "  'bias', # a random class has a 3x higher likelihood of being in g0\n",
    "  'large_cluster', # a random node and [g0_size] of its neighbors are in g0\n",
    "  'many_clusters', # 10 random nodes and [g0_size] of their neighbors are in g0\n",
    "  ])\n",
    "parser.add_argument('--g0_size', type=float, default=0.1)\n",
    "parser.add_argument('--attack_method', type=str, default='sll', choices=[\n",
    "  'sll', # Selective Learnability Lock\n",
    "  'sll_no_g', # Disable gradient guidance\n",
    "  'noise', # Noise protection\n",
    "  'heuristic' # Heuristic protection\n",
    "  ])\n",
    "parser.add_argument('--budget_pct', type=float, default=0.25)\n",
    "parser.add_argument('--attack_epochs', type=int, default=30)\n",
    "parser.add_argument('--save_results', type=str, default='Y', choices=['N', 'Y'])\n",
    "parser.add_argument('--save_graph', type=str, default='N', choices=['N', 'Y'])\n",
    "\n",
    "args = parser.parse_args(\"\")\n",
    "\n",
    "if args.gpu_id >= 0:\n",
    "  device = torch.device(f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if device != 'cpu': torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "import yaml\n",
    "from yaml import SafeLoader\n",
    "config = yaml.load(open(args.config), Loader=SafeLoader)[args.dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph ===================================\n",
    "graph = dataloader.load_DGL(args.dataset)\n",
    "feat = graph.ndata['feat'].to(device)\n",
    "labels = graph.ndata['label'].to(device)\n",
    "adj = graph.adj().to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G0 size: 757\n",
      "G0 pct: 9.99%\n"
     ]
    }
   ],
   "source": [
    "# Designate g0 ===================================\n",
    "g0_size = int(args.g0_size * graph.num_nodes())\n",
    "\n",
    "def get_clusters(num_roots: int, max_hops: int, target_size: int) -> torch.tensor:\n",
    "  root_nodes = torch.rand(graph.num_nodes()).topk(num_roots).indices\n",
    "\n",
    "  for hop in range(max_hops):\n",
    "    newNodes = adj[root_nodes].nonzero().t()[1]\n",
    "    root_nodes = torch.cat((root_nodes, newNodes))\n",
    "    root_nodes = torch.unique(root_nodes)\n",
    "    if root_nodes.shape[0] >= target_size:\n",
    "      break\n",
    "\n",
    "  g0 = torch.zeros(graph.num_nodes())\n",
    "  g0[root_nodes[:target_size]] = 1\n",
    "  g0 = g0.bool()\n",
    "  return g0\n",
    "\n",
    "if args.g0_method == 'many_clusters': # 10 nodes and their neighbors\n",
    "  g0 = get_clusters(10, 10, g0_size)\n",
    "elif args.g0_method == 'large_cluster': # 1 node and its neighbors\n",
    "  g0 = get_clusters(1, 10, g0_size)\n",
    "elif args.g0_method == 'random': # g0 is random/bias\n",
    "  g0_probs = torch.ones(graph.num_nodes())\n",
    "  g0_probs = g0_probs * (g0_size / g0_probs.sum())\n",
    "  g0_probs.clamp_(0, 1)\n",
    "  g0 = torch.bernoulli(g0_probs).bool()\n",
    "elif args.g0_method == 'bias': # g0 is skewed toward a class by factor of 3\n",
    "  bias = torch.randint(0, labels.max() + 1, [1]).item()\n",
    "  print(f'G0 class bias: {bias}')\n",
    "  g0_probs = torch.ones(graph.num_nodes())\n",
    "  g0_probs[labels == bias] = 3\n",
    "  g0_probs = g0_probs * (g0_size / g0_probs.sum())\n",
    "  g0_probs.clamp_(0, 1)\n",
    "  g0 = torch.bernoulli(g0_probs).bool()\n",
    "\n",
    "print(f'G0 size: {g0.sum().item()}')\n",
    "print(f'G0 pct: {g0.sum().item() / graph.num_nodes():.2%}')\n",
    "\n",
    "g0 = g0.cpu()\n",
    "gX = ~g0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[test] time: 0.07593ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "stopwatchTime = 0\n",
    "def showtime(mode, task=\"\"):\n",
    "  global stopwatchTime\n",
    "  if mode == 'start':\n",
    "    stopwatchTime = time.time() * 1000\n",
    "  if mode == 'stop':\n",
    "    print(f'[{task}] time: {(time.time() * 1000) - stopwatchTime:.5f}ms')\n",
    "    stopwatchTime = time.time() * 1000\n",
    "\n",
    "showtime('start')\n",
    "showtime('stop', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform attack ==========================\n",
    "\n",
    "import utils\n",
    "import models\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_perturbations = (graph.num_edges() / 2) * args.budget_pct\n",
    "\n",
    "print(f'Attacking with method: {args.attack_method}')\n",
    "\n",
    "if args.attack_method == 'heuristic':\n",
    "  locked_adj = adj.clone()\n",
    "  locked_adj[:, g0] = 0\n",
    "  locked_adj[g0, :] = 0\n",
    "elif args.attack_method == 'noise':\n",
    "  noise = torch.zeros_like(adj)\n",
    "  noise[g0, :] = 1\n",
    "  noise[:, gX] = 0\n",
    "  noise *= 2 * num_perturbations / noise.sum()\n",
    "  noise = torch.bernoulli(noise.clamp(0, 1))\n",
    "  noise = utils.make_symmetric(noise)\n",
    "  locked_adj = utils.get_modified_adj(adj, noise)\n",
    "elif args.attack_method == 'sll_no_g':\n",
    "\n",
    "  # Initialize perturbations\n",
    "  perturbations = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Create surrogate model to mimic downstream\n",
    "  surrogate = models.DenseGCN(\n",
    "      in_size=feat.shape[1],\n",
    "      out_size=labels.max().item()+1,\n",
    "      hid_size=config['hid_size'],\n",
    "      lr=config['lr'],\n",
    "      dropout=config['dropout'],\n",
    "      weight_decay=config['weight_decay']\n",
    "  ).to(device)\n",
    "\n",
    "  t = tqdm(range(args.attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "  t.set_description(\"SLL (no gradient guidance)\")\n",
    "\n",
    "  for epoch in t:\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).requires_grad_(True).float().to(device)\n",
    "\n",
    "    # Get grad of modified adj w.r.t attack loss\n",
    "    pred = surrogate(feat, modified_adj)\n",
    "    loss = F.cross_entropy(pred[g0], labels[g0]) \\\n",
    "        - F.cross_entropy(pred[gX], labels[gX])\n",
    "    adj_grad = torch.autograd.grad(loss, modified_adj)[0].cpu()\n",
    "\n",
    "    # Update perturbations\n",
    "    lr = (config['sll_no_g_lr']) / ((epoch + 1))\n",
    "    pre_projection = int(perturbations.sum() / 2)\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "    perturbations = utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the surrogate\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).to(device)\n",
    "    model_loss = surrogate.fit(feat, modified_adj, labels, epochs=1, verbose=False)\n",
    "\n",
    "    t.set_postfix({\"adj_l\": loss.item(),\n",
    "                    \"adj_g\": (adj_grad.sum().item()),\n",
    "                    \"pre-p\": pre_projection,\n",
    "                    \"target\": int(num_perturbations / 2),\n",
    "                    \"model_loss\": model_loss})\n",
    "elif args.attack_method == 'sll':\n",
    "  # Initialize perturbations\n",
    "  perturbations = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Initialize sampling matrix\n",
    "  import sampling_matrix\n",
    "  samplingMatrix = sampling_matrix.SamplingMatrix(\n",
    "    g0=g0, gX=gX, adj=adj, sample_size=config['sll_sample_size'])\n",
    "  count = torch.zeros_like(adj).float()\n",
    "\n",
    "  # Create surrogate model to mimic downstream\n",
    "  surrogate = models.DenseGCN(\n",
    "      in_size=feat.shape[1],\n",
    "      out_size=labels.max().item()+1,\n",
    "      hid_size=config['hid_size'],\n",
    "      lr=config['lr'],\n",
    "      dropout=config['dropout'],\n",
    "      weight_decay=config['weight_decay']\n",
    "  ).to(device)\n",
    "\n",
    "  t = tqdm(range(args.attack_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "  t.set_description(\"SLL\")\n",
    "\n",
    "  for epoch in t:\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).float()\n",
    "\n",
    "    for sample_epoch in range(config['sll_num_samples']): \n",
    "      # Get sample indices\n",
    "      showtime('start')\n",
    "      idx, rev = samplingMatrix.get_sample_pairs()\n",
    "      showtime('stop', 'start')\n",
    "\n",
    "      flatten = torch.zeros(modified_adj.shape[0], dtype=torch.bool)\n",
    "      flatten[torch.flatten(idx)] = True\n",
    "\n",
    "      sampled_adj = modified_adj[flatten, :][:, flatten].to(device)\n",
    "      sampled_feat = feat[flatten].to(device)\n",
    "      \n",
    "      rev_cuda = rev.to(device)\n",
    "      idx_cuda = idx.to(device)\n",
    "\n",
    "      sample = sampled_adj[rev[0], rev[1]].clone().detach().requires_grad_(True).to(device)\n",
    "      sampled_adj[rev[0], rev[1]] = sample\n",
    "\n",
    "      # Get grad\n",
    "      pred = surrogate(sampled_feat, sampled_adj)\n",
    "      loss = F.cross_entropy(pred[g0[flatten]], labels[flatten][g0[flatten]]) \\\n",
    "          - F.cross_entropy(pred[gX[flatten]], labels[flatten][gX[flatten]])\n",
    "\n",
    "      grad = torch.autograd.grad(loss, sample)[0].cpu()\n",
    "\n",
    "      # Implement averaging of duplicate samples\n",
    "      adj_grad[idx[0], idx[1]] += grad\n",
    "      count[idx[0], idx[1]] += 1\n",
    "\n",
    "\n",
    "    # Update the sampling matrix\n",
    "    samplingMatrix.updateByGrad(adj_grad, count)\n",
    "\n",
    "    # Average the gradient\n",
    "    adj_grad = torch.div(adj_grad, count)\n",
    "    adj_grad[adj_grad != adj_grad] = 0\n",
    "    \n",
    "    # Update perturbations\n",
    "    lr = (config['sll_lr']) / ((epoch + 1))\n",
    "    pre_projection = int(perturbations.sum())\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "\n",
    "    perturbations[graph.edges()[0], graph.edges()[1]].mul_(-1)\n",
    "    perturbations.clamp_(0, 1)\n",
    "\n",
    "    for k in range(5):\n",
    "      perturbations = (perturbations * (num_perturbations / perturbations.sum())).clamp(-1, 1)\n",
    "      if abs((perturbations.sum() / num_perturbations) - 1) > 0.9: break\n",
    "\n",
    "    # perturbations = utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the model\n",
    "    modified_adj = utils.get_modified_adj(adj, perturbations).to(device)\n",
    "    model_loss = surrogate.fit(feat, modified_adj, labels, epochs=1, verbose=False)\n",
    "\n",
    "    t.set_postfix({\"attack_loss\": loss.item(),\n",
    "                    # \"adj_g\": (adj_grad.sum().item()),\n",
    "                    \"pre-projection\": pre_projection,\n",
    "                    \"target\": int(num_perturbations),\n",
    "                    \"surrogate_loss\": model_loss})\n",
    "\n",
    "# Discretize the best locked_adj ============\n",
    "if args.attack_method in ['sll', 'sll_no_g']:\n",
    "  locked_adj = None\n",
    "  with torch.no_grad():\n",
    "    max_loss = -1000\n",
    "    for k in range(0,3):\n",
    "        sample = torch.bernoulli(perturbations)\n",
    "        modified_adj = utils.get_modified_adj(adj, sample)\n",
    "        modified_adj = utils.make_symmetric(modified_adj) \n",
    "        predictions = surrogate(feat, modified_adj.to(device)) \n",
    "\n",
    "        loss = F.cross_entropy(predictions[g0], labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], labels[gX])\n",
    "\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            locked_adj = modified_adj\n",
    "    \n",
    "    print(f\"Best sample loss: {max_loss:.2f}\")\n",
    "\n",
    "diff = adj - locked_adj\n",
    "print(f'Edges modified: {diff.abs().sum() / 2:.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GCN Training: 100%|██████████| 100/100 [00:10<00:00,  9.64it/s, loss=0.63]\n",
      "GCN Training: 100%|██████████| 100/100 [00:10<00:00,  9.76it/s, loss=0.54]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation ==============================\n",
    "import sklearn.metrics as metrics\n",
    "gX_train = torch.logical_and(gX, graph.ndata['train_mask'])\n",
    "gX_test = torch.logical_and(gX, graph.ndata['test_mask'])\n",
    "\n",
    "def eval_adj(test_adj: torch.tensor):\n",
    "    model = models.DenseGCN(\n",
    "        in_size=feat.shape[1],\n",
    "        out_size=labels.max().item()+1,\n",
    "        hid_size=config['hid_size'],\n",
    "        lr=config['lr'],\n",
    "        dropout=config['dropout'],\n",
    "        weight_decay=config['weight_decay']\n",
    "    ).to(device)\n",
    "    model.fit(feat, test_adj, labels, epochs=100, mask=gX_train)\n",
    "    pred = model(feat, test_adj).cpu()\n",
    "\n",
    "    f1_g0 = metrics.f1_score(labels[g0].cpu(), pred.argmax(dim=1)[g0], average='micro')\n",
    "    f1_gX = metrics.f1_score(labels[gX_test].cpu(), pred.argmax(dim=1)[gX_test], average='micro')\n",
    "    \n",
    "    return f1_g0, f1_gX\n",
    "\n",
    "f1_g0_base, f1_gX_base = eval_adj(adj)\n",
    "f1_g0_lock, f1_gX_lock = eval_adj(locked_adj)\n",
    "\n",
    "d_g0 = (f1_g0_lock - f1_g0_base) / f1_g0_base\n",
    "d_gX = (f1_gX_lock - f1_gX_base) / f1_gX_base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H Overall: 23.9%\n",
      "ACC      f1_g0\tf1_gX\tH_g0\tH_gX\tH_g0gX\n",
      "base   | 43.9%\t41.9%\t19.0%\t26.4%\t23.9%\n",
      "lock   | 23.0%\t40.9%\t17.3%\t24.6%\t21.8%\n",
      "delta  | -47.6%\t-2.3%\t-9.2%\t-6.8%\t-8.7%\n",
      "Changes\n",
      "g0: 12520.0\n",
      "gX: 25354.0\n",
      "g0gX: 43498.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "H_overall = utils.calc_homophily(adj, labels)\n",
    "\n",
    "H_g0_base = utils.calc_homophily(adj, labels, g0)\n",
    "H_gX_base = utils.calc_homophily(adj, labels, gX)\n",
    "H_g0gX_base = utils.inner_homophily(adj, labels, g0, gX)\n",
    "\n",
    "H_g0_lock = utils.calc_homophily(locked_adj, labels, g0)\n",
    "H_gX_lock = utils.calc_homophily(locked_adj, labels, gX)\n",
    "H_g0gX_lock = utils.inner_homophily(locked_adj, labels, g0, gX)\n",
    "\n",
    "d_H_g0 = (H_g0_lock - H_g0_base) / H_g0_base\n",
    "d_H_gX = (H_gX_lock - H_gX_base) / H_gX_base\n",
    "d_H_g0gX = (H_g0gX_lock - H_g0gX_base) / H_g0gX_base\n",
    "\n",
    "changes_g0 = diff[g0, :][:, g0].abs().sum().item()\n",
    "changes_gX = diff[gX, :][:, gX].abs().sum().item()\n",
    "changes_g0gX = diff.abs().sum().item() - (changes_g0 + changes_gX)\n",
    "\n",
    "print(f'H Overall: {H_overall:.1%}')\n",
    "print(f'ACC      f1_g0\\tf1_gX\\tH_g0\\tH_gX\\tH_g0gX')\n",
    "print(f'base   | {f1_g0_base:.1%}\\t{f1_gX_base:.1%}\\t{H_g0_base:.1%}\\t{H_gX_base:.1%}\\t{H_g0gX_base:.1%}')\n",
    "print(f'lock   | {f1_g0_lock:.1%}\\t{f1_gX_lock:.1%}\\t{H_g0_lock:.1%}\\t{H_gX_lock:.1%}\\t{H_g0gX_lock:.1%}')\n",
    "print(f'delta  | {d_g0:.1%}\\t{d_gX:.1%}\\t{d_H_g0:.1%}\\t{d_H_gX:.1%}\\t{d_H_g0gX:.1%}')\n",
    "\n",
    "print(f'Changes')\n",
    "print(f'g0: {changes_g0}')\n",
    "print(f'gX: {changes_gX}')\n",
    "print(f'g0gX: {changes_g0gX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset': 'flickr',\n",
       " 'seed': 123,\n",
       " 'config': 'config2.yml',\n",
       " 'g0_method': 'many_clusters',\n",
       " 'g0_size': 0.1,\n",
       " 'attack_epochs': 30,\n",
       " 'attack_method': 'sll',\n",
       " 'budget_pct': 0.25,\n",
       " 'f1_g0_base': 0.4385733157199472,\n",
       " 'f1_gX_base': 0.41857923497267757,\n",
       " 'f1_g0_lock': 0.22985468956406868,\n",
       " 'f1_gX_lock': 0.4087431693989071,\n",
       " 'd_g0': -0.47590361445783136,\n",
       " 'd_gX': -0.023498694516971255,\n",
       " 'H_overall': 0.2385854557892366,\n",
       " 'H_g0_base': 0.19039957093054438,\n",
       " 'H_gX_base': 0.26369172849075406,\n",
       " 'H_g0gX_base': 0.2385854557892366,\n",
       " 'H_g0_lock': 0.17292118170411772,\n",
       " 'H_gX_lock': 0.24577119261598251,\n",
       " 'H_g0gX_lock': 0.2178892661531725,\n",
       " 'd_H_g0': -0.09179846961316203,\n",
       " 'd_H_gX': -0.06796017447092545,\n",
       " 'd_H_g0gX': -0.0867453951356819,\n",
       " 'changes_g0': 12520.0,\n",
       " 'changes_gX': 25354.0,\n",
       " 'changes_g0gX': 43498.0,\n",
       " 'num_classes': 9,\n",
       " 'edges_modified': 40686,\n",
       " 'orig_edges': 239738}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the results\n",
    "import export\n",
    "import importlib\n",
    "importlib.reload(utils)\n",
    "\n",
    "results = {\n",
    "  'dataset': args.dataset,\n",
    "  'seed': args.seed,\n",
    "  'config': args.config,\n",
    "  'g0_method': args.g0_method,\n",
    "  'g0_size': args.g0_size,\n",
    "  'attack_epochs': args.attack_epochs,\n",
    "  'attack_method': args.attack_method,\n",
    "  'budget_pct': args.budget_pct,\n",
    "  'f1_g0_base': f1_g0_base,\n",
    "  'f1_gX_base': f1_gX_base,\n",
    "  'f1_g0_lock': f1_g0_lock,\n",
    "  'f1_gX_lock': f1_gX_lock,\n",
    "  'd_g0': d_g0,\n",
    "  'd_gX': d_gX,\n",
    "  'H_overall': H_overall,\n",
    "  'H_g0_base': H_g0_base,\n",
    "  'H_gX_base': H_gX_base,\n",
    "  'H_g0gX_base': H_g0gX_base,\n",
    "  'H_g0_lock': H_g0_lock,\n",
    "  'H_gX_lock': H_gX_lock,\n",
    "  'H_g0gX_lock': H_g0gX_lock,\n",
    "  'd_H_g0': d_H_g0,\n",
    "  'd_H_gX': d_H_gX,\n",
    "  'd_H_g0gX': d_H_g0gX,\n",
    "  'changes_g0': changes_g0,\n",
    "  'changes_gX': changes_gX,\n",
    "  'changes_g0gX': changes_g0gX,\n",
    "  'num_classes': int(labels.max().item()+1),\n",
    "  'edges_modified': int(diff.abs().sum() / 2),\n",
    "  'orig_edges': int(adj.sum() / 2)\n",
    "}\n",
    "\n",
    "results\n",
    "# if args.save_results: export.saveData('./out.csv', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.save_graph: utils.save_as_dgl(\n",
    "  graph, adj, g0, \n",
    "  name=f'{args.dataset} {args.g0_method} {args.attack_method} {args.budget_pct}', \n",
    "  root='./locked/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne = TSNE(2, perplexity=80, n_iter=400)\n",
    "\n",
    "# # Base\n",
    "# tsne_proj = tsne.fit_transform(base_pred.detach())\n",
    "# fig, ax = plt.subplots(figsize=(8,8))\n",
    "# X = tsne_proj[gX,0]\n",
    "# Y = tsne_proj[gX,1]\n",
    "# ax.scatter(X, Y, label = 'gX' ,alpha=1)\n",
    "# X = tsne_proj[g0,0]\n",
    "# Y = tsne_proj[g0,1]\n",
    "# ax.scatter(X, Y, label = 'g0' ,alpha=1)\n",
    "# ax.legend(fontsize='large', markerscale=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locked\n",
    "# tsne_proj = tsne.fit_transform(lock_pred.detach())\n",
    "# fig, ax = plt.subplots(figsize=(8,8))\n",
    "# X = tsne_proj[gX,0]\n",
    "# Y = tsne_proj[gX,1]\n",
    "# ax.scatter(X, Y, label = 'gX' ,alpha=1)\n",
    "# X = tsne_proj[g0,0]\n",
    "# Y = tsne_proj[g0,1]\n",
    "# ax.scatter(X, Y, label = 'g0' ,alpha=1)\n",
    "# ax.legend(fontsize='large', markerscale=2)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
    "# print(f\"Protected Size: {g0.sum() / graph.num_nodes():.2%}\")\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# data = labels[g0].cpu().numpy()\n",
    "# plt.hist(data, bins=np.arange(0, data.max() + 1.5) - 0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch_c116')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace00f8df87249d7fb913fbec74912fd8ad566274bc64c0a2570c224c3461cb4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
