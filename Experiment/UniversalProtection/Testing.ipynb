{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../..'))\n",
    "\n",
    "################################################\n",
    "# Arguments\n",
    "################################################\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed for model')\n",
    "parser.add_argument('--dataset', type=str, default='cora', help='dataset')\n",
    "\n",
    "parser.add_argument('--model_lr', type=float, default=0.01, help='Initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters)')\n",
    "parser.add_argument('--hidden_layers', type=int, default=32, help='Number of hidden layers')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate for GCN')\n",
    "\n",
    "parser.add_argument('--protect_size', type=float, default=0.1, help='Number of randomly chosen protected nodes')\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.25, help='Perturbation rate (percentage of available edges)')\n",
    "\n",
    "parser.add_argument('--sample_size', type=int, default=400, help='')\n",
    "parser.add_argument('--num_samples', type=int, default=5, help='')\n",
    "parser.add_argument('--num_subtasks', type=int, default=10, help='')\n",
    "\n",
    "parser.add_argument('--reg_epochs', type=int, default=25, help='Epochs to train models')\n",
    "parser.add_argument('--ptb_epochs', type=int, default=10, help='Epochs to perturb adj matrix')\n",
    "parser.add_argument('--surrogate_epochs', type=int, default=0, help='Epochs to train surrogate before perturb')\n",
    "\n",
    "parser.add_argument('--save', type=str, default='N', help='save the outputs to csv')\n",
    "parser.add_argument('--save_location', type=str, default=\"./UniversalProtection.csv\", help='where to save the outputs to csv')\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Environment ====\n",
      "  torch version: 1.10.2\n",
      "  device: cpu\n",
      "  torch seed: 123\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Environment\n",
    "################################################\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if device != 'cpu':\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print('==== Environment ====')\n",
    "print(f'  torch version: {torch.__version__}')\n",
    "print(f'  device: {device}')\n",
    "print(f'  torch seed: {args.seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Dataset: cora ====\n",
      "Loading cora dataset...\n",
      "\n",
      "==== Dataset Summary:  ====\n",
      "adj shape: [2708, 2708]\n",
      "feature shape: [2708, 1433]\n",
      "num labels: 7\n",
      "split seed: 123\n",
      "train|val|test: 140|500|1000\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Dataset\n",
    "################################################\n",
    "\n",
    "from Utils import GraphData\n",
    "from Utils import Metrics\n",
    "\n",
    "print(f'==== Dataset: {args.dataset} ====')\n",
    "\n",
    "graph = GraphData.getGraph(\"../../Datasets\", args.dataset, \"gcn\", args.seed, device)\n",
    "\n",
    "graph.summarize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils import Utils\n",
    "\n",
    "tasks = {}\n",
    "\n",
    "# Add labels as a task\n",
    "entropy, correlation, idx = Metrics.get_ent_cor(graph.labels.unsqueeze(1).float(), graph.labels, 1)\n",
    "tasks[-1] = {\n",
    "        \"ent\": entropy.item(),\n",
    "        \"corr\": correlation.item(),\n",
    "        \"feat\": graph.labels\n",
    "    }\n",
    "\n",
    "# Find highest entropy\n",
    "entropy, correlation, idx = Metrics.get_ent_cor(graph.features, graph.labels, args.num_subtasks)\n",
    "\n",
    "for f_idx in range(idx.shape[0]):\n",
    "    tasks[idx[f_idx].item()] = {\n",
    "        \"ent\": entropy[f_idx].item(),\n",
    "        \"corr\": correlation[f_idx].item(),\n",
    "        \"feat\": graph.features[:, idx[f_idx]]\n",
    "    }\n",
    "\n",
    "# Remove from feature set\n",
    "unselected = Utils.bool_to_idx(~Utils.idx_to_bool(idx, graph.features.shape[1])).squeeze()\n",
    "graph.features = graph.features[:, unselected]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protected nodes: 285\n",
      "Protected Size: 10.52%\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Designate protected\n",
    "################################################\n",
    "\n",
    "g0 = torch.rand(graph.features.shape[0]) <= args.protect_size\n",
    "# g0 = graph.labels == 5 \n",
    "g0 = g0.to(device)\n",
    "gX = ~g0\n",
    "\n",
    "print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
    "print(f\"Protected Size: {g0.sum() / graph.features.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Sampling Matrix\n",
    "################################################\n",
    "\n",
    "import Utils.Utils as Utils\n",
    "from SamplingMatrix import SamplingMatrix\n",
    "\n",
    "samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
    "\n",
    "samplingMatrix.get_sample()\n",
    "samplingMatrix.getRatio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Surrogate Model\n",
    "################################################\n",
    "\n",
    "from Models.GCN import GCN\n",
    "\n",
    "surrogate = GCN(\n",
    "    input_features=graph.features.shape[1],\n",
    "    output_classes=graph.labels.max().item()+1,\n",
    "    hidden_layers=args.hidden_layers,\n",
    "    device=device,\n",
    "    lr=args.model_lr,\n",
    "    dropout=args.dropout,\n",
    "    weight_decay=args.weight_decay,\n",
    "    name=f\"surrogate\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s, adj_l=0.477, adj_g=0, pre-p=459, target=1319, loss=tensor(0.4772, grad_fn=<SubBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Generate Perturbations\n",
    "################################################\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "perturbations = torch.zeros_like(graph.adj).float()\n",
    "count = torch.zeros_like(graph.adj).float()\n",
    "num_perturbations = args.ptb_rate * graph.adj.sum()\n",
    "\n",
    "t = tqdm(range(args.ptb_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "t.set_description(\"Perturbing\")\n",
    "\n",
    "for epoch in t:\n",
    "\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(graph.adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = Utils.get_modified_adj(graph.adj, perturbations).float().to(device)\n",
    "\n",
    "    # Do sampling\n",
    "    for sample_epoch in range(args.num_samples):\n",
    "        # Get sample indices\n",
    "        # sampled = torch.bernoulli(sampling_matrix)\n",
    "        idx = samplingMatrix.get_sample()\n",
    "        # print(idx)\n",
    "\n",
    "        # Map sample to adj\n",
    "        sample = modified_adj[idx[0], idx[1]].clone().detach().requires_grad_(True).to(device)\n",
    "        modified_adj[idx[0], idx[1]] = sample\n",
    "\n",
    "        # Get grad\n",
    "        predictions = surrogate(graph.features, modified_adj)\n",
    "        loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "        grad = torch.autograd.grad(loss, sample)[0]\n",
    "\n",
    "        # Implement averaging\n",
    "        adj_grad[idx[0], idx[1]] += grad\n",
    "        count[idx[0], idx[1]] += 1\n",
    "\n",
    "        # Update the sampling matrix\n",
    "        samplingMatrix.updateByGrad(adj_grad, count)\n",
    "        samplingMatrix.getRatio()\n",
    "\n",
    "        # Average the gradient\n",
    "        adj_grad = torch.div(adj_grad, count)\n",
    "        adj_grad[adj_grad != adj_grad] = 0\n",
    "    \n",
    "    # Update perturbations\n",
    "    lr = (num_perturbations) / (epoch + 1)\n",
    "    pre_projection = int(perturbations.sum() / 2)\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "    perturbations = Utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the model\n",
    "    modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "    surrogate.train1epoch(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_test)\n",
    "\n",
    "    t.set_postfix({\"adj_l\": loss.item(),\n",
    "                    \"adj_g\": int(adj_grad.sum()),\n",
    "                    \"pre-p\": pre_projection,\n",
    "                    \"target\": int(num_perturbations / 2),\n",
    "                    \"loss\": loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sample loss: 0.45\t Edges: 487\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Get best sample\n",
    "################################################\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    max_loss = -1000\n",
    "\n",
    "    for k in range(0,3):\n",
    "        sample = torch.bernoulli(perturbations)\n",
    "        modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "        modified_adj = Utils.make_symmetric(modified_adj) # Removing this creates \"impossible\" adj, but works well\n",
    "\n",
    "        predictions = surrogate(graph.features, modified_adj) \n",
    "\n",
    "        loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            best = sample\n",
    "    \n",
    "    print(f\"Best sample loss: {loss:.2f}\\t Edges: {best.abs().sum() / 2:.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Eval ====\n",
      "Task,\tΔG0,\tΔGX,\tEnt\tCorr\n",
      "-1,\t-19.3%,\t1.6%,\t2.64,\t1.00\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Evaluate universal\n",
    "################################################\n",
    "\n",
    "from Utils import Export\n",
    "\n",
    "print(\"==== Eval ====\")\n",
    "print(f\"Task,\\tΔG0,\\tΔGX,\\tEnt\\tCorr\")\n",
    "\n",
    "locked_adj = Utils.get_modified_adj(graph.adj, best)\n",
    "diff = locked_adj - graph.adj\n",
    "\n",
    "for t in tasks:\n",
    "    temp_labels = tasks[t][\"feat\"].long()\n",
    "    label_max = temp_labels.max().item() + 1\n",
    "\n",
    "    baseline_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=label_max,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"baseline\"\n",
    "    ).to(device)\n",
    "\n",
    "    baseline_model.fitManual(graph.features, graph.adj, temp_labels, graph.idx_train, graph.idx_test, args.reg_epochs, False)\n",
    "\n",
    "    pred = baseline_model(graph.features, graph.adj)\n",
    "    baseline_acc = Metrics.partial_acc(pred, temp_labels, g0, gX, False)\n",
    "\n",
    "\n",
    "    locked_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=label_max,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"locked\"\n",
    "    )\n",
    "\n",
    "    locked_model.fitManual(graph.features, locked_adj, temp_labels, graph.idx_train, graph.idx_test, args.reg_epochs, False)\n",
    "\n",
    "    pred = locked_model(graph.features, locked_adj)\n",
    "    locked_acc = Metrics.partial_acc(pred, temp_labels, g0, gX, False)\n",
    "\n",
    "    dg0 = locked_acc[\"g0\"] - baseline_acc[\"g0\"]\n",
    "    dgX = locked_acc[\"gX\"] - baseline_acc[\"gX\"]\n",
    "\n",
    "    print(f\"{t},\\t{dg0:.1%},\\t{dgX:.1%},\\t{tasks[t]['ent']:.2f},\\t{tasks[t]['corr']:.2f}\")\n",
    "\n",
    "    diffSummary = Metrics.show_metrics(diff, temp_labels, g0, device, False)\n",
    "\n",
    "    results = {\n",
    "        \"seed\": args.seed,\n",
    "        \"dataset\": args.dataset,\n",
    "        \"protect_size\": args.protect_size,\n",
    "        \"reg_epochs\": args.reg_epochs,\n",
    "        \"ptb_epochs\": args.ptb_epochs,\n",
    "        \"ptb_rate\": args.ptb_rate,\n",
    "        \"ptb_sample_num\": args.num_samples,\n",
    "        \"ptb_sample_size\": args.sample_size,\n",
    "        \"ratio_g0\": samplingMatrix.g0_ratio.item(),\n",
    "        \"ratio_gX\": samplingMatrix.gX_ratio.item(),\n",
    "        \"ratio_g0gX\": samplingMatrix.g0gX_ratio.item(),\n",
    "        \"feature\": t,\n",
    "        \"corr\": tasks[t][\"corr\"],\n",
    "        \"entropy\": tasks[t][\"ent\"],\n",
    "        \"base_g0\": baseline_acc[\"g0\"],\n",
    "        \"base_gX\": baseline_acc[\"gX\"],\n",
    "        \"d_g0\": dg0,\n",
    "        \"d_gX\": dgX,\n",
    "        \"edges\": int(diff.abs().sum().item()),\n",
    "    }\n",
    "\n",
    "    for add_remove in [\"add\", \"remove\"]:\n",
    "        for location in [\"g0\", \"gX\", \"g0gX\"]:\n",
    "            for similar in [\"same\", \"diff\"]:\n",
    "                results[\"_\".join([add_remove, location, similar])] = diffSummary[location][add_remove][similar]\n",
    "\n",
    "    Export.saveData(args.save_location, results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "284a17d5edba1b82bbb8793a64a3a9f6114640b8c8687fac297a1d74e5a299a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pygraph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
