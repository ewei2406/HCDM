{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(os.path.abspath(''), '../..'))\n",
    "\n",
    "################################################\n",
    "# Arguments\n",
    "################################################\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--seed', type=int, default=123, help='Random seed for model')\n",
    "parser.add_argument('--dataset', type=str, default='citeseer', help='dataset')\n",
    "\n",
    "parser.add_argument('--model_lr', type=float, default=0.01, help='Initial learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4, help='Weight decay (L2 loss on parameters)')\n",
    "parser.add_argument('--hidden_layers', type=int, default=32, help='Number of hidden layers')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='Dropout rate for GCN')\n",
    "\n",
    "parser.add_argument('--protect_size', type=float, default=0.1, help='Number of randomly chosen protected nodes')\n",
    "parser.add_argument('--ptb_rate', type=float, default=0.25, help='Perturbation rate (percentage of available edges)')\n",
    "\n",
    "parser.add_argument('--sample_size', type=int, default=400, help='')\n",
    "parser.add_argument('--num_samples', type=int, default=5, help='')\n",
    "parser.add_argument('--num_subtasks', type=int, default=10, help='')\n",
    "\n",
    "parser.add_argument('--reg_epochs', type=int, default=25, help='Epochs to train models')\n",
    "parser.add_argument('--ptb_epochs', type=int, default=10, help='Epochs to perturb adj matrix')\n",
    "parser.add_argument('--surrogate_epochs', type=int, default=0, help='Epochs to train surrogate before perturb')\n",
    "\n",
    "parser.add_argument('--save', type=str, default='N', help='save the outputs to csv')\n",
    "parser.add_argument('--save_location', type=str, default=\"./UniversalProtection.csv\", help='where to save the outputs to csv')\n",
    "\n",
    "args = parser.parse_args(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Environment ====\n",
      "  torch version: 1.10.2\n",
      "  device: cpu\n",
      "  torch seed: 123\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Environment\n",
    "################################################\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "if device != 'cpu':\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "print('==== Environment ====')\n",
    "print(f'  torch version: {torch.__version__}')\n",
    "print(f'  device: {device}')\n",
    "print(f'  torch seed: {args.seed}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Dataset: citeseer ====\n",
      "Loading citeseer dataset...\n",
      "Downloading from https://raw.githubusercontent.com/danielzuegner/gnn-meta-attack/master/data/citeseer.npz to ../../Datasets/citeseer.npz\n",
      "Done!\n",
      "\n",
      "==== Dataset Summary:  ====\n",
      "adj shape: [3312, 3312]\n",
      "feature shape: [3312, 3703]\n",
      "num labels: 6\n",
      "split seed: 123\n",
      "train|val|test: 120|500|1000\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Dataset\n",
    "################################################\n",
    "\n",
    "from Utils import GraphData\n",
    "from Utils import Metrics\n",
    "\n",
    "print(f'==== Dataset: {args.dataset} ====')\n",
    "\n",
    "graph = GraphData.getGraph(\"../../Datasets\", args.dataset, \"gcn\", args.seed, device)\n",
    "\n",
    "graph.summarize()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3312, 3703])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([  65, 1868, 2568,  729, 3472,  601, 2143, 3618, 3010, 3583])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_entropy(data: torch.tensor):\n",
    "    bins = torch.histc(data, bins=50)\n",
    "    bins /= bins.sum()\n",
    "    return ((bins * torch.log2(bins)).nan_to_num().sum() * -1).item()\n",
    "\n",
    "def calc_correlation(tensor1: torch.tensor, tensor2: torch.tensor):\n",
    "    cat = torch.cat((tensor1.unsqueeze(0), tensor2.unsqueeze(0))).numpy()\n",
    "    return np.corrcoef(cat)[0][1]\n",
    "\n",
    "def get_ent_cor(features: torch.tensor, labels: torch.tensor, num: int=10):\n",
    "    \"\"\"\n",
    "    Return the features with most entropy and/or correlation\n",
    "    \n",
    "    Parameters\n",
    "    ---\n",
    "    par_name : par_type\n",
    "        par_description\n",
    "    \n",
    "    Returns\n",
    "    ---\n",
    "    entropy, correlation, index\n",
    "    \n",
    "    Examples\n",
    "    ---\n",
    "    >>>example\n",
    "    \n",
    "    \"\"\"\n",
    "    print(features.shape)\n",
    "    ent_cor = torch.zeros(3, features.shape[1])\n",
    "\n",
    "    for r in range(features.shape[1]):\n",
    "        feat = features.t()[r]\n",
    "        entropy = calc_entropy(feat)\n",
    "        correlation = abs(calc_correlation(feat, labels))\n",
    "        ent_cor[0][r] = entropy\n",
    "        ent_cor[1][r] = correlation\n",
    "        ent_cor[2][r] = entropy + correlation\n",
    "\n",
    "    ent_cor.nan_to_num_()\n",
    "\n",
    "    idx = torch.topk(ent_cor[2], num - 3, sorted=True).indices\n",
    "    idx = torch.cat((idx, torch.topk(ent_cor[2], 3, sorted=True, largest=False).indices))\n",
    "    data = ent_cor[:,idx]\n",
    "\n",
    "    return data[0], data[1], idx\n",
    "\n",
    "\n",
    "ent, cor, idx = get_ent_cor(graph.features, graph.labels)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 3312 is out of bounds for dimension 0 with size 3312",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb Cell 4'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=6'>7</a>\u001b[0m tasks[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=7'>8</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ment\u001b[39m\u001b[39m\"\u001b[39m: entropy\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=8'>9</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorr\u001b[39m\u001b[39m\"\u001b[39m: correlation\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=9'>10</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m\"\u001b[39m: graph\u001b[39m.\u001b[39mlabels\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=10'>11</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=12'>13</a>\u001b[0m \u001b[39m# Find highest entropy\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=13'>14</a>\u001b[0m entropy, correlation, idx \u001b[39m=\u001b[39m Metrics\u001b[39m.\u001b[39;49mget_ent_cor(graph\u001b[39m.\u001b[39;49mfeatures, graph\u001b[39m.\u001b[39;49mlabels, args\u001b[39m.\u001b[39;49mnum_subtasks)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=15'>16</a>\u001b[0m \u001b[39mfor\u001b[39;00m f_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(idx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=16'>17</a>\u001b[0m     tasks[idx[f_idx]\u001b[39m.\u001b[39mitem()] \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=17'>18</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39ment\u001b[39m\u001b[39m\"\u001b[39m: entropy[f_idx]\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=18'>19</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorr\u001b[39m\u001b[39m\"\u001b[39m: correlation[f_idx]\u001b[39m.\u001b[39mitem(),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=19'>20</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m\"\u001b[39m: graph\u001b[39m.\u001b[39mfeatures[:, idx[f_idx]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ewei/HCDM/Experiment/UniversalProtection/Testing.ipynb#ch0000015?line=20'>21</a>\u001b[0m     }\n",
      "File \u001b[0;32m~/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py:143\u001b[0m, in \u001b[0;36mget_ent_cor\u001b[0;34m(features, labels, num)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ewei/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py?line=140'>141</a>\u001b[0m entropy \u001b[39m=\u001b[39m calc_entropy(feat)\n\u001b[1;32m    <a href='file:///Users/ewei/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py?line=141'>142</a>\u001b[0m correlation \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(calc_correlation(feat, labels))\n\u001b[0;32m--> <a href='file:///Users/ewei/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py?line=142'>143</a>\u001b[0m ent_cor[\u001b[39m0\u001b[39m][r] \u001b[39m=\u001b[39m entropy\n\u001b[1;32m    <a href='file:///Users/ewei/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py?line=143'>144</a>\u001b[0m ent_cor[\u001b[39m1\u001b[39m][r] \u001b[39m=\u001b[39m correlation\n\u001b[1;32m    <a href='file:///Users/ewei/HCDM/Experiment/UniversalProtection/../../Utils/Metrics.py?line=144'>145</a>\u001b[0m ent_cor[\u001b[39m2\u001b[39m][r] \u001b[39m=\u001b[39m entropy \u001b[39m+\u001b[39m correlation\n",
      "\u001b[0;31mIndexError\u001b[0m: index 3312 is out of bounds for dimension 0 with size 3312"
     ]
    }
   ],
   "source": [
    "from Utils import Utils\n",
    "\n",
    "tasks = {}\n",
    "\n",
    "# Add labels as a task\n",
    "entropy, correlation, idx = Metrics.get_ent_cor(graph.labels.unsqueeze(1).float(), graph.labels, 1)\n",
    "tasks[-1] = {\n",
    "        \"ent\": entropy.item(),\n",
    "        \"corr\": correlation.item(),\n",
    "        \"feat\": graph.labels\n",
    "    }\n",
    "\n",
    "# Find highest entropy\n",
    "entropy, correlation, idx = Metrics.get_ent_cor(graph.features, graph.labels, args.num_subtasks)\n",
    "\n",
    "for f_idx in range(idx.shape[0]):\n",
    "    tasks[idx[f_idx].item()] = {\n",
    "        \"ent\": entropy[f_idx].item(),\n",
    "        \"corr\": correlation[f_idx].item(),\n",
    "        \"feat\": graph.features[:, idx[f_idx]]\n",
    "    }\n",
    "\n",
    "# Remove from feature set\n",
    "unselected = Utils.bool_to_idx(~Utils.idx_to_bool(idx, graph.features.shape[1])).squeeze()\n",
    "graph.features = graph.features[:, unselected]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of protected nodes: 285\n",
      "Protected Size: 10.52%\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Designate protected\n",
    "################################################\n",
    "\n",
    "g0 = torch.rand(graph.features.shape[0]) <= args.protect_size\n",
    "# g0 = graph.labels == 5 \n",
    "g0 = g0.to(device)\n",
    "gX = ~g0\n",
    "\n",
    "print(f\"Number of protected nodes: {g0.sum():.0f}\")\n",
    "print(f\"Protected Size: {g0.sum() / graph.features.shape[0]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Sampling Matrix\n",
    "################################################\n",
    "\n",
    "import Utils.Utils as Utils\n",
    "from SamplingMatrix import SamplingMatrix\n",
    "\n",
    "samplingMatrix = SamplingMatrix(g0, gX, graph.adj, args.sample_size)\n",
    "\n",
    "samplingMatrix.get_sample()\n",
    "samplingMatrix.getRatio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "# Surrogate Model\n",
    "################################################\n",
    "\n",
    "from Models.GCN import GCN\n",
    "\n",
    "surrogate = GCN(\n",
    "    input_features=graph.features.shape[1],\n",
    "    output_classes=graph.labels.max().item()+1,\n",
    "    hidden_layers=args.hidden_layers,\n",
    "    device=device,\n",
    "    lr=args.model_lr,\n",
    "    dropout=args.dropout,\n",
    "    weight_decay=args.weight_decay,\n",
    "    name=f\"surrogate\"\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Perturbing: 100%|██████████| 10/10 [00:09<00:00,  1.05it/s, adj_l=0.477, adj_g=0, pre-p=459, target=1319, loss=tensor(0.4772, grad_fn=<SubBackward0>)]\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Generate Perturbations\n",
    "################################################\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "perturbations = torch.zeros_like(graph.adj).float()\n",
    "count = torch.zeros_like(graph.adj).float()\n",
    "num_perturbations = args.ptb_rate * graph.adj.sum()\n",
    "\n",
    "t = tqdm(range(args.ptb_epochs), bar_format='{l_bar}{bar:10}{r_bar}{bar:-10b}')\n",
    "t.set_description(\"Perturbing\")\n",
    "\n",
    "for epoch in t:\n",
    "\n",
    "    # Re-initialize adj_grad\n",
    "    adj_grad = torch.zeros_like(graph.adj).float()\n",
    "\n",
    "    # Get modified adj\n",
    "    modified_adj = Utils.get_modified_adj(graph.adj, perturbations).float().to(device)\n",
    "\n",
    "    # Do sampling\n",
    "    for sample_epoch in range(args.num_samples):\n",
    "        # Get sample indices\n",
    "        # sampled = torch.bernoulli(sampling_matrix)\n",
    "        idx = samplingMatrix.get_sample()\n",
    "        # print(idx)\n",
    "\n",
    "        # Map sample to adj\n",
    "        sample = modified_adj[idx[0], idx[1]].clone().detach().requires_grad_(True).to(device)\n",
    "        modified_adj[idx[0], idx[1]] = sample\n",
    "\n",
    "        # Get grad\n",
    "        predictions = surrogate(graph.features, modified_adj)\n",
    "        loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "        grad = torch.autograd.grad(loss, sample)[0]\n",
    "\n",
    "        # Implement averaging\n",
    "        adj_grad[idx[0], idx[1]] += grad\n",
    "        count[idx[0], idx[1]] += 1\n",
    "\n",
    "        # Update the sampling matrix\n",
    "        samplingMatrix.updateByGrad(adj_grad, count)\n",
    "        samplingMatrix.getRatio()\n",
    "\n",
    "        # Average the gradient\n",
    "        adj_grad = torch.div(adj_grad, count)\n",
    "        adj_grad[adj_grad != adj_grad] = 0\n",
    "    \n",
    "    # Update perturbations\n",
    "    lr = (num_perturbations) / (epoch + 1)\n",
    "    pre_projection = int(perturbations.sum() / 2)\n",
    "    perturbations = perturbations + (lr * adj_grad)\n",
    "    perturbations = Utils.projection(perturbations, num_perturbations)\n",
    "\n",
    "    # Train the model\n",
    "    modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "    surrogate.train1epoch(graph.features, modified_adj, graph.labels, graph.idx_train, graph.idx_test)\n",
    "\n",
    "    t.set_postfix({\"adj_l\": loss.item(),\n",
    "                    \"adj_g\": int(adj_grad.sum()),\n",
    "                    \"pre-p\": pre_projection,\n",
    "                    \"target\": int(num_perturbations / 2),\n",
    "                    \"loss\": loss})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best sample loss: 0.45\t Edges: 487\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Get best sample\n",
    "################################################\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    max_loss = -1000\n",
    "\n",
    "    for k in range(0,3):\n",
    "        sample = torch.bernoulli(perturbations)\n",
    "        modified_adj = Utils.get_modified_adj(graph.adj, perturbations)\n",
    "        modified_adj = Utils.make_symmetric(modified_adj) # Removing this creates \"impossible\" adj, but works well\n",
    "\n",
    "        predictions = surrogate(graph.features, modified_adj) \n",
    "\n",
    "        loss = F.cross_entropy(predictions[g0], graph.labels[g0]) \\\n",
    "            - F.cross_entropy(predictions[gX], graph.labels[gX])\n",
    "\n",
    "        if loss > max_loss:\n",
    "            max_loss = loss\n",
    "            best = sample\n",
    "    \n",
    "    print(f\"Best sample loss: {loss:.2f}\\t Edges: {best.abs().sum() / 2:.0f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Eval ====\n",
      "Task,\tΔG0,\tΔGX,\tEnt\tCorr\n",
      "-1,\t-19.3%,\t1.6%,\t2.64,\t1.00\n"
     ]
    }
   ],
   "source": [
    "################################################\n",
    "# Evaluate universal\n",
    "################################################\n",
    "\n",
    "from Utils import Export\n",
    "\n",
    "print(\"==== Eval ====\")\n",
    "print(f\"Task,\\tΔG0,\\tΔGX,\\tEnt\\tCorr\")\n",
    "\n",
    "locked_adj = Utils.get_modified_adj(graph.adj, best)\n",
    "diff = locked_adj - graph.adj\n",
    "\n",
    "for t in tasks:\n",
    "    temp_labels = tasks[t][\"feat\"].long()\n",
    "    label_max = temp_labels.max().item() + 1\n",
    "\n",
    "    baseline_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=label_max,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"baseline\"\n",
    "    ).to(device)\n",
    "\n",
    "    baseline_model.fitManual(graph.features, graph.adj, temp_labels, graph.idx_train, graph.idx_test, args.reg_epochs, False)\n",
    "\n",
    "    pred = baseline_model(graph.features, graph.adj)\n",
    "    baseline_acc = Metrics.partial_acc(pred, temp_labels, g0, gX, False)\n",
    "\n",
    "\n",
    "    locked_model = GCN(\n",
    "        input_features=graph.features.shape[1],\n",
    "        output_classes=label_max,\n",
    "        hidden_layers=args.hidden_layers,\n",
    "        device=device,\n",
    "        lr=args.model_lr,\n",
    "        dropout=args.dropout,\n",
    "        weight_decay=args.weight_decay,\n",
    "        name=f\"locked\"\n",
    "    )\n",
    "\n",
    "    locked_model.fitManual(graph.features, locked_adj, temp_labels, graph.idx_train, graph.idx_test, args.reg_epochs, False)\n",
    "\n",
    "    pred = locked_model(graph.features, locked_adj)\n",
    "    locked_acc = Metrics.partial_acc(pred, temp_labels, g0, gX, False)\n",
    "\n",
    "    dg0 = locked_acc[\"g0\"] - baseline_acc[\"g0\"]\n",
    "    dgX = locked_acc[\"gX\"] - baseline_acc[\"gX\"]\n",
    "\n",
    "    print(f\"{t},\\t{dg0:.1%},\\t{dgX:.1%},\\t{tasks[t]['ent']:.2f},\\t{tasks[t]['corr']:.2f}\")\n",
    "\n",
    "    diffSummary = Metrics.show_metrics(diff, temp_labels, g0, device, False)\n",
    "\n",
    "    results = {\n",
    "        \"seed\": args.seed,\n",
    "        \"dataset\": args.dataset,\n",
    "        \"protect_size\": args.protect_size,\n",
    "        \"reg_epochs\": args.reg_epochs,\n",
    "        \"ptb_epochs\": args.ptb_epochs,\n",
    "        \"ptb_rate\": args.ptb_rate,\n",
    "        \"ptb_sample_num\": args.num_samples,\n",
    "        \"ptb_sample_size\": args.sample_size,\n",
    "        \"ratio_g0\": samplingMatrix.g0_ratio.item(),\n",
    "        \"ratio_gX\": samplingMatrix.gX_ratio.item(),\n",
    "        \"ratio_g0gX\": samplingMatrix.g0gX_ratio.item(),\n",
    "        \"feature\": t,\n",
    "        \"corr\": tasks[t][\"corr\"],\n",
    "        \"entropy\": tasks[t][\"ent\"],\n",
    "        \"base_g0\": baseline_acc[\"g0\"],\n",
    "        \"base_gX\": baseline_acc[\"gX\"],\n",
    "        \"d_g0\": dg0,\n",
    "        \"d_gX\": dgX,\n",
    "        \"edges\": int(diff.abs().sum().item()),\n",
    "    }\n",
    "\n",
    "    for add_remove in [\"add\", \"remove\"]:\n",
    "        for location in [\"g0\", \"gX\", \"g0gX\"]:\n",
    "            for similar in [\"same\", \"diff\"]:\n",
    "                results[\"_\".join([add_remove, location, similar])] = diffSummary[location][add_remove][similar]\n",
    "\n",
    "    Export.saveData(args.save_location, results)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "284a17d5edba1b82bbb8793a64a3a9f6114640b8c8687fac297a1d74e5a299a9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pygraph')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
